{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "import string\n",
    "import inflection\n",
    "import json\n",
    "import subprocess\n",
    "from unqlite import UnQLite\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from re import finditer\n",
    "stemmer = PorterStemmer()\n",
    "from skopt import *\n",
    "class CorpusParser:\n",
    "\n",
    "\tdef __init__(self, filename):\n",
    "\t\tself.filename = filename\n",
    "\t\tself.regex = re.compile('^#\\s*\\d+')\n",
    "\t\tself.corpus = dict()\n",
    "\n",
    "\tdef parse(self):\n",
    "\t\twith open(self.filename) as f:\n",
    "\t\t\ts = ''.join(f.readlines())\n",
    "\t\tblobs = s.split('#')[1:]\n",
    "\t\tfor x in blobs:\n",
    "\t\t\ttext = x.split()\n",
    "\t\t\tdocid = text.pop(0)\n",
    "\t\t\tself.corpus[docid] = text\n",
    "\n",
    "\tdef get_corpus(self):\n",
    "\t\treturn self.corpus\n",
    "\n",
    "\n",
    "class QueryParser:\n",
    "\n",
    "\tdef __init__(self, filename):\n",
    "\t\tself.filename = filename\n",
    "\t\tself.queries = []\n",
    "\n",
    "\tdef parse(self):\n",
    "\t\twith open(self.filename) as f:\n",
    "\t\t\tlines = ''.join(f.readlines())\n",
    "\t\tself.queries = [x.rstrip().split() for x in lines.split('\\n')[:-1]]\n",
    "\n",
    "\tdef get_queries(self):\n",
    "\t\treturn self.queries\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     qp = QueryParser('/data/hdj/BM25/text/queries.txt')\n",
    "#     qp.parse()\n",
    "#     print (qp.get_queries())\n",
    "#     cp=CorpusParser('/data/hdj/BM25/text/corpus.txt')\n",
    "#     cp.parse()\n",
    "    \n",
    "    \n",
    "\n",
    "def _process_notes(note,note_not_tokenize, repository_path):\n",
    "    #ea6b85284ac8a5b67421bafebe99a7cc2ca6d73e 00001f23f4c6321b7b4468d2eae6b4a1e0e24bce\n",
    "    (note_content_sha, note_object_sha) = note.split(' ')\n",
    "    (note_content_sha_not_tokenize, note_object_sha_not_tokenize) = note_not_tokenize.split(' ')\n",
    "\n",
    "    note_content = cat_file_blob(repository_path, note_content_sha)\n",
    "    note_content_not_tokenize = cat_file_blob(repository_path, note_content_sha_not_tokenize)\n",
    "    ast_extraction_result = json.loads(note_content)\n",
    "    ast_extraction_result_not_tokenize = json.loads(note_content_not_tokenize)\n",
    "    ast_extraction_result['methodContent']=ast_extraction_result_not_tokenize['methodContent']\n",
    "    return note_object_sha, ast_extraction_result\n",
    "def cat_file_blob(repository_path, sha, encoding='latin-1'):\n",
    "    #git -C /data/hdj/SourceFile/tracking_buggy_files_tomcat_dataset cat-file blob ea6b85284ac8a5b67421bafebe99a7cc2ca6d73e > note_content.txt\n",
    "    #返回所有文件相关的信息 包括分割好的方法和源码 dict格式\n",
    "    cmd = ' '.join(['git', '-C', repository_path, 'cat-file', 'blob', sha])\n",
    "    cat_file_process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    result = cat_file_process.stdout.read().decode(encoding)\n",
    "    return result\n",
    "\n",
    "def _f(args):\n",
    "    return _process_notes(args[0], args[1],args[2])\n",
    "def list_notes(repository_path, refs='refs/notes/commits'):\n",
    "    #当ref=='refs/notes/tokenized_counters'时，解析出来的就是tokenized后的文件\n",
    "    #git -C /data/hdj/SourceFile/tracking_buggy_files_tomcat_dataset notes --ref refs/notes/commits list\n",
    "    #返回一堆 shas\n",
    "    cmd = ' '.join(['git', '-C', repository_path, 'notes', '--ref', refs, 'list'])\n",
    "    notes_lines = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().decode('latin-1').split('\\n')\n",
    "    return notes_lines\n",
    "\n",
    "tokenized_refs = 'refs/notes/tokenized_counters'\n",
    "ast_notes = list_notes(repository_path, refs=tokenized_refs)\n",
    "ast_notes_not_tokenize=list_notes(repository_path)\n",
    "print(\"existing tokenized notes \", len(ast_notes))\n",
    "print(\"existing not tokenized notes \", len(ast_notes_not_tokenize))\n",
    "\n",
    "ast_notes[:2],ast_notes_not_tokenize[:2]\n",
    "\n",
    "\n",
    "note_object_sha=None\n",
    "ast_extraction_result=None\n",
    "for note,note_not_tokenize in zip(ast_notes,ast_notes_not_tokenize):\n",
    "    note_object_sha, ast_extraction_result=_f([note,note_not_tokenize, repository_path])\n",
    "    break\n",
    "\n",
    "note_object_sha\n",
    "\n",
    "# ast_extraction_result\n",
    "\n",
    "\n",
    "repository_path='/data/hdj/SourceFile/tracking_buggy_files_tomcat_dataset'\n",
    "# refs='refs/notes/commits'\n",
    "refs='refs/notes/tokenized_counters'\n",
    "cmd = ' '.join(['git', '-C', repository_path, 'notes', '--ref', refs, 'list'])\n",
    "notes_lines = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().decode('latin-1').split('\\n')\n",
    "\n",
    "len(notes_lines)\n",
    "\n",
    "repository_path='/data/hdj/SourceFile/tracking_buggy_files_tomcat_dataset'\n",
    "refs_not_tokenize='refs/notes/commits'\n",
    "cmd = ' '.join(['git', '-C', repository_path, 'notes', '--ref', refs_not_tokenize, 'list'])\n",
    "notes_lines_not_tokenize = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout.read().decode('latin-1').split('\\n')\n",
    "\n",
    "len(notes_lines_not_tokenize)\n",
    "\n",
    "sha='52179dced2a1cf65ba73119a8296d362d7d0107e'\n",
    "encoding='latin-1'\n",
    "cmd = ' '.join(['git', '-C', repository_path, 'cat-file', 'blob', sha])\n",
    "cat_file_process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "result = cat_file_process.stdout.read().decode(encoding)\n",
    "\n",
    "sha_not_tokenize='ea6b85284ac8a5b67421bafebe99a7cc2ca6d73e '\n",
    "encoding='latin-1'\n",
    "cmd = ' '.join(['git', '-C', repository_path, 'cat-file', 'blob', sha_not_tokenize])\n",
    "cat_file_process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "result_not_tokenize = cat_file_process.stdout.read().decode(encoding)\n",
    "\n",
    "# eval(result_not_tokenize)\n",
    "\n",
    "# eval(result)\n",
    "\n",
    "# notes_lines\n",
    "\n",
    "\n",
    "bug_report_files_collection_db = UnQLite(\"/data/hdj/tracking_buggy_files/tomcat/tomcat_bug_report_files_collection_db\",\n",
    "                                             flags=0x00000100 | 0x00000001)\n",
    "tomcat_ast_cache_collection_db=UnQLite(\"/data/hdj/tracking_buggy_files/tomcat/tomcat_ast_cache_collection_db\",\n",
    "                                             flags=0x00000100 | 0x00000001)\n",
    "\n",
    "type(bug_report_files_collection_db),type(tomcat_ast_cache_collection_db)\n",
    "\n",
    "bug_report_files=[item for item in bug_report_files_collection_db]\n",
    "tomcat_ast=[item for item in tomcat_ast_cache_collection_db]\n",
    "\n",
    "len(tomcat_ast),len(bug_report_files_collection_db)\n",
    "\n",
    "# tomcat_ast[0]\n",
    "\n",
    "# bug_report_files[0]\n",
    "\n",
    "tomcat_ast_file = pickle.loads(tomcat_ast_cache_collection_db['ebc8fbfa0b2fa6a61fef2955583440192785e6fb'])\n",
    "\n",
    "type(tomcat_ast_file)\n",
    "tomcat_ast=[item for item in tomcat_ast_cache_collection_db]\n",
    "for key,val in tomcat_ast_file.items():\n",
    "    print(key,type(val),len(val))\n",
    "\n",
    "len(tomcat_ast_file['tokenizedMethods'])\n",
    "tomcat_ast_file['tokenizedMethods'][0]\n",
    "\n",
    "# tomcat_ast_file['methodContent'][:10]\n",
    "\n",
    "# tomcat_ast_file['tokenizedSource']\n",
    "\n",
    "# tomcat_ast[:1]\n",
    "\n",
    "# bug_report_files[:1]\n",
    "\n",
    "import pickle\n",
    "current_files = pickle.loads(bug_report_files_collection_db['1cc6641c54f6b0d2dd69ab5323487ffa51f9df9d'])\n",
    "# bug_report_files_collection_db.close()\n",
    "\n",
    "type(current_files)\n",
    "for key,val in current_files.items():\n",
    "    print(key,type(val),len(val))\n",
    "\n",
    "# current_files['shas']\n",
    "\n",
    "# current_files['sha_to_file_name']\n",
    "\n",
    "# current_files['class_name_to_sha']\n",
    "\n",
    "# 检查vectorize_ast生成的数据格式\n",
    "\n",
    "data_prefix='/data/hdj/tracking_buggy_files/tomcat/tomcat'\n",
    "asts = UnQLite(data_prefix+\"_ast_index_collection_index_db\", flags = 0x00000100 | 0x00000001)\n",
    "types = UnQLite(data_prefix+\"_ast_types_collection_index_db\", flags = 0x00000100 | 0x00000001)\n",
    "bug_report_index_collection = UnQLite(data_prefix+\"_bug_report_index_collection_index_db\")\n",
    "raw_count_data = sparse.load_npz(data_prefix+'_raw_count_data.npz')\n",
    "feature_names=json.load(open(data_prefix+'_feature_names_dict'))\n",
    "ast_index=[item for item in asts]\n",
    "ast_types=[item for item in types]\n",
    "bug_report_index=[item for item in bug_report_index_collection]\n",
    "\n",
    "# bug_report_index\n",
    "raw_count_data.shape\n",
    "raw_count_data[0]\n",
    "\n",
    "print(ast_index[0][0],len(ast_index))\n",
    "print(ast_types[0][0],len(ast_types))\n",
    "print(bug_report_index[0][0],len(bug_report_index))\n",
    "ast_index_files = pickle.loads(asts[ast_index[0][0]])\n",
    "ast_types_files = pickle.loads(types[ast_types[0][0]])\n",
    "bug_report_index_files = pickle.loads(bug_report_index_collection[bug_report_index[0][0]])\n",
    "\n",
    "len(feature_names),\n",
    "# feature_names\n",
    "\n",
    "print('ast_index_files :',ast_index_files)\n",
    "print('ast_types_files :',ast_types_files)\n",
    "print('bug_report_index_files :',bug_report_index_files)\n",
    "\n",
    "#检查 _partial_enriched_api_index_lookup\n",
    "bug_report_id='0ae33cb'\n",
    "feature_names=json.load(open(data_prefix+'_'+bug_report_id+'_partial_enriched_api_index_lookup'))\n",
    "\n",
    "len(feature_names)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "data = [ {'city':'Dubai','temperature':33.},\n",
    "    {'city':'London','temperature':12.},\n",
    "    {'city':'San Fransisco','temperature':18.},]\n",
    "dictVector = DictVectorizer()\n",
    "vectorize_data=dictVector.fit_transform(data)\n",
    "print(vectorize_data)\n",
    "print(dictVector.feature_names_)\n",
    "\n",
    "\n",
    "type(vectorize_data),vectorize_data\n",
    "\n",
    "dictVector.inverse_transform(vectorize_data)\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack,vstack\n",
    "a1=coo_matrix([1,1,1,1])\n",
    "b1=coo_matrix([2,2,2,2])\n",
    "c1=coo_matrix([3,3,3,3])\n",
    "rows=[a1,b1,c1]\n",
    "matrix=sparse.vstack(rows)\n",
    "\n",
    "matrix.toarray()\n",
    "\n",
    "# 检查vectorize_ast结束\n",
    "\n",
    "# 检查convert_tf_idf\n",
    "\n",
    "raw_count_data = sparse.load_npz(data_prefix+'_0ae2f34_tf_idf_data.npz')\n",
    "bug_report_id='0ae2f34'\n",
    "feature_names=json.load(open(data_prefix+'_'+bug_report_id+'_tf_idf_index_lookup'))\n",
    "\n",
    "print(raw_count_data.shape)\n",
    "print(len(feature_names))\n",
    "for i,(key,val) in enumerate(feature_names.items()):\n",
    "    if i<=1780:\n",
    "        continue\n",
    "    print(key,val)\n",
    "\n",
    "# 检查convert_tf_idf结束\n",
    "\n",
    "# 检查calculate_feature_3\n",
    "\n",
    "raw_count_data = sparse.load_npz(data_prefix+'_0ae2f34_feature_3_data.npz')\n",
    "bug_report_id='0ae2f34'\n",
    "feature_names=json.load(open(data_prefix+'_'+bug_report_id+'_feature_3_index_lookup'))\n",
    "\n",
    "# feature_names\n",
    "python3 fix_and_augment.py /data/hdj/tracking_buggy_files/swt/swt_base.json /data/hdj/SourceFile/tracking_buggy_files_swt_dataset/ > swt_aug.json\n",
    "\n",
    "print(raw_count_data.shape)\n",
    "print(len(feature_names))\n",
    "for i,(key,val) in enumerate(feature_names.items()):\n",
    "    if i<=1780:\n",
    "        continue\n",
    "    print(key,val)\n",
    "\n",
    "len(sum(raw_count_data.toarray()))\n",
    "for num in sum(raw_count_data.toarray()):\n",
    "    if num!=0:\n",
    "        print(num)\n",
    "\n",
    "raw_count_data = sparse.load_npz(data_prefix+'_0ae2f34_feature_3_data.npz')\n",
    "\n",
    "# 检查calculate_feature_3结束\n",
    "\n",
    "# 检查save_normalized_fold_dataframes.py\n",
    "\n",
    "file_path = '/data/hdj/tracking_buggy_files/swt/swt_fe06bdb_features.npz'\n",
    "features_data = sparse.load_npz(file_path).tocsr()\n",
    "\n",
    "print(features_data.shape)\n",
    "features_data.toarray()\n",
    "\n",
    "file_path='/data/hdj/tracking_buggy_files/swt/swt_fe06bdb_files'\n",
    "filenames=None\n",
    "with open(file_path, 'r') as f:\n",
    "    files_list = json.load(f)\n",
    "#     return files_list\n",
    "    print(type(files_list))\n",
    "    filenames=files_list\n",
    "\n",
    "df = pd.DataFrame(features_data.todense(), index=filenames)\n",
    "df.columns = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15',\n",
    "                      'f16', 'f17', 'f18', 'f19', 'used_in_fix']\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "swt_normalized_training_fold_0=pd.read_pickle('/data/hdj/tracking_buggy_files/swt/swt_normalized_training_fold_0')\n",
    "\n",
    "print(swt_normalized_training_fold_0.index.get_level_values(0).unique().shape[0])\n",
    "print(swt_normalized_training_fold_0.index.get_level_values(1).unique().shape[0])\n",
    "#虽然每个report都对应200+个文档，但是文档是有重复的，去重后的个数为7186个\n",
    "\n",
    "index=list(swt_normalized_training_fold_0.index.get_level_values(1).unique())\n",
    "\n",
    "index=sorted(index)\n",
    "index[:10],index[7180:]\n",
    "\n",
    "max_level_0=500\n",
    "max_level_1=7186\n",
    "fold=0\n",
    "#将第一维的索引从0开始重新索引\n",
    "swt_normalized_training_fold_0.index.set_levels(range(fold * max_level_0, (fold + 1) * max_level_0), level=0, inplace=True)\n",
    "# swt_normalized_training_fold_0.index.set_levels(range(fold * max_level_1, (fold + 1) * max_level_1), level=1, inplace=True)\n",
    "\n",
    "swt_normalized_training_fold_0.tail()\n",
    "\n",
    "swt_normalized_training_fold_0.tail()\n",
    "\n",
    "swt_normalized_training_fold_0.index.set_levels(range(fold * max_level_1, (fold + 1) * max_level_1), level=1, inplace=True)\n",
    "\n",
    "swt_normalized_training_fold_0.loc['1839d9b'].shape\n",
    "\n",
    "print(swt_normalized_training_fold_0.shape)\n",
    "swt_normalized_training_fold_0.head()\n",
    "\n",
    "swt_normalized_training_fold_0[swt_normalized_training_fold_0['used_in_fix']==1].shape\n",
    "\n",
    "type(swt_normalized_training_fold_0.loc['1839d9b'])\n",
    "\n",
    "swt_normalized_training_fold_0.loc['b067e67']\n",
    "\n",
    "swt_normalized_training_fold_0.shape\n",
    "swt_normalized_training_fold_0[swt_normalized_training_fold_0['used_in_fix']==1]\n",
    "swt_normalized_training_fold_0.head(10)\n",
    "\n",
    "swt_normalized_training_fold_0.tail()\n",
    "\n",
    "df[df['used_in_fix']==1]\n",
    "\n",
    "# 检查save_normalized_fold_dataframes.py结束\n",
    "\n",
    "src_addresses = glob.glob(str('/data/hdj/tracking_buggy_files/swt') + '/**/*.npz', recursive=True)\n",
    "\n",
    "len(src_addresses),len(size)\n",
    "size1=[v[0] for v in size]\n",
    "\n",
    "max(size1),min(size1),sum(size1),len(src_addresses),sum(size1)/len(src_addresses)\n",
    "\n",
    "raw_count_data.toarray()\n",
    "\n",
    "size=[]\n",
    "for file in src_addresses:\n",
    "    raw_count_data = sparse.load_npz(file)\n",
    "    print(raw_count_data.shape)\n",
    "    size.append(raw_count_data.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "data_filename_memmap='/data/hdj/tracking_buggy_files/joblib_memmap_tomcat/tomcat/data_memmap'\n",
    "fold_number, fold_testing, fold_training = load(data_filename_memmap, mmap_mode=\"r\")\n",
    "\n",
    "fold_number,type(fold_testing),type(fold_training)\n",
    "\n",
    "for i,(key ,val) in enumerate(fold_training.items()):\n",
    "    if i<=1:\n",
    "#         break\n",
    "        continue\n",
    "    print(key)\n",
    "    print(type(val),val.shape)\n",
    "    print(val)\n",
    "\n",
    "for i,(key ,val) in enumerate(fold_testing.items()):\n",
    "    if i<=1:\n",
    "#         break\n",
    "        continue\n",
    "    print(key)\n",
    "    print(type(val),val.shape)\n",
    "    print(val)\n",
    "\n",
    "print(cp.get_corpus()['2'])\n",
    "\n",
    "corpus=cp.get_corpus()\n",
    "\n",
    "for i,(key,val) in enumerate(corpus.items()):\n",
    "    print(key,val)\n",
    "    if i==2:\n",
    "        break\n",
    "\n",
    "#将数据组织成dict格式\n",
    "#query {bug_id:{'summary':string,'description':string,'fixed_files':string}}\n",
    "#code {code_path:string}\n",
    "query=dict()\n",
    "\n",
    "with open('/data/hdj/SourceFile/data/sourceFile_aspectj/bugreport.txt','r',encoding='utf-8') as f_in:\n",
    "    for i, line in enumerate(f_in) :\n",
    "        if i==0:\n",
    "            continue\n",
    "#         print(line.strip().split('\\t'))\n",
    "        contents=line.strip().split('\\t')\n",
    "        bug_id=contents[1]\n",
    "        summary=contents[2]\n",
    "        description=contents[3]\n",
    "        #处理files，添加commit\n",
    "        fixed_list=[]\n",
    "        for file_path in contents[9].split(' '):\n",
    "            if ':' in file_path:\n",
    "                continue\n",
    "#             str_list=file_path.split('/')\n",
    "#             str_list[-1]=contents[7]+' '+str_list[-1]\n",
    "# #             print(str_list)\n",
    "#             new_string='/'.join(str_list)\n",
    "#             fixed_list.append(new_string)\n",
    "            fixed_list.append(file_path)\n",
    "        query[bug_id]={'summary':summary,'description':description,'fixed_files':fixed_list}\n",
    "#         print(query)\n",
    "#         break\n",
    "\n",
    "# query\n",
    "\n",
    "def _split_camelcase(tokens):\n",
    "\n",
    "    # Copy tokens\n",
    "    returning_tokens = tokens[:]\n",
    "    for token in tokens:\n",
    "        split_tokens = re.split(fr'[{string.punctuation}]+', token)\n",
    "        # If token is split into some other tokens\n",
    "        if len(split_tokens) > 1:\n",
    "            returning_tokens.remove(token)\n",
    "            # Camel case detection for new tokens\n",
    "            for st in split_tokens:\n",
    "                camel_split = inflection.underscore(st).split('_')\n",
    "                if len(camel_split) > 1:\n",
    "                    returning_tokens.append(st)\n",
    "                    returning_tokens += camel_split\n",
    "                else:\n",
    "                    returning_tokens.append(st)\n",
    "        else:\n",
    "            camel_split = inflection.underscore(token).split('_')\n",
    "            if len(camel_split) > 1:\n",
    "                returning_tokens += camel_split\n",
    "\n",
    "    return returning_tokens\n",
    "\n",
    "\n",
    "\n",
    "split_tokens = re.split(fr'[{string.punctuation}]+', 'java.lang.VerifyError')\n",
    "\n",
    "split_tokens\n",
    "\n",
    "\n",
    "\n",
    "query['251326']\n",
    "\n",
    "#准备code\n",
    "aspectj_test=pd.read_csv('/data/hdj/SourceFile/data/sourceFile_aspectj/aspectj_test_pos_neg.csv')\n",
    "aspectj_test.head()\n",
    "\n",
    "#queries准备完成\n",
    "queries=[]\n",
    "bugid_set=set()\n",
    "for bug_id in aspectj_test['id'].values:\n",
    "#     print(key,val)\n",
    "    if bug_id in bugid_set:\n",
    "        continue\n",
    "#     if bug_id==256400:\n",
    "    bugid_set.add(bug_id)\n",
    "    summary=query[str(bug_id)]['summary']\n",
    "    summary_list=_split_camelcase(nltk.wordpunct_tokenize(summary))\n",
    "    summary_list = [x for x in summary_list if x != '']\n",
    "    queries.append(summary_list)\n",
    "\n",
    "#         break\n",
    "# print(queries[:2])\n",
    "# print(queries)\n",
    "\n",
    "len(queries)\n",
    "\n",
    "aspectj_root_path='/data/hdj/SourceFile/tracking_buggy_files_aspectj_dataset/'\n",
    "# path_set=set()\n",
    "# for path in aspectj_test['path'].values:\n",
    "#     path_set.add(path)\n",
    "\n",
    "def code_pre(outstring):\n",
    "    #主要目的是去除所有的注释 ，import， 替换回车换行符 ，过滤掉词长度小于3，最终返回数据格式是list\n",
    "    m = re.compile(r'/\\*.*?\\*/', re.S)\n",
    "    outtmp = re.sub(m, '', outstring)\n",
    "    outstring = outtmp\n",
    "    m = re.compile(r'//.*')\n",
    "    outtmp = re.sub(m, '', outstring)\n",
    "    outstring = outtmp\n",
    "    m=re.compile(r'import.*')\n",
    "    outtmp=re.sub(m,'',outstring)\n",
    "    outstring = outtmp\n",
    "    m = re.compile(r'package.*')\n",
    "    outtmp = re.sub(m, '', outstring)\n",
    "    outstring = outtmp\n",
    "    m = re.compile(r'#.*')\n",
    "    outtmp = re.sub(m, ' ', outstring)\n",
    "    outstring = outtmp\n",
    "    for char in ['\\r\\n', '\\r', '\\n']:\n",
    "        outstring = outstring.replace(char, ' ')\n",
    "    list_str=outstring.split()\n",
    "    list_str=[t for t in list_str if len(t)>=3]\n",
    "    # outstring = ' '.join(list_str)\n",
    "    return list_str\n",
    "\n",
    "src_addresses = glob.glob(str('/data/hdj/SourceFile/tracking_buggy_files_aspectj_dataset') + '/**/*.java', recursive=True)\n",
    "len(src_addresses)\n",
    "\n",
    "\n",
    "reg='/data/hdj/SourceFile/tracking_buggy_files_aspectj_dataset/'\n",
    "'/data/hdj/SourceFile/tracking_buggy_files_aspectj_dataset/weaver/testsrc/org/aspectj/weaver/CommonsTraceTest.java'.split(reg)\n",
    "\n",
    "path_set=set()\n",
    "reg='/data/hdj/SourceFile/tracking_buggy_files_aspectj_dataset/'\n",
    "for path in src_addresses:\n",
    "    path_set.add(path.split(reg)[1])\n",
    "#     src_addresses[:10]\n",
    "\n",
    "# len(path_set.)\n",
    "# 'asm/src/org/aspectj/asm/internal/JDTLikeHandleProvider.java' in path_set\n",
    "path_filter=[]\n",
    "for i,path in enumerate(path_set):\n",
    "    path_filter.append(path)\n",
    "    if i==1000:\n",
    "        break\n",
    "\n",
    "len(path_set)\n",
    "# 'asm/src/org/aspectj/asm/internal/JDTLikeHandleProvider.java' in path_filter\n",
    "\n",
    "# len(path_set)\n",
    "# path_set=aspectj_test_pos_neg['path'][1000:1000+1000]\n",
    "\n",
    "# path_set\n",
    "corpus=dict()\n",
    "for path in path_set:\n",
    "#     print(path)\n",
    "    code_path=aspectj_root_path+path\n",
    "    rf = open(code_path, 'r', encoding='utf-8', errors='ignore')\n",
    "    data = rf.read()\n",
    "    code = ' '.join(code_pre(data))\n",
    "#     print(code)\n",
    "    code_list=_split_camelcase(nltk.wordpunct_tokenize(code))\n",
    "    code_list = [x for x in code_list if x != '']\n",
    "#     print(code_list)\n",
    "    corpus[path]=code_list\n",
    "#     break\n",
    "\n",
    "len(corpus),type(corpus)\n",
    "\n",
    "# for i,(key,val) in enumerate(corpus.items()):\n",
    "# #     if i==10:\n",
    "# #         break\n",
    "#     print(key)\n",
    "\n",
    "import pickle\n",
    "with open('/data/hdj/SourceFile/data/sourceFile_eclipseUI/corpus.pickle', 'wb') as file:\n",
    "        pickle.dump(corpus, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('/data/hdj/SourceFile/data/sourceFile_eclipseUI/querys.pickle', 'wb') as file:\n",
    "        pickle.dump(queries, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "token=['Bug', '423257', 'LTW', 'java', 'lang', 'VerifyError', 'Bad', 'return', 'type', 'with', 'generics', 'and', 'local', 'variables', '', '', '', '', '', 'verify', 'error', '', '']\n",
    "token.remove('')\n",
    "\n",
    "\n",
    "\n",
    "all_results_df_before[:5]\n",
    "\n",
    "# 检查指标的计算过程，最终找出前50个文件，获得bid fid 与db之间关系\n",
    "\n",
    "import pickle\n",
    "# all_results_df_after=pickle.load(open('/data/hdj/tracking_buggy_files/joblib_memmap_swt/all_results_df_after.pickle', 'rb'))\n",
    "all_results_df_before=pickle.load(open('/data/hdj/tracking_buggy_files/joblib_memmap_swt/all_results_df_before.pickle', 'rb'))\n",
    "\n",
    "# 生成训练数据 pos vs neg\n",
    "\n",
    "194+5713,194/5713\n",
    "\n",
    "#\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "k=10\n",
    "\n",
    "train_fold = defaultdict(list)\n",
    "for i,(bug_report, bug_report_files_dataframe)  in enumerate(all_results_df_before.groupby(level=0, sort=False)):\n",
    "    sorted_df = bug_report_files_dataframe.sort_values(ascending=False, by=['result'])\n",
    "# #     print(sorted_df)\n",
    "#     used_in_fix=sorted_df[sorted_df['used_in_fix']==1.0]\n",
    "#     not_used_in_fix=sorted_df[sorted_df['used_in_fix']==0.0]\n",
    "# #     print(used_in_fix)\n",
    "#     used_in_fix_k=used_in_fix.shape[0]\n",
    "# #     not_used_in_fix_k=k-used_in_fix_k\n",
    "#     not_used_in_fix_k=used_in_fix_k\n",
    "# #     if not_used_in_fix_k<=0:\n",
    "# #         not_used_in_fix_k=used_in_fix_k\n",
    "# #     print(used_in_fix.shape)\n",
    "#     train=pd.concat([used_in_fix,not_used_in_fix[-not_used_in_fix_k:],not_used_in_fix[:not_used_in_fix_k*2]])\n",
    "#     print(train.shape)\n",
    "    train=sorted_df[:50]\n",
    "    train_fold[i//500].append(train)\n",
    "#     print(len(train_fold[i//500]))\n",
    "#     if i==2:\n",
    "#         break\n",
    "\n",
    "# train_fold[0][:3]\n",
    "\n",
    "print(len(train_fold))\n",
    "for key,val in train_fold.items():\n",
    "    print(key,len(val))\n",
    "    data=pd.concat(train_fold[key])\n",
    "    data.to_csv('/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_hard_'+str(key+1)+'.csv',index=True)\n",
    "\n",
    "train_1=pd.read_csv('/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv')\n",
    "train_1.head(10)\n",
    "\n",
    "train_1['used_in_fix'].sum()/train_1.shape[0],train_1['used_in_fix'].sum(),train_1.shape[0]\n",
    "\n",
    "# 生成训练数据 pos vs neg 1:3结束\n",
    "\n",
    "# 生成top-50测试数据\n",
    "\n",
    "#\n",
    "k=50\n",
    "\n",
    "train_fold = defaultdict(list)\n",
    "for i,(bug_report, bug_report_files_dataframe)  in enumerate(all_results_df_before.groupby(level=0, sort=False)):\n",
    "    sorted_df = bug_report_files_dataframe.sort_values(ascending=False, by=['result'])\n",
    "\n",
    "    train_fold[i//500].append(sorted_df[:50])\n",
    "    print(len(train_fold[i//500]))\n",
    "#     if i==2:\n",
    "#         break\n",
    "print(len(train_fold))\n",
    "for key,val in train_fold.items():\n",
    "    print(key,len(val))\n",
    "    data=pd.concat(train_fold[key])\n",
    "    data.to_csv('/data/hdj/tracking_buggy_files/joblib_memmap_swt/test_'+str(key+1)+'.csv',index=True)\n",
    "\n",
    "# #最后平均得分 top10rank, MRRrank, MAPrank : 0.212 0.08386737159603685 0.08130901352903222 用的tracking数据预训练的模型\n",
    "# #最后平均得分 top10rank, MRRrank, MAPrank : 0.308 0.10942885958858604 0.09805982264520696 用的之前预训练保存的模型\n",
    "# #最后平均得分 top10rank, MRRrank, MAPrank : 0.342 0.14929821137611463 0.1260024167896456\n",
    "with open('/data/hdj/tracking_buggy_files/joblib_memmap_swt/0521_sumDes_tokSour_50_rawmodel.txt','r',encoding='utf-8') as f_in:\n",
    "    for i,line in enumerate(f_in):\n",
    "#         print(line)\n",
    "        contents=line.split('<CODESPLIT>')\n",
    "        print(contents[0],contents[1],contents[5],contents[6])\n",
    "        if i==5:\n",
    "            break\n",
    "\n",
    "# 生成top-50测试数据结束\n",
    "\n",
    "data.to_csv('/data/hdj/tracking_buggy_files/joblib_memmap_swt/test.csv')\n",
    "\n",
    "test=pd.read_csv('/data/hdj/tracking_buggy_files/joblib_memmap_swt/test.csv')\n",
    "test.head()\n",
    "\n",
    "# 计算前k个的recall，将前k个文件保存起来开始\n",
    "\n",
    "sum_val=0\n",
    "average_precision_per_bug_report = []\n",
    "reciprocal_ranks = []\n",
    "k_range=range(1, 21)\n",
    "bug_report_number = 0\n",
    "accuracy_at_k = dict.fromkeys(k_range, 0)\n",
    "top_k_list=[]\n",
    "all_pos_list=[]\n",
    "recall_list=[]\n",
    "all_results_df_top_k=[]\n",
    "k=50\n",
    "left_len=[]\n",
    "for i,(bug_report, bug_report_files_dataframe)  in enumerate(all_results_df_before.groupby(level=0, sort=False)):\n",
    "    print('index :',i,bug_report,bug_report_files_dataframe.shape[0])\n",
    "    sum_val+=bug_report_files_dataframe.shape[0]\n",
    "#     print(bug_report_files_dataframe)\n",
    "#     min_fix_result = bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix'] == 1.0]['result'].min()\n",
    "#     bug_report_files_dataframe2 = bug_report_files_dataframe[bug_report_files_dataframe[\"result\"] >= min_fix_result]\n",
    "#     print('取fix得分最小值之前的所有文件 ',bug_report_files_dataframe2.shape,' 过滤前大小 :',bug_report_files_dataframe.shape,\n",
    "#           ' min_fix_result :',min_fix_result)\n",
    "#     sorted_df = bug_report_files_dataframe2.sort_values(ascending=False, by=['result'])\n",
    "    sorted_df = bug_report_files_dataframe.sort_values(ascending=False, by=['result'])\n",
    "    #假如过滤后没有值，直接用原始的dataframe\n",
    "    if sorted_df.shape[0] == 0:\n",
    "        sorted_df = bug_report_files_dataframe.copy().sort_values(ascending=False, by=['result'])\n",
    "#     print(sorted_df)\n",
    "    top_k=sorted_df[:k]\n",
    "    top_k_pos=top_k['used_in_fix'].sum()\n",
    "    all_pos=sorted_df['used_in_fix'].sum()\n",
    "    recall=top_k_pos/all_pos\n",
    "    print('top_k :',top_k_pos,' all :',all_pos,' recall :',recall)\n",
    "    top_k_list.append(top_k_pos)\n",
    "    all_pos_list.append(all_pos)\n",
    "    recall_list.append(recall)\n",
    "    all_results_df_top_k.append(top_k)\n",
    "    left_len.append(top_k.shape[0])\n",
    "#     if i==10:\n",
    "#         break\n",
    "sum(recall_list)/len(recall_list),len(recall_list)\n",
    "#10 (0.6132802407120562, 3645)\n",
    "#20 (0.7098751551130549, 3645)\n",
    "#30 (0.7619124531045822, 3645)\n",
    "#50 (0.8176689635982688, 3645)\n",
    "#60 (0.8375844719269108, 3645)\n",
    "#70 (0.8500262484623857, 3645)\n",
    "#80 (0.8607365838268616, 3645)\n",
    "#90 (0.8704146788592242, 3645)\n",
    "#100 (0.8796385848682644, 3645\n",
    "#150 (0.9100992868124254, 3645)\n",
    "#200 (0.9261051897470557, 3645)\n",
    "#300 (0.9458304773765746, 3645)\n",
    "\n",
    "# sorted(left_len,reverse=True)\n",
    "data=pd.DataFrame()\n",
    "data['len']=left_len\n",
    "data['len'].value_counts()\n",
    "# for key,val in data['len'].value_counts().items():当输出内容太多被折叠时，自己手动打印值\n",
    "#     print(key,val)\n",
    "\n",
    "all_results_df_top_k = pd.concat(all_results_df_top_k)\n",
    "\n",
    "print(all_results_df_top_k.loc['a0c9251'].shape)\n",
    "all_results_df_top_k\n",
    "\n",
    "182250/50\n",
    "\n",
    "# all_results_df_test_fold=dict()\n",
    "# rows=0\n",
    "# file_prefix='/data/hdj/tracking_buggy_files/joblib_memmap_swt/swt'\n",
    "# for i in range(8):\n",
    "#     all_results_df_test_fold[i+1]=all_results_df_top_k.loc[(i+1)*500:(i+2)*500-1]\n",
    "#     all_results_df_test_fold[i+1].to_pickle(file_prefix + '_second_testing_fold_' + str(i+1))\n",
    "#     rows+=all_results_df_test_fold[i+1].shape[0]\n",
    "\n",
    "rows,all_results_df_top_k.shape\n",
    "\n",
    "# 根据bid 和fid获取相应的字符\n",
    "\n",
    "# fold_number, fold_testing, fold_training = load('/data/hdj/tracking_buggy_files/joblib_memmap_swt/swt/data_memmap', mmap_mode=\"r\")\n",
    "\n",
    "raw=swt_testing_fold_1.loc['a0c9251']\n",
    "\n",
    "reset=fold_testing[1].loc[500]\n",
    "\n",
    "# fold_testing[1]\n",
    "\n",
    "#获取bug_report TODO\n",
    "def load_bug_reports(bug_report_file_path):\n",
    "    \"\"\"load bug report file (the one generated from xml)\"\"\"\n",
    "    with open(bug_report_file_path) as bug_report_file:\n",
    "        bug_reports = json.load(bug_report_file)\n",
    "        return bug_reports\n",
    "bug_report_file_path='/data/hdj/tracking_buggy_files/swt.json'\n",
    "\n",
    "bug_reports = load_bug_reports(bug_report_file_path)\n",
    "#这个是原始的report ，我还可以获取经过分词的report\n",
    "bug_report_id='bae8b23'\n",
    "current_bug_report = bug_reports[bug_report_id]['bug_report']\n",
    "current_bug_report\n",
    "\n",
    "# English stop words\n",
    "stop_words = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself',\n",
    "'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',\n",
    "'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
    "'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
    "'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\n",
    "'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
    "'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
    "'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
    "'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
    "'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm',\n",
    "'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn',\n",
    "'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn',\n",
    "'weren', 'won', 'wouldn', 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p',\n",
    "'q', 'u', 'v', 'w', 'x', 'z', 'us'])\n",
    "\n",
    "# Java language keywords\n",
    "java_keywords = set(['abstract', 'assert', 'boolean', 'break', 'byte', 'case',\n",
    "'catch', 'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n",
    "'else', 'enum', 'extends', 'false', 'final', 'finally', 'float', 'for', 'goto',\n",
    "'if', 'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n",
    "'native', 'new', 'null', 'package', 'private', 'protected', 'public', 'return',\n",
    "'short', 'static', 'strictfp', 'super', 'switch', 'synchronized', 'this',\n",
    "'throw', 'throws', 'transient', 'true', 'try', 'void', 'volatile', 'while'])\n",
    "def clean_string_report(string):\n",
    "    outtmp=re.findall(r'[\\w\"]+|[.,!?;{}:()\\+\\-\\*\\/=><\"]',string)\n",
    "    outtmp=[token for token in outtmp if token not in stop_words]\n",
    "    outtmp= [token for token in outtmp if token not in java_keywords]\n",
    "    outtmp=[token for token in outtmp if token.isalnum() ]\n",
    "    return ' '.join(outtmp)\n",
    "clean_string_report(current_bug_report['summary'])\n",
    "clean_string_report(current_bug_report['description'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3 0.57\n",
    "\n",
    "outstring=current_bug_report['description']\n",
    "for char in ['\\r\\n', '\\r', '\\n']:\n",
    "        outstring = outstring.replace(char, ' ')\n",
    "\n",
    "#获取code信息\n",
    "# data_prefix='/data/hdj/tracking_buggy_files/tomcat/tomcat'\n",
    "# ast_cache_db = UnQLite(data_prefix+\"_ast_cache_collection_db\")\n",
    "swt_ast_cache_collection_db=UnQLite(\"/data/hdj/tracking_buggy_files/swt/swt_ast_cache_collection_db\",\n",
    "                                             flags=0x00000100 | 0x00000001)\n",
    "# swt_ast=[item for item in swt_ast_cache_collection_db]\n",
    "\n",
    "# for key,val in tomcat_ast_file.items():\n",
    "#     print(key,type(val),len(val))\n",
    "\n",
    "# swt_ast[0]\n",
    "swt_ast_file = pickle.loads(swt_ast_cache_collection_db['13141282f571747cbdb288e7b94787736c724819'])\n",
    "# swt_ast_file\n",
    "for key,val in swt_ast_file.items():\n",
    "    print(key,type(val),len(val))\n",
    "\n",
    "\n",
    "# swt_ast_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# method_top_k\n",
    "\n",
    "# 生成PLI需要的数据 采用set去重 只取6个tokenized内容\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "def mergeTokenziedSource(tokenizedSource):#dict 类型\n",
    "    return ' '.join(tokenizedSource.keys())\n",
    "removed = u'!\"#%&\\'()*+,-./:;<=>?@[\\]^_`{|}~1234567890'\n",
    "utf_translate_table = dict((ord(char), u' ') for char in removed)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text, stemmer):\n",
    "    sanitized_text = text.translate(utf_translate_table)\n",
    "    tokens = wordpunct_tokenize(sanitized_text)\n",
    "    all_tokens = []\n",
    "    for token in tokens:\n",
    "        additional_tokens = camel_case_split(token)\n",
    "        if len(additional_tokens)>1:\n",
    "            for additional_token in additional_tokens:\n",
    "                all_tokens.append(additional_token)\n",
    "        all_tokens.append(token)\n",
    "    return Counter([stemmer.stem(token) for token in all_tokens if token.lower() not in stop_words])\n",
    "def convert_dict2string_set(dict_list):\n",
    "    counter_list = set()\n",
    "    for dict_item in dict_list:\n",
    "        for k,v in dict_item.items():\n",
    "            counter_list.add(k)\n",
    "#     print('len dict_set :',len(dict_list),'merge len :',len(counter_list))\n",
    "    return ' '.join(counter_list)\n",
    "def convert_dict2string_list(dict_list):\n",
    "    counter_list = []\n",
    "    for dict_item in dict_list:\n",
    "        for k,v in dict_item.items():\n",
    "            counter_list.append(k)\n",
    "#     print('len dict_list :',len(dict_list),'merge len :',len(counter_list))\n",
    "    return ' '.join(counter_list)  \n",
    "def clean_string(string):\n",
    "    m = re.compile(r'/\\*.*?\\*/', re.S)\n",
    "    outstring = re.sub(m, '', string)\n",
    "    m = re.compile(r'//.*')\n",
    "    outtmp = re.sub(m, '', outstring)\n",
    "    for char in ['\\r\\n', '\\r', '\\n']:\n",
    "        outtmp = outtmp.replace(char, ' ')\n",
    "    outtmp=' '.join(outtmp.split())\n",
    "    outtmp=re.findall(r'[\\w\"]+|[.,!?;{}:()\\+\\-\\*\\/=><\"]',outtmp)\n",
    "    return ' '.join(outtmp)\n",
    "def get_method_top_k(choosed_methods:list,k=50):\n",
    "    method_list=[]\n",
    "    for string_bef in choosed_methods:\n",
    "#         print('string_bef :',string_bef)\n",
    "        string=clean_string(string_bef)\n",
    "        if '{'  in string and '}'in string :\n",
    "            method_list.append(string)\n",
    "#     print('函数体的方法数量: ',len(method_list))\n",
    "    num=len(method_list)\n",
    "    method_list = sorted(method_list,key = lambda string:len(string.split()),reverse=True)\n",
    "#     print(len(method_list))\n",
    "#     for method in method_list:\n",
    "#         print(method)\n",
    "    return method_list[:k],num\n",
    "method_top_k,num=get_method_top_k(swt_ast_file['methodContent'],10)  \n",
    "num\n",
    "\n",
    "\n",
    "train_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv' \n",
    "train_k=pd.read_csv(train_k_path)\n",
    "train_k.set_index(['bid','fid'], inplace = True)\n",
    "train_k.head(10)\n",
    "all_ast_index=list(train_k.index.get_level_values(1).unique())\n",
    "method_len=[]\n",
    "for ast_index in all_ast_index:\n",
    "#     print(ast_index)\n",
    "    ast_file = pickle.loads(swt_ast_cache_collection_db[ast_index])\n",
    "    print('method len',len(ast_file['methodContent']))\n",
    "    method_top_k=get_method_top_k(ast_file['methodContent'],10)  \n",
    "    print('method_top_k :',len(method_top_k))\n",
    "    method_len.append(len(method_top_k))\n",
    "    \n",
    "\n",
    "statis(method_len,'method_len')\n",
    "\n",
    "# with open('/data/hdj/BERT_PLI/train_1_tokenzied2_15_stage2.json','r',encoding='utf-8') as f_in:\n",
    "#     for i,line in enumerate(f_in):\n",
    "#         line=eval(line)\n",
    "#         print(len(line['c_paras']))\n",
    "#         if i==100:\n",
    "#             break\n",
    "# for ast_index in all_ast_index:\n",
    "#     print(ast_index)\n",
    "#     ast_file = pickle.loads(swt_ast_cache_collection_db[ast_index])\n",
    "#     print(ast_file['methodContent'])\n",
    "#     res=get_method_top_k(ast_file['methodContent'],10)\n",
    "#     print(len(res))\n",
    "# swt_ast_cache_collection_db\n",
    "\n",
    "\n",
    "def generate_pli(train_k_path,out_path):\n",
    "    #生成PLI需要的数据集\n",
    "    task1_list=[]\n",
    "    #train_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv' \n",
    "    #out_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.txt'\n",
    "    train_k=pd.read_csv(train_k_path)\n",
    "    train_k.set_index(['bid','fid'], inplace = True)\n",
    "    train_k.head(10)\n",
    "    #取bid所对应的report\n",
    "    bid_list=list(train_k.index.get_level_values(0).unique())\n",
    "    len(bid_list)\n",
    "    summarys=[]\n",
    "    descriptions=[]\n",
    "    for bug_report_id in bid_list:\n",
    "        current_bug_report = bug_reports[bug_report_id]['bug_report']\n",
    "        summarys.append(current_bug_report['summary'])\n",
    "        descriptions.append(current_bug_report['description'])\n",
    "    report_dataFrame=pd.DataFrame({'summary':summarys,'description':descriptions},index=bid_list)\n",
    "    report_dataFrame.index.names=['bid']\n",
    "    def remove_twoHeadWord(string):\n",
    "        contents=string.split(' ')[2:]\n",
    "        return ' '.join(contents)\n",
    "    report_dataFrame.fillna(\"\",inplace=True)\n",
    "    report_dataFrame['summary']=report_dataFrame['summary'].apply(remove_twoHeadWord)\n",
    "    report_dataFrame['summary']=report_dataFrame['summary'].apply(clean_string_report)\n",
    "    report_dataFrame['description']=report_dataFrame['description'].apply(clean_string_report)\n",
    "    report_dataFrame.head()\n",
    "    all_ast_index=list(train_k.index.get_level_values(1).unique())\n",
    "    type(all_ast_index)\n",
    "#     all_ast_file_tokenizedSource=[]\n",
    "    all_ast_file_tokenizedMethods=[]\n",
    "    all_ast_file_tokenizedClassNames =[]\n",
    "    all_ast_file_tokenizedMethodNames=[]\n",
    "    all_ast_file_tokenizedVariableNames=[]\n",
    "    all_ast_file_tokenizedComments =[]\n",
    "    all_ast_file_methods=[]\n",
    "    for ast_index in all_ast_index:\n",
    "    #     print(ast_index)\n",
    "        ast_file = pickle.loads(swt_ast_cache_collection_db[ast_index])\n",
    "#         all_ast_file_tokenizedSource.append(convert_dict2string_set([ast_file['tokenizedSource']]))\n",
    "        all_ast_file_tokenizedMethods.append(convert_dict2string_set(ast_file['tokenizedMethods']))\n",
    "        top_k_method,num=get_method_top_k(ast_file['methodContent'],10)\n",
    "        print('total method num :',len(ast_file['methodContent']),' 有方法体的函数数量 :',num,' choosed method num :',len(top_k_method))\n",
    "        all_ast_file_methods.append(top_k_method )\n",
    "        all_ast_file_tokenizedClassNames.append(convert_dict2string_set(ast_file['tokenizedClassNames']))\n",
    "        all_ast_file_tokenizedMethodNames.append(convert_dict2string_set(ast_file['tokenizedMethodNames']))\n",
    "        all_ast_file_tokenizedVariableNames.append(convert_dict2string_set(ast_file['tokenizedVariableNames']))\n",
    "        all_ast_file_tokenizedComments.append(convert_dict2string_set(ast_file['tokenizedComments']))\n",
    "    # all_ast_index\n",
    "    all_ast_index_dataframe=pd.DataFrame({'all_ast_file_methods':all_ast_file_methods,'tokenizedMethods':all_ast_file_tokenizedMethods,\n",
    "                                          'tokenizedClassNames':all_ast_file_tokenizedClassNames,'tokenizedMethodNames':all_ast_file_tokenizedMethodNames,\n",
    "                                          'tokenizedVariableNames':all_ast_file_tokenizedVariableNames,'tokenizedComments':all_ast_file_tokenizedComments},index=all_ast_index)\n",
    "    # print(all_ast_index_dataframe.head())\n",
    "    print(all_ast_index_dataframe.shape)\n",
    "    # all_ast_index_dataframe.set_index([\"ast_index\"], inplace=True)\n",
    "    # all_ast_index_dataframe\n",
    "    all_ast_index_dataframe.index.names=['fid']\n",
    "    all_ast_index_dataframe.head(2)\n",
    "    #开始连接字段\n",
    "    all_methodContent_index_dataframe=all_ast_index_dataframe.join(train_k,how='inner')\n",
    "    all_methodContent_index_dataframe.head()\n",
    "    all_report_methodContent_index_dataframe=report_dataFrame.join(all_methodContent_index_dataframe,how='inner')\n",
    "    all_report_methodContent_index_dataframe.head()\n",
    "    \n",
    "    task1_list=[]\n",
    "    all_report_methodContent_index_dataframe.fillna(\"\",inplace=True)\n",
    "    sum_label=0\n",
    "    for i,(index, row) in enumerate(all_report_methodContent_index_dataframe.iterrows()):\n",
    "    #     print(index[0],index[1],row['summary'],row['description'],row['methodContent'],row['used_in_fix'])\n",
    "#         if len(row['tokenizedSource'])==0:\n",
    "#             continue\n",
    "        guid='_'.join([index[0],index[1]])\n",
    "        \n",
    "#         q_paras=[mergeTokenziedSource(tokenize(row['summary'],stemmer)),mergeTokenziedSource(tokenize(row['description'],stemmer))]\n",
    "        q_paras=[row['summary'],row['description']]\n",
    "        c_paras=[row['tokenizedMethods'],row['tokenizedClassNames'],\n",
    "                 row['tokenizedMethodNames'],row['tokenizedVariableNames'],row['tokenizedComments'],]\n",
    "#         print('all_ast_file_methods :',len(row['all_ast_file_methods']))\n",
    "        c_paras.extend(row['all_ast_file_methods'])\n",
    "#         print(i,'summary :',type(row['summary']),'description :',type(row['description']),'len summary :',len(q_paras.split()),'source :',len(c_paras.split()))\n",
    "        summary_len.append(len(q_paras[0].split()))\n",
    "        description_len.append(len(q_paras[1].split()))\n",
    "#         tokenizedSource_len.append(len(c_paras[0].split()))\n",
    "\n",
    "#         tokenizedMethods_len.append(len(c_paras[1].split()))      \n",
    "#         tokenizedClassNames_len.append(len(c_paras[2].split()))   \n",
    "#         tokenizedMethodNames_len.append(len(c_paras[3].split()))   \n",
    "#         tokenizedVariableNames_len.append(len(c_paras[4].split())) \n",
    "#         tokenizedComments_len.append(len(c_paras[5].split()))\n",
    "        label=int(row['used_in_fix'])\n",
    "        task1=dict()\n",
    "        task1['guid']=guid\n",
    "        task1['q_paras']=q_paras\n",
    "        task1['c_paras']=c_paras\n",
    "        task1['label']=label\n",
    "        task1_list.append(task1)\n",
    "#         if i==10:\n",
    "#             break\n",
    "    print(task1_list[0])\n",
    "    with open(out_path,'w',encoding='utf-8') as f_out:\n",
    "        for task1 in task1_list:\n",
    "            out_line = json.dumps(task1, ensure_ascii=False) + '\\n'\n",
    "            f_out.write(out_line)\n",
    "   \n",
    "summary_len=[]\n",
    "description_len=[]\n",
    "tokenizedSource_len=[]\n",
    "tokenizedMethods_len=[]\n",
    "tokenizedClassNames_len=[]\n",
    "tokenizedMethodNames_len=[]\n",
    "tokenizedVariableNames_len=[]\n",
    "tokenizedComments_len=[]\n",
    "train_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv' \n",
    "train_out_path='/data/hdj/BERT_PLI/train_1_tokenzied2_15_stage2.json' \n",
    "# generate_pli(train_k_path,train_out_path)\n",
    "\n",
    "test_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/test_2.csv' \n",
    "test_out_path='/data/hdj/BERT_PLI/test_2_tokenzied2_15_stage2.json'  \n",
    "generate_pli(test_k_path,test_out_path)\n",
    "\n",
    "# summary_len=[]\n",
    "def statis(sourceLen,name):#list\n",
    "    a = np.array(sourceLen)\n",
    "    print(name,'code 的 10%', np.percentile(a, 10))\n",
    "    print('20 :', np.percentile(a, 20))\n",
    "    print('30 :', np.percentile(a, 30))\n",
    "    print('40 :', np.percentile(a, 40))\n",
    "    print('50 :', np.median(a))\n",
    "    print('60 :', np.percentile(a, 60))\n",
    "    print('70 :', np.percentile(a, 70))\n",
    "    print('80 :', np.percentile(a, 80))\n",
    "    print('90 :', np.percentile(a, 90))\n",
    "statis(summary_len,'summary_len')\n",
    "statis(description_len,'description_len')\n",
    "statis(tokenizedSource_len,'tokenizedSource_len')\n",
    "statis(tokenizedMethods_len,'tokenizedMethods_len')\n",
    "statis(tokenizedClassNames_len,'tokenizedClassNames_len')\n",
    "statis(tokenizedMethodNames_len,'tokenizedMethodNames_len')\n",
    "statis(tokenizedVariableNames_len,'tokenizedVariableNames_len')\n",
    "statis(tokenizedComments_len,'tokenizedComments_len')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert_dict2string_list([swt_ast_file['tokenizedSource']])\n",
    "\n",
    "source\n",
    "\n",
    "# methods=convert_dict2string_list(swt_ast_file['tokenizedMethods'])\n",
    "# methods\n",
    "\n",
    "# methods=convert_dict2string_set(swt_ast_file['tokenizedMethods'])\n",
    "# methods\n",
    "\n",
    "ClassNames=convert_dict2string(swt_ast_file['tokenizedClassNames'])\n",
    "ClassNames\n",
    "\n",
    "# MethodNames=convert_dict2string_list(swt_ast_file['tokenizedMethodNames'])\n",
    "# MethodNames\n",
    "\n",
    "# convert_dict2string_set(swt_ast_file['tokenizedMethodNames'])\n",
    "\n",
    "# VariableNames=convert_dict2string(swt_ast_file['tokenizedVariableNames'])\n",
    "# VariableNames\n",
    "\n",
    "\n",
    "# Comments=convert_dict2string_list(swt_ast_file['tokenizedComments'])\n",
    "# Comments\n",
    "\n",
    "\n",
    "# swt_ast_file['tokenizedComments']\n",
    "\n",
    "convert_dict2string_list(swt_ast_file['tokenizedComments'])\n",
    "\n",
    "swt_ast_file['tokenizedClassNames'],swt_ast_file['classNames'],\n",
    "\n",
    "# swt_ast_file['tokenizedSource']\n",
    "\n",
    "# summary=0\n",
    "# for method in swt_ast_file['tokenizedMethods']:\n",
    "# #     print(len(method))\n",
    "#     summary+=len(method)\n",
    "# print(summary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "import string\n",
    "import inflection\n",
    "import json\n",
    "import subprocess\n",
    "from unqlite import UnQLite\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from skopt import *\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "# swt_testing_fold_1=pd.read_pickle('/data/hdj/tracking_buggy_files/swt/swt_testing_fold_0')\n",
    "swt_all_results_df_before=pd.read_pickle('/data/hdj/tracking_buggy_files/joblib_memmap_swt/all_results_df_before.pickle')\n",
    "# # swt_all_results_df_before\n",
    "# swt_testing_fold_1\n",
    "\n",
    "swt_all_results_df_before.head(10)\n",
    "\n",
    "with open('/data/hdj/BERT_PLI/train_1_tokenzied2_6_stage2.json','r',encoding='utf-8') as f_in:\n",
    "    with open('/data/hdj/BERT_PLI/train_1_tokenzied2_6_stage2_replace.json','w',encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            line=eval(line)\n",
    "#             out=dict()\n",
    "#             out['guid']=line['guid']\n",
    "#             out['q_paras']=line['text_a']\n",
    "#             out['c_paras']=line['text_b']\n",
    "#             out['label']=line['label']\n",
    "            if line['label']==1 or line['label']=='1':\n",
    "                for i in range(10):\n",
    "                    out_line = json.dumps(line, ensure_ascii=False) + '\\n'\n",
    "                    f_out.write(out_line)\n",
    "            else:\n",
    "                out_line = json.dumps(line, ensure_ascii=False) + '\\n'\n",
    "                f_out.write(out_line)\n",
    "\n",
    "# for id in stage2_result.index.unique():\n",
    "#     print(id)\n",
    "\n",
    "# count=0\n",
    "# for id in stage2_result.index.unique():\n",
    "#     bug_report_files_dataframe=stage2_result.loc[id]\n",
    "#     min_fix_result = bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix'] == 1.0]['result'].min()\n",
    "#     bug_report_files_dataframe2 = bug_report_files_dataframe[bug_report_files_dataframe[\"result\"] >= min_fix_result]\n",
    "#     sorted_df = bug_report_files_dataframe2.sort_values(ascending=False, by=['result'])\n",
    "#     print(sorted_df.shape)\n",
    "#     if sorted_df.shape[0]<=20:\n",
    "#         count+=1\n",
    "# print(count)\n",
    "\n",
    "168/500\n",
    "\n",
    "k_range=range(1, 21)\n",
    "average_precision_per_bug_report = []\n",
    "reciprocal_ranks = []\n",
    "# calculate per each query (bug report)\n",
    "accuracy_at_k = dict.fromkeys(k_range, 0)\n",
    "bug_report_number = 0\n",
    "for bug_report, bug_report_files_dataframe in stage2_result.groupby(level=0, sort=False):\n",
    "        min_fix_result = bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix'] == 1.0]['result'].min()\n",
    "        bug_report_files_dataframe2 = bug_report_files_dataframe[bug_report_files_dataframe[\"result\"] >= min_fix_result]\n",
    "        sorted_df = bug_report_files_dataframe2.sort_values(ascending=False, by=['result'])\n",
    "#         print(sorted_df)\n",
    "        precision_at_k = []\n",
    "        # precision per k in range\n",
    "        tmp = sorted_df\n",
    "        a = range(1, tmp.shape[0] + 1)\n",
    "        tmp['position'] = pd.Series(a, index=tmp.index)\n",
    "        precision_at_k = []\n",
    "        # precision per k in range\n",
    "        tmp = sorted_df\n",
    "        a = range(1, tmp.shape[0] + 1)\n",
    "        tmp['position'] = pd.Series(a, index=tmp.index)\n",
    "        print(tmp)\n",
    "        large_k_p = tmp[(tmp['used_in_fix'] == 1.0)]['position'].tolist()\n",
    "#         print('large_k_p :',large_k_p)\n",
    "        unique_results = sorted_df['result'].unique().tolist()\n",
    "        unique_results.sort()\n",
    "#         print('unique_results',unique_results)\n",
    "        for fk in large_k_p:\n",
    "            k = int(fk)\n",
    "            k_largest = unique_results[-k:]\n",
    "\n",
    "            largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "            real_fixes_at_k = (largest_at_k['used_in_fix'] == 1.0).sum()\n",
    "\n",
    "            p = float(real_fixes_at_k) / float(k)\n",
    "            precision_at_k.append(p)\n",
    "#         print('precision_at_k :',precision_at_k)\n",
    "        average_precision = pd.Series(precision_at_k).mean()\n",
    "#         print('average_precision :',average_precision)\n",
    "        \n",
    "        # accuracy\n",
    "        for k in k_range:\n",
    "            k_largest = unique_results[-k:]\n",
    "\n",
    "            largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "            real_fixes_at_k = largest_at_k['used_in_fix'][(largest_at_k['used_in_fix'] == 1.0)].count()\n",
    "            if real_fixes_at_k >= 1:\n",
    "                accuracy_at_k[k] += 1\n",
    "        print('accuracy_at_k :',accuracy_at_k)\n",
    "        \n",
    "        # reciprocal rank\n",
    "        indexes_of_fixes = np.flatnonzero(sorted_df['used_in_fix'] == 1.0)\n",
    "        if indexes_of_fixes.size == 0:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "        else:\n",
    "            first_index = indexes_of_fixes[0]\n",
    "            reciprocal_rank = 1.0 / (first_index + 1)\n",
    "            reciprocal_ranks.append(reciprocal_rank)\n",
    "        # bug number\n",
    "        bug_report_number += 1\n",
    "        print('reciprocal_ranks :',reciprocal_ranks)\n",
    "        if bug_report_number==15:\n",
    "            break\n",
    "\n",
    "# task1_list=[]\n",
    "# for line in res:\n",
    "#     print(line)\n",
    "#     task1=dict()\n",
    "#     task1['id_']=line[0]\n",
    "#     task1['res']=line[1]\n",
    "#     task1_list.append(task1)\n",
    "# with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task2/sort_2_6_train_0524_nolabel.json','w',encoding='utf-8') as f_out:\n",
    "#     for task1 in task1_list:\n",
    "#         out_line = json.dumps(task1, ensure_ascii=False) + '\\n'\n",
    "#         f_out.write(out_line)\n",
    "\n",
    "swt_all_results_df_before.reset_index(level=1, drop=True, inplace=True)\n",
    "\n",
    "swt_all_results_df_before.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# swt_all_results_df_before\n",
    "\n",
    "# calculate_metrics(swt_all_results_df_before)\n",
    "\n",
    "# 使用tokenizedSource summary+description，按照CodeSearch组织数据\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from re import finditer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "def mergeTokenziedSource(tokenizedSource):#dict 类型\n",
    "    return ' '.join(tokenizedSource.keys())\n",
    "removed = u'!\"#%&\\'()*+,-./:;<=>?@[\\]^_`{|}~1234567890'\n",
    "utf_translate_table = dict((ord(char), u' ') for char in removed)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text, stemmer):\n",
    "    sanitized_text = text.translate(utf_translate_table)\n",
    "    tokens = wordpunct_tokenize(sanitized_text)\n",
    "    all_tokens = []\n",
    "    for token in tokens:\n",
    "        additional_tokens = camel_case_split(token)\n",
    "        if len(additional_tokens)>1:\n",
    "            for additional_token in additional_tokens:\n",
    "                all_tokens.append(additional_token)\n",
    "        all_tokens.append(token)\n",
    "    return Counter([stemmer.stem(token) for token in all_tokens if token.lower() not in stop_words])\n",
    "\n",
    "mergeTokenziedSource(tokenize('DirectoryDialog There is no disk in the drive . Please insert a disk into drive Device Harddisk0 DR0 Any operation which brings up a file dialog box rooted at \"My Computer\" tosses up a dialog box with the above error message whenever a harddrive is expanded . Click cancel or try again on dialog ( has cancel , try again , and continue ) twice , and then it opens up the disk , and proceeds normally . If you un - expand a drive , it does the same thing . Then again when you re - expand a drive . Examples : Import - > File System - > click Browse Feature . xml - > Export - > click Browse I have seen this all the way back through M4 . May have existed sooner . Currently using RC3 . Platform : Windows 2K SP3',stemmer))\n",
    "\n",
    "def mergeTokenziedSource(tokenizedSource):#dict 类型\n",
    "    return ' '.join(tokenizedSource.keys())\n",
    "     \n",
    "def generate(train_k_path,out_path):\n",
    "    #生成fine-tune需要的数据集\n",
    "    #train_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv' \n",
    "    #out_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.txt'\n",
    "    train_k=pd.read_csv(train_k_path)\n",
    "    train_k.set_index(['bid','fid'], inplace = True)\n",
    "    train_k.head(10)\n",
    "    #取bid所对应的report\n",
    "    bid_list=list(train_k.index.get_level_values(0).unique())\n",
    "    len(bid_list)\n",
    "    summarys=[]\n",
    "    descriptions=[]\n",
    "    for bug_report_id in bid_list:\n",
    "        current_bug_report = bug_reports[bug_report_id]['bug_report']\n",
    "        summarys.append(current_bug_report['summary'])\n",
    "        descriptions.append(current_bug_report['description'])\n",
    "    report_dataFrame=pd.DataFrame({'summary':summarys,'description':descriptions},index=bid_list)\n",
    "    report_dataFrame.index.names=['bid']\n",
    "    def remove_twoHeadWord(string):\n",
    "        contents=string.split(' ')[2:]\n",
    "        return ' '.join(contents)\n",
    "    report_dataFrame['summary']=report_dataFrame['summary'].apply(remove_twoHeadWord)\n",
    "    report_dataFrame.head()\n",
    "    all_ast_index=list(train_k.index.get_level_values(1).unique())\n",
    "    type(all_ast_index)\n",
    "    all_ast_file_methodContent=[]\n",
    "    for ast_index in all_ast_index:\n",
    "    #     print(ast_index)\n",
    "        ast_file = pickle.loads(swt_ast_cache_collection_db[ast_index])\n",
    "        all_ast_file_methodContent.append(mergeTokenziedSource(ast_file['tokenizedSource']))\n",
    "    # all_ast_index\n",
    "    all_ast_index_dataframe=pd.DataFrame({'tokenizedSource':all_ast_file_methodContent},index=all_ast_index)\n",
    "    # print(all_ast_index_dataframe.head())\n",
    "    print(all_ast_index_dataframe.shape)\n",
    "    # all_ast_index_dataframe.set_index([\"ast_index\"], inplace=True)\n",
    "    # all_ast_index_dataframe\n",
    "    all_ast_index_dataframe.index.names=['fid']\n",
    "    all_ast_index_dataframe.head(2)\n",
    "    #开始连接字段\n",
    "    all_methodContent_index_dataframe=all_ast_index_dataframe.join(train_k,how='inner')\n",
    "    all_methodContent_index_dataframe.head()\n",
    "    all_report_methodContent_index_dataframe=report_dataFrame.join(all_methodContent_index_dataframe,how='inner')\n",
    "    all_report_methodContent_index_dataframe.head()\n",
    "    \n",
    "    task1_list=[]\n",
    "    all_report_methodContent_index_dataframe.fillna(\"\",inplace=True)\n",
    "    def clean_string(string):\n",
    "        outstring_list=re.findall(r'[\\w\"]+|[.,!?;{}:()\\+\\-\\*\\/=><\"]',string)\n",
    "        return ' '.join(outstring_list)\n",
    "    sum_label=0\n",
    "    for i,(index, row) in enumerate(all_report_methodContent_index_dataframe.iterrows()):\n",
    "    #     print(index[0],index[1],row['summary'],row['description'],row['methodContent'],row['used_in_fix'])\n",
    "        if len(row['tokenizedSource'])==0:\n",
    "            continue\n",
    "        guid='_'.join([index[0],index[1]])\n",
    "        \n",
    "#         q_paras=clean_string(row['summary']+' '+row['description'])\n",
    "        q_paras=mergeTokenziedSource(tokenize(row['summary']+' '+row['description'],stemmer))\n",
    "        c_paras=row['tokenizedSource']\n",
    "#         print(i,'summary :',type(row['summary']),'description :',type(row['description']),'len summary :',len(q_paras.split()),'source :',len(c_paras.split()))\n",
    "        summary_len.append(len(q_paras.split()))\n",
    "        source_len.append(len(c_paras.split()))\n",
    "        label=str(int(row['used_in_fix']))\n",
    "        list_all=[label,guid,'hdj',q_paras,c_paras]\n",
    "        task1_list.append('<CODESPLIT>'.join(list_all))\n",
    "#         if i==10:\n",
    "#             break\n",
    "    print(task1_list[0])\n",
    "    with open(out_path,'w',encoding='utf-8') as f_out:\n",
    "        for task1 in task1_list:\n",
    "            f_out.write(task1+'\\n')\n",
    "train_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv' \n",
    "out_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.txt'    \n",
    "summary_len=[]\n",
    "source_len=[]\n",
    "generate(train_k_path,out_path)\n",
    "\n",
    "\n",
    "\n",
    "statis(summary_len)\n",
    "statis(source_len)\n",
    "\n",
    "# summary_len\n",
    "\n",
    "# 按照CodeSearch组织数据结束\n",
    "\n",
    "# 生成fine-tune阶段的数据 为每一个fold都生成训练数据\n",
    "\n",
    "def generate_fine_tune(train_k_path,out_path):\n",
    "    #生成fine-tune需要的数据集\n",
    "    #train_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv' \n",
    "    #out_path='/home/hdj/BERT-PLI-IJCAI2020/examples/task2/data_sample_1.json'\n",
    "    train_k=pd.read_csv(train_k_path)\n",
    "    train_k.set_index(['bid','fid'], inplace = True)\n",
    "    train_k.head(10)\n",
    "    #取bid所对应的report\n",
    "    bid_list=list(train_k.index.get_level_values(0).unique())\n",
    "    len(bid_list)\n",
    "    summarys=[]\n",
    "    descriptions=[]\n",
    "    for bug_report_id in bid_list:\n",
    "        current_bug_report = bug_reports[bug_report_id]['bug_report']\n",
    "        summarys.append(current_bug_report['summary'])\n",
    "        descriptions.append(current_bug_report['description'])\n",
    "    report_dataFrame=pd.DataFrame({'summary':summarys,'description':descriptions},index=bid_list)\n",
    "    report_dataFrame.index.names=['bid']\n",
    "    def remove_twoHeadWord(string):\n",
    "        contents=string.split(' ')[2:]\n",
    "        return ' '.join(contents)\n",
    "    report_dataFrame['summary']=report_dataFrame['summary'].apply(remove_twoHeadWord)\n",
    "    report_dataFrame.head()\n",
    "    all_ast_index=list(train_k.index.get_level_values(1).unique())\n",
    "    type(all_ast_index)\n",
    "    all_ast_file_methodContent=[]\n",
    "    for ast_index in all_ast_index:\n",
    "    #     print(ast_index)\n",
    "        ast_file = pickle.loads(swt_ast_cache_collection_db[ast_index])\n",
    "        all_ast_file_methodContent.append(ast_file['methodContent'])\n",
    "    # all_ast_index\n",
    "    all_ast_index_dataframe=pd.DataFrame({'methodContent':all_ast_file_methodContent},index=all_ast_index)\n",
    "    # print(all_ast_index_dataframe.head())\n",
    "    print(all_ast_index_dataframe.shape)\n",
    "    # all_ast_index_dataframe.set_index([\"ast_index\"], inplace=True)\n",
    "    # all_ast_index_dataframe\n",
    "    all_ast_index_dataframe.index.names=['fid']\n",
    "    all_ast_index_dataframe.head(2)\n",
    "    #开始连接字段\n",
    "    all_methodContent_index_dataframe=all_ast_index_dataframe.join(train_k,how='inner')\n",
    "    all_methodContent_index_dataframe.head()\n",
    "    all_report_methodContent_index_dataframe=report_dataFrame.join(all_methodContent_index_dataframe,how='inner')\n",
    "    all_report_methodContent_index_dataframe.head()\n",
    "    \n",
    "    task1_list=[]\n",
    "    all_report_methodContent_index_dataframe.fillna(\"\",inplace=True)\n",
    "    def clean_string(string):\n",
    "        outstring_list=re.findall(r'[\\w\"]+|[.,!?;{}:()\\+\\-\\*\\/=><\"]',string)\n",
    "        return ' '.join(outstring_list)\n",
    "    sum_label=0\n",
    "    for i,(index, row) in enumerate(all_report_methodContent_index_dataframe.iterrows()):\n",
    "    #     print(index[0],index[1],row['summary'],row['description'],row['methodContent'],row['used_in_fix'])\n",
    "        if len(row['methodContent'])==0:\n",
    "            continue\n",
    "        guid='_'.join([index[0],index[1]])\n",
    "#         print(i,'summary :',type(row['summary']),'description :',type(row['description']))\n",
    "#         q_paras=[clean_string(row['summary']),clean_string(row['description'])]\n",
    "#         c_paras=[clean_string(string) for string in row['methodContent']]\n",
    "        q_paras=[mergeTokenziedSource(tokenize(row['summary'],stemmer)),mergeTokenziedSource(tokenize(row['description'],stemmer))]\n",
    "        c_paras=[mergeTokenziedSource(tokenize(string,stemmer)) for string in row['methodContent']]\n",
    "        \n",
    "        label=int(row['used_in_fix'])\n",
    "\n",
    "        for q in q_paras:\n",
    "            if q==\"\":\n",
    "                continue\n",
    "            for c in c_paras:\n",
    "                if c==\"\":\n",
    "                    continue\n",
    "                task1=dict()\n",
    "\n",
    "                task1['guid']=guid\n",
    "                task1['text_a']=q\n",
    "                task1['text_b']=c\n",
    "                task1['label']=label\n",
    "                sum_label+=label\n",
    "                task1_list.append(task1)\n",
    "    #     print(task1)\n",
    "    #     if i==2:\n",
    "    #         break\n",
    "    print(len(task1_list),sum_label,sum_label/len(task1_list))\n",
    "    task1_list[0]\n",
    "    label=0\n",
    "    label_0=0\n",
    "    for task in task1_list:\n",
    "    #     print(task)\n",
    "        label+=task['label']\n",
    "        if task['label']==0:\n",
    "            label_0+=1\n",
    "    #     break\n",
    "    print(label)\n",
    "    with open(out_path,'w',encoding='utf-8') as f_out:\n",
    "        for task1 in task1_list:\n",
    "            out_line = json.dumps(task1, ensure_ascii=False) + '\\n'\n",
    "            f_out.write(out_line)\n",
    "train_k_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv'\n",
    "out_path='/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1_fine_tune.txt'\n",
    "generate_fine_tune(train_k_path,out_path)\n",
    "\n",
    "train_k=pd.read_csv('/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1.csv')\n",
    "train_k.set_index(['bid','fid'], inplace = True)\n",
    "train_k.head(10)\n",
    "#取bid所对应的report\n",
    "bid_list=list(train_k.index.get_level_values(0).unique())\n",
    "\n",
    "len(bid_list)\n",
    "summarys=[]\n",
    "descriptions=[]\n",
    "for bug_report_id in bid_list:\n",
    "    current_bug_report = bug_reports[bug_report_id]['bug_report']\n",
    "    summarys.append(current_bug_report['summary'])\n",
    "    descriptions.append(current_bug_report['description'])\n",
    "report_dataFrame=pd.DataFrame({'summary':summarys,'description':descriptions},index=bid_list)\n",
    "report_dataFrame.index.names=['bid']\n",
    "def remove_twoHeadWord(string):\n",
    "    contents=string.split(' ')[2:]\n",
    "    return ' '.join(contents)\n",
    "report_dataFrame['summary']=report_dataFrame['summary'].apply(remove_twoHeadWord)\n",
    "report_dataFrame.head()\n",
    "\n",
    "ast_file = pickle.loads(swt_ast_cache_collection_db['ca0faf5d670090c4b5c3f4dccbcedefe28fb6b01'])\n",
    "ast_file\n",
    "\n",
    "all_ast_index=list(train_k.index.get_level_values(1).unique())\n",
    "type(all_ast_index)\n",
    "all_ast_file_methodContent=[]\n",
    "count=0\n",
    "sourceLen=[]\n",
    "for ast_index in all_ast_index:\n",
    "#     print(ast_index)\n",
    "    ast_file = pickle.loads(swt_ast_cache_collection_db[ast_index])\n",
    "    print(ast_index,len(ast_file['tokenizedMethods']),len(ast_file['tokenizedSource']))#TODO\n",
    "    if len(ast_file['tokenizedMethods'])==0:\n",
    "        count+=1\n",
    "    sourceLen.append(len(ast_file['tokenizedSource']))\n",
    "    all_ast_file_methodContent.append(ast_file['methodContent'])\n",
    "print(count)\n",
    "\n",
    "576/len(all_ast_index)\n",
    "\n",
    "\n",
    "# all_ast_index\n",
    "all_ast_index_dataframe=pd.DataFrame({'methodContent':all_ast_file_methodContent},index=all_ast_index)\n",
    "# print(all_ast_index_dataframe.head())\n",
    "print(all_ast_index_dataframe.shape)\n",
    "# all_ast_index_dataframe.set_index([\"ast_index\"], inplace=True)\n",
    "# all_ast_index_dataframe\n",
    "all_ast_index_dataframe.index.names=['fid']\n",
    "all_ast_index_dataframe.head(2)\n",
    "\n",
    "all_ast_index_dataframe['methodContent']\n",
    "\n",
    "#开始连接字段\n",
    "all_methodContent_index_dataframe=all_ast_index_dataframe.join(train_k,how='inner')\n",
    "all_methodContent_index_dataframe.head()\n",
    "\n",
    "all_report_methodContent_index_dataframe=report_dataFrame.join(all_methodContent_index_dataframe,how='inner')\n",
    "all_report_methodContent_index_dataframe.head()\n",
    "\n",
    "sum(all_report_methodContent_index_dataframe['used_in_fix'])/all_report_methodContent_index_dataframe.shape[0]\n",
    "\n",
    "all_report_methodContent_index_dataframe.fillna(\"\",inplace=True)\n",
    "\n",
    "task1_list=[]\n",
    "all_report_methodContent_index_dataframe.fillna(\"\",inplace=True)\n",
    "def clean_string(string):\n",
    "    outstring_list=re.findall(r'[\\w\"]+|[.,!?;{}:()\\+\\-\\*\\/=><\"]',string)\n",
    "    return ' '.join(outstring_list)\n",
    "sum_label=0\n",
    "for i,(index, row) in enumerate(all_report_methodContent_index_dataframe.iterrows()):\n",
    "#     print(index[0],index[1],row['summary'],row['description'],row['methodContent'],row['used_in_fix'])\n",
    "    if len(row['methodContent'])==0:\n",
    "        continue\n",
    "    guid='_'.join([index[0],index[1]])\n",
    "    print(i,'summary :',type(row['summary']),'description :',type(row['description']))\n",
    "    q_paras=[clean_string(row['summary']),clean_string(row['description'])]\n",
    "    c_paras=[clean_string(string) for string in row['methodContent']]\n",
    "    label=int(row['used_in_fix'])\n",
    "    \n",
    "    for q in q_paras:\n",
    "        if q==\"\":\n",
    "            continue\n",
    "        for c in c_paras:\n",
    "            if c==\"\":\n",
    "                continue\n",
    "            task1=dict()\n",
    "            \n",
    "            task1['guid']=guid\n",
    "            task1['text_a']=q\n",
    "            task1['text_b']=c\n",
    "            task1['label']=label\n",
    "            sum_label+=label\n",
    "            task1_list.append(task1)\n",
    "#     print(task1)\n",
    "#     if i==2:\n",
    "#         break\n",
    "\n",
    "\n",
    "print(len(task1_list),sum_label,sum_label/len(task1_list))\n",
    "task1_list[0]\n",
    "label=0\n",
    "label_0=0\n",
    "for task in task1_list:\n",
    "#     print(task)\n",
    "    label+=task['label']\n",
    "    if task['label']==0:\n",
    "        label_0+=1\n",
    "#     break\n",
    "label\n",
    "with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task2/data_sample.json','w',encoding='utf-8') as f_out:\n",
    "    for task1 in task1_list:\n",
    "        out_line = json.dumps(task1, ensure_ascii=False) + '\\n'\n",
    "        f_out.write(out_line)\n",
    "\n",
    "label_0+190582\n",
    "\n",
    "# 生成fine-tune阶段的数据结束\n",
    "\n",
    "bid_list=list(all_results_df_top_k.index.get_level_values(0).unique())\n",
    "len(bid_list)\n",
    "bid_list[0], bug_reports[bid_list[0]]['bug_report']['summary'], bug_reports[bid_list[0]]['bug_report']['description']\n",
    "\n",
    "bug_reports\n",
    "summarys=[]\n",
    "descriptions=[]\n",
    "for bug_report_id in bid_list:\n",
    "    current_bug_report = bug_reports[bug_report_id]['bug_report']\n",
    "    summarys.append(current_bug_report['summary'])\n",
    "    descriptions.append(current_bug_report['description'])\n",
    "report_dataFrame=pd.DataFrame({'summary':summarys,'description':descriptions},index=bid_list)\n",
    "report_dataFrame.index.names=['bid']\n",
    "def remove_twoHeadWord(string):\n",
    "    contents=string.split(' ')[2:]\n",
    "    return ' '.join(contents)\n",
    "report_dataFrame['summary']=report_dataFrame['summary'].apply(remove_twoHeadWord)\n",
    "report_dataFrame.head()\n",
    "\n",
    "# all_results_df_top_k.index\n",
    "\n",
    "bug_reports['9cdad39']['bug_report']\n",
    "\n",
    "# tomcat_ast_file = pickle.loads(tomcat_ast_cache_collection_db['b6437f63a95972f4a2d3de68d4c9d5691c5792fd'])\n",
    "# tomcat_ast_file\n",
    "\n",
    "len(all_ast_index)\n",
    "\n",
    "all_ast_index=list(all_results_df_top_k.index.get_level_values(1).unique())\n",
    "type(all_ast_index)\n",
    "all_ast_file_methodContent=[]\n",
    "for ast_index in all_ast_index:\n",
    "#     print(ast_index)\n",
    "    ast_file = pickle.loads(swt_ast_cache_collection_db[ast_index])\n",
    "    all_ast_file_methodContent.append(ast_file['methodContent'])\n",
    "\n",
    "count=0\n",
    "for method in all_ast_file_methodContent:\n",
    "    if len(method)==0:\n",
    "        count+=1\n",
    "print(count,len(all_ast_file_methodContent))\n",
    "\n",
    "# all_ast_index\n",
    "all_ast_index_dataframe=pd.DataFrame({'methodContent':all_ast_file_methodContent},index=all_ast_index)\n",
    "# print(all_ast_index_dataframe.head())\n",
    "print(all_ast_index_dataframe.shape)\n",
    "# all_ast_index_dataframe.set_index([\"ast_index\"], inplace=True)\n",
    "# all_ast_index_dataframe\n",
    "all_ast_index_dataframe.index.names=['fid']\n",
    "all_ast_index_dataframe.head(2)\n",
    "\n",
    "# all_ast_index_dataframe['methodContent'].loc['6f547898bd6d011af1e621a2c002f10c39856cd4']\n",
    "\n",
    "# all_ast_index_dataframe['methodContent'].loc['b5f5e1a2070ba15a8d5a8cd955de857225e0815b']\n",
    "\n",
    "all_ast_index_dataframe.columns\n",
    "\n",
    "# all_ast_index_dataframe.to_csv('')\n",
    "# tomcat_testing_fold_1.lef\n",
    "# pd.merge(tomcat_testing_fold_1,all_ast_index_dataframe,right_on='ast_index',left_index=True, how='outer')\n",
    "all_methodContent_index_dataframe=all_ast_index_dataframe.join(all_results_df_top_k,how='inner')\n",
    "\n",
    "all_methodContent_index_dataframe.head()\n",
    "\n",
    "report_dataFrame.head()\n",
    "\n",
    "\n",
    "\n",
    "sum(all_report_methodContent_index_dataframe.loc['1839d9b']['used_in_fix'])\n",
    "\n",
    "all_report_methodContent_index_dataframe=report_dataFrame.join(all_methodContent_index_dataframe,how='inner')\n",
    "all_report_methodContent_index_dataframe.head()\n",
    "\n",
    "all_report_methodContent_index_dataframe=all_report_methodContent_index_dataframe[['summary','description','methodContent','used_in_fix']]\n",
    "all_report_methodContent_index_dataframe.head()\n",
    "\n",
    "all_report_methodContent_index_dataframe.fillna(\"\",inplace=True)\n",
    "\n",
    "task1_list=[]\n",
    "def clean_string(string):\n",
    "    outstring_list=re.findall(r'[\\w\"]+|[.,!?;{}:()\\+\\-\\*\\/=><\"]',string)\n",
    "    return ' '.join(outstring_list)\n",
    "for i,(index, row) in enumerate(all_report_methodContent_index_dataframe.iterrows()):\n",
    "#     print(index[0],index[1],row['summary'],row['description'],row['methodContent'],row['used_in_fix'])\n",
    "    task1=dict()\n",
    "    task1['guid']='_'.join([index[0],index[1]])\n",
    "    task1['q_paras']=[clean_string(row['summary']),clean_string(row['description'])]\n",
    "    task1['c_paras']=[clean_string(string) for string in row['methodContent']]\n",
    "    task1['label']=int(row['used_in_fix'])\n",
    "    task1_list.append(task1)\n",
    "#     if i==100:\n",
    "#         break\n",
    "with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task1/case_para_sample.json','w',encoding='utf-8') as f_out:\n",
    "    for task1 in task1_list:\n",
    "        out_line = json.dumps(task1, ensure_ascii=False) + '\\n'\n",
    "        f_out.write(out_line)\n",
    "\n",
    "guids = []\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "token_type_ids = []\n",
    "for i in range(3):\n",
    "    input_ids_item = []\n",
    "    attention_mask_item = []\n",
    "    token_type_ids_item = []\n",
    "    for j in range(2):\n",
    "        input_ids_row = []\n",
    "        attention_mask_row = []\n",
    "        token_type_ids_row = []\n",
    "        for k in range(10):\n",
    "            input_ids_row.append([1,1,1])\n",
    "            attention_mask_row.append([2,2,2])\n",
    "            token_type_ids_row.append([3,3,3])\n",
    "        input_ids_item.append(input_ids_row)\n",
    "        attention_mask_item.append(attention_mask_row)\n",
    "        token_type_ids_item.append(token_type_ids_row)\n",
    "    guids.append('guid')\n",
    "    input_ids.append(input_ids_item)\n",
    "    attention_mask.append(attention_mask_item)\n",
    "    token_type_ids.append(token_type_ids_item)\n",
    "\n",
    "import torch\n",
    "print(guids)\n",
    "print(input_ids)\n",
    "print(attention_mask)\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "attention_mask = torch.LongTensor(attention_mask)\n",
    "token_type_ids = torch.LongTensor(token_type_ids)\n",
    "\n",
    "input_ids.shape\n",
    "\n",
    "bert = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "for k in range(input_ids.shape[0]):\n",
    "    for i in range(0,2,2):\n",
    "        _,lst=bert(input_ids[k,i:i+2].view(-1,10),attention_mask=attention_mask[k,i:i+2].view(-1,10))\n",
    "        \n",
    "\n",
    "_.shape\n",
    "\n",
    "lst.shape\n",
    "\n",
    "# 10.10开始跑的\n",
    "\n",
    "25000/1000,25*10/60\n",
    "\n",
    "for k in range(input_ids.size()[0]):\n",
    "    for i in range(0,2,2):\n",
    "        input_id_tmp=input_ids[k,i:i+2]\n",
    "        print(k,input_id_tmp.shape)\n",
    "        input_id_view=input_id_tmp.view(-1,3)\n",
    "        print(k,input_id_view.shape)\n",
    "\n",
    "with open('/data/hdj/tracking_buggy_files/joblib_memmap_swt/train_1_tokenzied2_6_stage2.json','r',encoding='utf-8') as f_in:\n",
    "    label=0\n",
    "    i=0\n",
    "    for line in f_in:\n",
    "        line=eval(line)\n",
    "        label+=line['label']\n",
    "        i+=1\n",
    "    print(label,i,label/i)\n",
    "\n",
    "# 组装成rnn需要的数据格式\n",
    "\n",
    "\n",
    "label_1=0\n",
    "j=0\n",
    "for key,val in guid_label.items():\n",
    "    j+=1\n",
    "    label_1+=val\n",
    "print(label_1)\n",
    "print(j)\n",
    "print(label_1/j)\n",
    "\n",
    "len(guid_label)\n",
    "# guid_label\n",
    "\n",
    "#这里是在oversample\n",
    "with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task1/embedding_sample.json','r',encoding='utf-8') as f_in:\n",
    "    with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task1/embedding_sample_label.json','w',encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            line_dict=eval(line)\n",
    "    #         for key,val in line.items():\n",
    "            if line_dict['label']==1:\n",
    "                for i in range(10):\n",
    "                    f_out.write(line)\n",
    "            else:\n",
    "                f_out.write(line)\n",
    "\n",
    "\n",
    "#把500个文件分割成20个，看看哪里效果很差 为啥\n",
    "import collections\n",
    "out_path_label='/home/hdj/BERT-PLI-IJCAI2020/result/sort_floss_2_15_train_0525_2hard_1easy_right7model_label.json'\n",
    "bid_list=[]\n",
    "used_in_fix=[]\n",
    "result=[]\n",
    "all_data=collections.defaultdict(list)\n",
    "with open(out_path_label,'r',encoding='utf-8') as f_in:\n",
    "    for i,line in enumerate(f_in):\n",
    "#         line=eval(line)\n",
    "        all_data[i//1000].append(line)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "calculate_metrics(all_data[0])\n",
    "\n",
    "def calculate_metrics(f_in, k_range=range(1, 21)):\n",
    "    for line in f_in:\n",
    "        line=eval(line)\n",
    "        bid_list.append(line['id_'].split('_')[0])\n",
    "        used_in_fix.append(line['label'])\n",
    "        result.append(line['res'][1])\n",
    "    stage2_result=pd.DataFrame()\n",
    "    stage2_result['bid']=bid_list\n",
    "    stage2_result['used_in_fix']=used_in_fix\n",
    "    stage2_result['result']=result\n",
    "    stage2_result.set_index('bid',inplace=True)\n",
    "    stage2_result.shape\n",
    "    verification_df=stage2_result\n",
    "    average_precision_per_bug_report = []\n",
    "    reciprocal_ranks = []\n",
    "    # calculate per each query (bug report)\n",
    "    accuracy_at_k = dict.fromkeys(k_range, 0)\n",
    "    bug_report_number = 0\n",
    "    for bug_report, bug_report_files_dataframe in verification_df.groupby(level=0, sort=False):\n",
    "        print(bug_report)\n",
    "        min_fix_result = bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix'] == 1.0]['result'].min()\n",
    "        bug_report_files_dataframe2 = bug_report_files_dataframe[bug_report_files_dataframe[\"result\"] >= min_fix_result]\n",
    "        sorted_df = bug_report_files_dataframe2.sort_values(ascending=False, by=['result'])\n",
    "        if sorted_df.shape[0] == 0:\n",
    "            sorted_df = bug_report_files_dataframe.copy().sort_values(ascending=False, by=['result'])\n",
    "            # print((bug_report_files_dataframe['used_in_fix'] == 1.0).sum())\n",
    "\n",
    "        precision_at_k = []\n",
    "        # precision per k in range\n",
    "        tmp = sorted_df\n",
    "        a = range(1, tmp.shape[0] + 1)\n",
    "        tmp['position'] = pd.Series(a, index=tmp.index)\n",
    "        \n",
    "        large_k_p = tmp[(tmp['used_in_fix'] == 1.0)]['position'].tolist()\n",
    "        unique_results = sorted_df['result'].unique().tolist()\n",
    "        unique_results.sort()\n",
    "        for fk in large_k_p:\n",
    "            k = int(fk)\n",
    "            k_largest = unique_results[-k:]\n",
    "\n",
    "            largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "            real_fixes_at_k = (largest_at_k['used_in_fix'] == 1.0).sum()\n",
    "\n",
    "            p = float(real_fixes_at_k) / float(k)\n",
    "            precision_at_k.append(p)\n",
    "\n",
    "        # average precision is sum of k precisions divided by K\n",
    "        # K is set of positions of relevant documents in the ranked list\n",
    "        average_precision = pd.Series(precision_at_k).mean()\n",
    "        # average_precision = pd.Series(precision_at_k).sum() / float(large_k)\n",
    "        average_precision_per_bug_report.append(average_precision)\n",
    "\n",
    "        # accuracy\n",
    "        for k in k_range:\n",
    "            k_largest = unique_results[-k:]\n",
    "\n",
    "            largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "            real_fixes_at_k = largest_at_k['used_in_fix'][(largest_at_k['used_in_fix'] == 1.0)].count()\n",
    "            if real_fixes_at_k >= 1:\n",
    "                accuracy_at_k[k] += 1\n",
    "\n",
    "        # reciprocal rank\n",
    "        indexes_of_fixes = np.flatnonzero(sorted_df['used_in_fix'] == 1.0)\n",
    "        if indexes_of_fixes.size == 0:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "        else:\n",
    "            first_index = indexes_of_fixes[0]\n",
    "            reciprocal_rank = 1.0 / (first_index + 1)\n",
    "            reciprocal_ranks.append(reciprocal_rank)\n",
    "        # bug number\n",
    "        bug_report_number += 1\n",
    "\n",
    "        del bug_report, bug_report_files_dataframe\n",
    "\n",
    "    # print(\"average_precision_per_bug_report\", average_precision_per_bug_report)\n",
    "    mean_average_precision = pd.Series(average_precision_per_bug_report).mean()\n",
    "    # print('mean average precision', mean_average_precision)\n",
    "    mean_reciprocal_rank = pd.Series(reciprocal_ranks).mean()\n",
    "    # print('mean reciprocal rank', mean_reciprocal_rank)\n",
    "    print('accuracy_at_k :',accuracy_at_k)\n",
    "    for k in k_range:\n",
    "        accuracy_at_k[k] = accuracy_at_k[k] / bug_report_number\n",
    "        # print('accuracy for k', accuracy_at_k[k], k)\n",
    "    return accuracy_at_k, mean_average_precision, mean_reciprocal_rank\n",
    "\n",
    "# swt_testing_fold_1.rename(index={0:'bid',1:'fid'},  inplace=True)\n",
    "# swt_testing_fold_1.index.names = ['bid','fid']\n",
    "#TODO 增加label\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "guid_label=dict()\n",
    "# with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task1/case_para_sample_8.json','r',encoding='utf-8') as f_in:\n",
    "with open('/data/hdj/BERT_PLI/test_2_tokenzied2_6_stage2.json','r',encoding='utf-8') as f_in:\n",
    "    label=0\n",
    "    i=0\n",
    "    for line in f_in:\n",
    "        line=eval(line)\n",
    "        i+=1\n",
    "        guid_label[line['guid']]=line['label']\n",
    "\n",
    "pred_no_label='/home/hdj/BERT-PLI-IJCAI2020/result/sort_floss_2_15_train_0525_2hard_1easy_right7model_less.json'\n",
    "out_path_label='/home/hdj/BERT-PLI-IJCAI2020/result/sort_floss_2_15_train_0525_2hard_1easy_right7model_less_label.json'\n",
    "with open(pred_no_label,'r',encoding='utf-8') as f_in:\n",
    "    with open(out_path_label,'w',encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            line=eval(line)\n",
    "            line['label']=guid_label[line['id_']]\n",
    "            out_line = json.dumps(line, ensure_ascii=False) + '\\n'\n",
    "            f_out.write(out_line)\n",
    "print(i)\n",
    "bid_list=[]\n",
    "used_in_fix=[]\n",
    "result=[]\n",
    "with open(out_path_label,'r',encoding='utf-8') as f_in:\n",
    "    for line in f_in:\n",
    "        line=eval(line)\n",
    "        bid_list.append(line['id_'].split('_')[0])\n",
    "        used_in_fix.append(line['label'])\n",
    "        result.append(line['res'][1])\n",
    "stage2_result=pd.DataFrame()\n",
    "stage2_result['bid']=bid_list\n",
    "stage2_result['used_in_fix']=used_in_fix\n",
    "stage2_result['result']=result\n",
    "stage2_result.set_index('bid',inplace=True)\n",
    "stage2_result.shape\n",
    "def calculate_metrics(f_in, k_range=range(1, 21)):\n",
    "    for line in f_in:\n",
    "        line=eval(line)\n",
    "        bid_list.append(line['id_'].split('_')[0])\n",
    "        used_in_fix.append(line['label'])\n",
    "        result.append(line['res'][1])\n",
    "    stage2_result=pd.DataFrame()\n",
    "    stage2_result['bid']=bid_list\n",
    "    stage2_result['used_in_fix']=used_in_fix\n",
    "    stage2_result['result']=result\n",
    "    stage2_result.set_index('bid',inplace=True)\n",
    "    stage2_result.shape\n",
    "    verification_df=stage2_result\n",
    "    average_precision_per_bug_report = []\n",
    "    reciprocal_ranks = []\n",
    "    # calculate per each query (bug report)\n",
    "    accuracy_at_k = dict.fromkeys(k_range, 0)\n",
    "    bug_report_number = 0\n",
    "    for bug_report, bug_report_files_dataframe in verification_df.groupby(level=0, sort=False):\n",
    "        print(bug_report)\n",
    "        min_fix_result = bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix'] == 1.0]['result'].min()\n",
    "        bug_report_files_dataframe2 = bug_report_files_dataframe[bug_report_files_dataframe[\"result\"] >= min_fix_result]\n",
    "        sorted_df = bug_report_files_dataframe2.sort_values(ascending=False, by=['result'])\n",
    "        if sorted_df.shape[0] == 0:\n",
    "            sorted_df = bug_report_files_dataframe.copy().sort_values(ascending=False, by=['result'])\n",
    "            # print((bug_report_files_dataframe['used_in_fix'] == 1.0).sum())\n",
    "\n",
    "        precision_at_k = []\n",
    "        # precision per k in range\n",
    "        tmp = sorted_df\n",
    "        a = range(1, tmp.shape[0] + 1)\n",
    "        tmp['position'] = pd.Series(a, index=tmp.index)\n",
    "        \n",
    "        large_k_p = tmp[(tmp['used_in_fix'] == 1.0)]['position'].tolist()\n",
    "        unique_results = sorted_df['result'].unique().tolist()\n",
    "        unique_results.sort()\n",
    "        for fk in large_k_p:\n",
    "            k = int(fk)\n",
    "            k_largest = unique_results[-k:]\n",
    "\n",
    "            largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "            real_fixes_at_k = (largest_at_k['used_in_fix'] == 1.0).sum()\n",
    "\n",
    "            p = float(real_fixes_at_k) / float(k)\n",
    "            precision_at_k.append(p)\n",
    "\n",
    "        # average precision is sum of k precisions divided by K\n",
    "        # K is set of positions of relevant documents in the ranked list\n",
    "        average_precision = pd.Series(precision_at_k).mean()\n",
    "        # average_precision = pd.Series(precision_at_k).sum() / float(large_k)\n",
    "        average_precision_per_bug_report.append(average_precision)\n",
    "\n",
    "        # accuracy\n",
    "        for k in k_range:\n",
    "            k_largest = unique_results[-k:]\n",
    "\n",
    "            largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "            real_fixes_at_k = largest_at_k['used_in_fix'][(largest_at_k['used_in_fix'] == 1.0)].count()\n",
    "            if real_fixes_at_k >= 1:\n",
    "                accuracy_at_k[k] += 1\n",
    "\n",
    "        # reciprocal rank\n",
    "        indexes_of_fixes = np.flatnonzero(sorted_df['used_in_fix'] == 1.0)\n",
    "        if indexes_of_fixes.size == 0:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "        else:\n",
    "            first_index = indexes_of_fixes[0]\n",
    "            reciprocal_rank = 1.0 / (first_index + 1)\n",
    "            reciprocal_ranks.append(reciprocal_rank)\n",
    "        # bug number\n",
    "        bug_report_number += 1\n",
    "\n",
    "        del bug_report, bug_report_files_dataframe\n",
    "\n",
    "    # print(\"average_precision_per_bug_report\", average_precision_per_bug_report)\n",
    "    mean_average_precision = pd.Series(average_precision_per_bug_report).mean()\n",
    "    # print('mean average precision', mean_average_precision)\n",
    "    mean_reciprocal_rank = pd.Series(reciprocal_ranks).mean()\n",
    "    # print('mean reciprocal rank', mean_reciprocal_rank)\n",
    "    print('accuracy_at_k :',accuracy_at_k)\n",
    "    for k in k_range:\n",
    "        accuracy_at_k[k] = accuracy_at_k[k] / bug_report_number\n",
    "        # print('accuracy for k', accuracy_at_k[k], k)\n",
    "    return accuracy_at_k, mean_average_precision, mean_reciprocal_rank\n",
    "\n",
    "calculate_metrics(stage2_result)\n",
    "\n",
    "25000/3\n",
    "\n",
    "# 组装成rnn需要的数据格式 完成\n",
    "\n",
    "# output_data=dict()\n",
    "# import json\n",
    "# with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task1/embedding_sample_0524_test.json','r',encoding='utf-8') as f_in:\n",
    "#     with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task1/embedding_sample_0524_test_label.json','w',encoding='utf-8') as f_out:\n",
    "#         for line in f_in:\n",
    "#             line=eval(line)\n",
    "#             output_data['guid']=line['id_']\n",
    "#             output_data['res']=line['res']\n",
    "#             output_data['label']=guid_label[line['id_']]\n",
    "# #             for key,val in output_data.items():\n",
    "# #                 print(key)\n",
    "# #             print(type(line))\n",
    "#             out_line = json.dumps(output_data, ensure_ascii=False) + '\\n'\n",
    "#             if guid_label==1:\n",
    "#                 for i in range(10):\n",
    "#                     f_out.write(out_line)\n",
    "#             else:\n",
    "#                 f_out.write(out_line)\n",
    "# #             break\n",
    "\n",
    "c_paras_len=[]\n",
    "c_paras_single_len=[]\n",
    "for task1 in task1_list:\n",
    "    c_paras=task1['c_paras']\n",
    "    c_paras_len.append(len(c_paras))\n",
    "    single_len=[]\n",
    "    for para in c_paras:\n",
    "        single_len.append(len(para.split(' ')))\n",
    "    c_paras_single_len.append(single_len)\n",
    "\n",
    "\n",
    "# task1_list[8]\n",
    "\n",
    "# for one in c_paras_single_len:\n",
    "#     print(len(one))\n",
    "\n",
    "a = np.array(c_paras_len)\n",
    "print('code 的 10%', np.percentile(a, 10))\n",
    "print('20 :', np.percentile(a, 20))\n",
    "print('30 :', np.percentile(a, 30))\n",
    "print('40 :', np.percentile(a, 40))\n",
    "print('50 :', np.median(a))\n",
    "print('60 :', np.percentile(a, 60))\n",
    "print('70 :', np.percentile(a, 70))\n",
    "print('80 :', np.percentile(a, 80))\n",
    "print('90 :', np.percentile(a, 90))\n",
    "\n",
    "25000/(60*60)\n",
    "\n",
    "val_count=data['len'].value_counts()\n",
    "\n",
    "val_count.head()\n",
    "\n",
    "task1_list[25000-50]['guid'],task1_list[25000-2]['guid'],task1_list[25000-1]['guid'],task1_list[25000]['guid']\n",
    "\n",
    "with open('/home/hdj/BERT-PLI-IJCAI2020/examples/task1/case_para_sample.json','w',encoding='utf-8') as f_out:\n",
    "    for task1 in task1_list:\n",
    "        out_line = json.dumps(task1, ensure_ascii=False) + '\\n'\n",
    "        f_out.write(out_line)\n",
    "\n",
    "# public boolean isSubtype ( ObjectType type , ObjectType possibleSupertype ) throws ClassNotFoundException { if ( DEBUG_QUERIES ) { System . out . println ( \"isSubtype: check \" + type + \" subtype of \" + possibleSupertype ) ; } if ( type . equals ( possibleSupertype ) ) { if ( DEBUG_QUERIES ) { System . out . println ( \"  ==> yes, types are same\" ) ; } return true ; } ClassDescriptor typeClassDescriptor = DescriptorFactory . getClassDescriptor ( type ) ; ClassDescriptor possibleSuperclassClassDescriptor = DescriptorFactory . getClassDescriptor ( possibleSupertype ) ; return isSubtype ( typeClassDescriptor , possibleSuperclassClassDescriptor ) ; }\n",
    "# public boolean isSubtype(ObjectType type, ObjectType possibleSupertype) throws ClassNotFoundException {\n",
    "#         if (DEBUG_QUERIES) {\n",
    "#             System.out.println(\"isSubtype: check \" + type + \" subtype of \" + possibleSupertype);\n",
    "#         }\n",
    "\n",
    "#         if (type.equals(possibleSupertype)) {\n",
    "#             if (DEBUG_QUERIES) {\n",
    "#                 System.out.println(\"  ==> yes, types are same\");\n",
    "#             }\n",
    "#             return true;\n",
    "#         }\n",
    "#         ClassDescriptor typeClassDescriptor = DescriptorFactory.getClassDescriptor(type);\n",
    "#         ClassDescriptor possibleSuperclassClassDescriptor = DescriptorFactory.getClassDescriptor(possibleSupertype);\n",
    "\n",
    "#         return isSubtype(typeClassDescriptor, possibleSuperclassClassDescriptor);\n",
    "#     }\n",
    "\n",
    "# {\"guid\": \"005_001\", \"q_paras\": [\"Strickland, J. : This is an application for judicial review of two decisions of the Canadian Human Rights Commission (the Commission), both dated August 27, 2014, declining to deal with the Applicant's complaints, pursuant to s 41(1)(a) of the Canadian Human Rights Act , RSC 1985, c H-6 ( CHR Act ), because the Applicant had not exhausted grievance or review procedures otherwise reasonably available to him. The application for judicial review is brought pursuant to s 18.1 of the Federal Courts Act , RSC 1985, c F-7. Background\", \"The Applicant is a member of the Canadian Forces (CF), the Respondent herein as represented by the Attorney General of Canada. He alleges that in 2010, while he was serving in Haiti, his immediate superior officer discriminated against him, largely on racial grounds. In June 2010, the Applicant filed a complaint with the CF Ombudsman who decided not to deal with the case because other avenues of redress had not yet been exhausted.\", \"In August 2010 the Applicant met with his Commanding Officer (CO) and the harassment complaint was discussed. His CO subsequently wrote to him on two occasions in September 2010, requesting additional information so as to determine whether the Applicant's allegations constituted a harassment complaint pursuant to the Harassment Prevention and Resolution Guidelines . No response was received from the Applicant.\", \"By letter of October 5, 2010 counsel for the Applicant advised the CO, amongst other things, that the Applicant did not intend to avail himself of a harassment complaint pursuant to the Department of Nation Defence/CF Harassment Prevention and Resolution policy but reserved his right to make any such complaints in another forum.\", \"In December 2010 the Applicant filed a complaint with the Commission. By decisions dated July 6, 2011 the Commission decided not to deal with the complaint at that time, pursuant to s 41(1)(a) of the CHR Act , as the Applicant ought first to exhaust grievance or review procedures otherwise reasonably available to him. It stated that at the end of the grievance or review process, the Applicant could ask the Commission to reactivate the complaint. The Commission's decisions of July 6, 2011 are not under review.\", \"The Applicant then engaged in the CF Redress of Grievance process. On November 29, 2011 the Commander of Canadian Forces Joint Support Group released terms of reference, thereby convening an administrative investigation into the Applicant's allegations of racist conduct, discrimination and harassment. The Administrative Investigation report of the Initial Authority (IA) was released on April 20, 2012. The Applicant was dissatisfied with the IA decision and requested that his grievance be forwarded for determination by the Final Authority (FA).\", \"On July 4, 2012 the Applicant's grievance was referred to the Canadian Forces Grievance Board (CFGB) which released its findings and recommendations on September 14, 2012. By letter of March 4, 2013, the Chief of the Defence Staff advised that he concurred with the CFGB findings and recommendations, which included that there were important evidentiary omissions and procedural errors that compromised the integrity of the process and resulted in a fatally flawed Administrative Investigation and, therefore, an equally flawed IA decision. The Chief of the Defence Staff also stated that he was concerned that the Applicant had chosen not to avail himself of the CF harassment process and that this had contributed to the CFGB's findings. Given his finding that the IA decision was invalid, the Chief of the Defence Staff concluded that he could not make a properly informed decision on the validity of the allegations and, therefore, that it was imperative that the Applicant initiate the harassment investigation process. The Applicant was given 90 days to do so. The Chief of the Defence Staff stated that when the investigation was complete the Applicant could grieve its outcome if its conclusions and recommendations were not to his satisfaction.\", \"On May 14, 2013 the Applicant submitted a copy of his original grievance, not a harassment complaint. The Responsible Officer (RO) wrote to the Applicant on May 22, 2013, requesting submission of a proper complaint and inclusion of any information necessary to enable the conduct of a thorough and complete investigation of the allegations of harassment. The RO sent a second letter on July 19, 2013. The Applicant did not respond to either letter.\", \"On December 24, 2012, while the grievance process was in progress, the Applicant requested the Commission to reactivate his complaint. While the Commission was considering whether s 41(1)(a) applied to the complaint, the FA decision was issued. The Applicant filed an application for judicial review of the FA on April 24, 2012. A November 4, 2013 Section 40/41 Report recommended that the Commission not deal with the complaint while the judicial review was before this Court. On January 23, 2014, before the Commission decided that matter, the Applicant discontinued his application for judicial review of the FA and asked the Commission to deal with his complaint.\", \"On May 6, 2014 two Section 40/41 Reports were prepared. The first concerned the Applicant's complaint against the CF (file no 20101251) and the second concerned his complaint against his CO (file no 20101252) (collectively, the Section 40/41 Report). Substantially similar Records of Decision Under Section 40/41 were issued by the Commission on August 27, 2014, for both matters (collectively, the Decision). For the purposes of this judicial review, all references shall be to the Section 40/41 Report and decision of the Commission pertaining to the complaint against the CF.\", \"On September 26, 2014 the Applicant filed his application for judicial review of the Commission's Decision. Decision Under Review\", \"The Commission decided not to deal with the complaint pursuant to s 41(1)(a) of the CHR Act , as the Applicant ought to have exhausted grievance or review procedures otherwise reasonably available to him. A decision under s 41(1)(d) of the CHR Act was, therefore, unnecessary. The Commission adopted the conclusion of the May 6, 2014 Section 40/41 Report: The complainant's allegations are serious, and in the public interest. On the other hand, the harassment complaint process was reasonably available to the complainant after the Final Authority decision in 2013. The Final Authority decision included measures to be taken as part of the harassment complaint process in order to address the concerns with timeliness and fairness from the preceding administrative investigation. The complainant was provided multiple opportunities to participate in this harassment complaint process, but he chose not to. He was clearly aware of the inadequacies of the grievance form to initiate a harassment complaint as several attempts were made to obtain the correct information from him. Based on the above, the failure to exhaust that process was attributable to him.\", \"In rendering its decision the Commission reviewed the December 8, 2010 complaint form, the Section 40/41 Report of May 6, 2014 and Supplemental Section 40/41 Report of June 12, 2014, the Applicant's submissions of June 17, 2014, the CF's submissions of July 2, 2014, and cross-disclosure submissions of the CF dated July 25, 2014.\", \"The Section 40/41 Report set out the background to the complaint and summarized the positions of the parties based on their submissions. It noted that, based on the CF's position, it appeared that the remaining alternative redress process, the CF's harassment complaint process, was no longer available to the Applicant because he did not file a formal harassment complaint prior to the deadline set out in the FA. The question to be answered was whether the harassment complaint process was reasonably available to the Applicant and, if so, whether he should have exhausted that procedure. And, if that process ought to have been exhausted, whether the failure to do so was solely attributable to the Applicant in accordance with s 42(2) of the CHR Act .\", \"On the first point, it is expedient to reproduce particular paragraphs of the report as it sets out relevance background information as well as its analysis: Was the harassment complaint process reasonably available to the complainant? 64. Some relevant background information can be found in the \\\"Findings and Recommendations\\\" of the Canadian Forces Grievance Board (\\\"CFGB\\\"), dated September 14, 2012, which was provided by the complainant. In August 2011, after the Commission's decision not to deal with his complaint at that time, the complainant filed a grievance with the respondent. In March 2012, a final report on the Administrative investigation of the complainant's grievance was issued and disclosed to the grievor who provided comments. On April 20, 2012, the Initial Authority decision was rendered, finding \\\"professional shortcomings\\\" on the part of the superior officer but not granting the grievor's requests for apologies, damages, or legal fees. The complainant was unsatisfied with this decision and asked for a decision by the Final Authority. The matter was referred to the CFGB for review and recommendations. 65. The CFGB found that because a harassment investigation did not occur, the Initial Authority did not have adequate information upon which to make his decision, even though he attempted to insert elements of a harassment investigation into the grievance process. This investigation was found to have various defects, such as not all witnesses were interviewed, not all relevant information appeared to be considered, and the fact that the superior officer, alleged to be the harasser, was not provided an opportunity to respond to the investigation. For these reasons, the CFGB recommended that the Final Authority not make a final decision on the matter, but order a new harassment investigation be initiated. It also noted that the respondent's policies should be amended to ensure that harassment complaints are required to be dealt with under the harassment policy prior to resorting to the grievance process, given that neither the CFGB nor the CDS is equipped to conduct its own investigation in such situations. 66. On March 4, 2013, the Final Authority rendered his decision. Referring to the CFGB findings and recommendations, the CDS found that there were \\\" important evidentiary omissions and procedural errors that compromised the integrity of the process and resulted in what the CFGB quite rightly referred to as a 'fatally flawed' administrative investigation and consequently led to an equally flawed IA decision based solely on that investigation.\\\" The CDS determined that due to these errors and the incompleteness of the underlying investigation, he could not render \\\" ...a properly informed decision on the validity and reliability of the allegations... \\\" 67. The CDS also raised concerns that a harassment investigation did not occur. In the Final Authority decision, the CDS stated: Yet, I am troubled by your allegations and most particularly, by the lack of a proper investigation into their validity. As I mentioned, the CF takes this issue very seriously and consequently, it is imperative that you initiate the harassment investigation process by submitting a formal harassment complaint to the CO of 4 CFMCU. You have 90 days from the receipt of this letter to make your submission. The CO of 4 CFMCU shall, upon receipt of your harassment complaint, initiate a harassment investigation into the allegations therein. Let me be very clear on this point: an investigation will not commence until there is a formal harassment complaint submitted to the CO of 4 CFMCU and you commit, in writing, that you will fully participate in such an investigation to ensure that a thorough and impartial analysis may determine the legitimacy of your allegations. If you do not respond within 90 days, there will be no further action taken on your alleged complaints. Given the complexities of this case, compounded by the length of time that has elapsed since the alleged incidents, the CO of the 4 CFMCU might consider engaging a private consulting firm to assist with the investigation. Such corporations specialize in the investigation of harassment cases. Additionally, a monthly situation report will be sent to the Director General Canadian forces [ sic ] Grievance Authority (DGCFGA) Implementation Officer on the status of the forthcoming harassment investigation. When the investigation has been completed, you may grieve its outcome if the conclusions and recommendations are not to your satisfaction. 68. The CDS suggested that a private consulting firm be engaged to investigate the matter. The complainant is of the view that this demonstrates that the respondent is unable to investigate a complaint of this complexity. He suggests that this is further supported by the finding of the CFGB that the initial administrative investigation to be [ sic ] ' fatally flawed '. 69. The lack of procedural fairness in the IA decision was acknowledged in the Final Authority decision. The CDS attempted to address the concerns for procedural fairness and expediency with his recommendations: a) that the matter be dealt with through a formal harassment complaint; and, b) that they hire an outside firm to investigate the matter. 70. The respondent's \\\"Harassment Prevention and Resolution Guidelines\\\" (the Guidelines) outline a complaint process which is overseen by Responsible Officers (ROs) in the respondent's organization. The Guidelines define harassment the same as it is defined under the Act. The Guidelines further provide that other workplace harassment may also be addressed through this process. Once a formal harassment complaint is received, the RO oversees a situational assessment to determine whether or not the complaint is related to harassment - if it is, efforts are made to resolve the matter, identify other possible redress avenues such as mediation, and the appropriate action(s) will be initiated. If mediation is not appropriate, an administrative investigation is to be undertaken by a harassment investigator. The harassment investigator prepares a draft report on which both parties may comment before the final report is completed and submitted to the RO for decision. If any person is not in agreement with the RO's decision, they may file a grievance. Although the harassment and grievance processes do not provide for an independent third party decision-maker, matters dealing with harassment and racist conduct are referred to an independent body (the CFGB, now the MGERC) for findings and recommendation prior to the Final Authority decision being rendered. In addition, a review by the Federal Court is also available. 71. The complainant states that only the Commission will be able to fully deal with all of the matters in his complaint. While the complaint deals with harassment, there are also allegations of adverse differential treatment within the workplace. If the harassment complaint process were still available to him, it is not clear whether it could deal with the allegations with respect to the transfer occurring as a result of his marital status, for example. However, it could have dealt with much [ sic ] of the substantive allegations in the complaint. 72. Timeliness is another consideration with respect to this complaint, which is nearly four years old. It appears that both parties bear some responsibility for the delays in the process. In 2010, the complainant did attempt to speak to his superiors about his allegations, but he did not provide them with the requested information when asked in September 2010. As outlined in paragraph 64, the respondent's process is lengthy, and there appears [ sic ] to have been delays in the processing of the complainant's grievance. 73. In terms of delay, it is not clear that the Commission's process would be more expedient than that of the respondent's internal harassment complaint process, if it were still available. Nonetheless, it appears at this stage, the Commission's process is the only remaining process available to him. 74. The complainant's human rights allegations, if proven, are serious; this is acknowledged by the respondent. The complainant has not provided any information to indicate that he is vulnerable personally, but there is a public interest in investigating allegations of racism in Canadian government institutions such as the Canadian Forces. Although the complainant describes the complaint as one of systemic discrimination, no policy or practice has been identified in the summary of complaint. There is no indication that there is or was any potential for harm to anyone if the respondent's harassment complaint process had been used. 75. The complainant suggests that that [ sic ] requiring him to engage in the harassment complaint process is unfair. Although the complainant's scepticism may be understandable given the results of the IA decision, there is no indication that the respondent's alternate redress procedures overall are so flawed that it could not have dealt with the complainant's allegations within its harassment complaint process. In this instance, the Final Authority made recommendations in order to specifically address the issues that occurred previously, including any potential unfairness and the issue of delay. 76. Having regard for all of the above, it appears that the complainant had another review procedure, the harassment complaint process, reasonably available to him after the Final Authority decision in 2013.\", \"On the question of whether the failure to exhaust the CF's harassment complaint process was attributable to the Applicant and not another, as set out in s 42(2) of the CHR Act , the Section 40/41 Report noted that the history of the complaint illustrated that much of the discord related to the appropriate process for addressing a complaint that raises a variety of issues such as was the case in this matter, which involved allegations of harassment, abuse of authority and adverse differential treatment.\", \"Ultimately, however, it concluded that: 80. The complainant acknowledged the internal harassment complaint process. After the Final Authority decision, the complainant sent in his grievance form in lieu of a harassment complaint. In his letter to the complainant dated May 22, 2013, the RO responded to the complainant to explain that the grievance form did not provide the information necessary to initiate a harassment complaint. The letter described what was required in order to file a harassment complaint and also offered assistance to the complainant. On July 19, 2013, the RO sent a follow-up letter to the complainant requesting additional information, but the complainant did not provide it. Therefore, while he acknowledged this process, he refused to participate by not providing the appropriate material in order for it to have any possibility of success. This is solely attributable to the complainant. Relevant Legislation\", \"The sole issue in this application for judicial review is whether the Commission's interpretation and application of s 41(1)(a) were reasonable.\", \"Jurisprudence has previously held that the appropriate standard of review with respect to decisions of the Commission not to deal with a complaint pursuant to s 41(1) of the CHR Act is reasonableness ( FRAGMENT_SUPPRESSED).\", \"Reasonableness is concerned with the existence of justification, transparency and intelligibility within the decision-making process. It is also concerned with whether the decision falls within a range of possible, acceptable outcomes which are defensible in respect of the facts and the law ( FRAGMENT_SUPPRESSED). Preliminary Issue\", \"A preliminary issue that has been raised by the Respondent is the admissibility of the affidavit of Nathalie Bui sworn October 24, 2014 (the Bui Affidavit). It attaches what the affiant describes as the entire record of official correspondence between the Commission and the Applicant concerning files 20101251 and 20101252.\", \"As noted by the Respondent, this is the Applicant's second attempt to place this material before the Court. The Applicant previously brought a motion, pursuant to Rule 317 of the Federal Courts Rules , SOR/98-106 [ FC Rules ], seeking to have the Commission's entire file produced as part of the Certified Tribunal Record (CTR).\", \"By Order dated November 26, 2014, Prothonotary Tabib refused that request on the basis that the Applicant was not entitled to further production, noting, in particular, that the Applicant had not identified any document that might have been considered by the Commission, or might be relevant to the determining of the issues before the Court, and which had not been included in the CTR.\", \"Therefore, in my view, this Court has already ruled on this issue. The conduct of this judicial review on the merits of the Applicant's claim does not serve as an appeal of the Prothonotary's decision. An appeal of a prothonotary's decision must be brought by motion to the Federal Court ( FC Rules , R 51(1), the Applicant did not bring such a motion and the time within which he was required to do so has elapsed. On that basis alone the Bui Affidavit is inadmissible and submissions made in reliance on it or in reference to it are to be disregarded.\", \"In any event, this Court has also previously held that material that was not before the decision-maker should generally be excluded for purposes of judicial review ( FRAGMENT_SUPPRESSED). And in this case, the Bui Affidavit does not fall under any of the recognized exceptions to this rule ( FRAGMENT_SUPPRESSED).\", \"The Commission listed the documentation that was reviewed when making its decision, which was contained in the CTR. Accordingly, the additional material contained in the Bui Affidavit is not relevant to the issue before this Court, being whether the Commission's Decision was reasonable based on the record that was before it.\", \"Further, the May 6, 2014 Section 40/41 Report specifically acknowledges that the parties had raised facts in their submissions that had occurred prior to the original Section 40/41 Report of July 6, 2011. The May 6, 2014 Section 40/41 Report stated that, because the Commission had already dealt with the alternative redress procedures available to the Applicant prior to July 2011, its current analysis would be based only on information arising after the Commission's original decision (para 61). As seen from the analysis contained in the May 6, 2014 Section 40/41 Report, prior facts which were set out in the parties' submissions do not constitute a part of the analysis.\", \"For all of these reasons, it is my view that the Bui Affidavit is inadmissible. Accordingly, the Applicant's submissions made in reliance on or in reference to that affidavit have not been considered in this judicial review. Position of the Parties\", \"The Applicant's position is that he has spent years attempting to satisfy the CF grievance system and acted in good faith including through a CF led biased and fatally flawed investigation and ensuing administrative investigation report. He characterises as cyclical, absurd and unreasonable the suggestion that to satisfy all reasonable steps he should now engage the harassment procedure and, if unsatisfied, pursue a new grievance, potentially all the way to judicial review. That process would create an endless procedural loop that would prevent the Applicant from exercising his right to complain under the CHR Act . Accordingly, the Commission unreasonably concluded that the CF's internal process was reasonably available to him.\", \"The Applicant further submits that the Commission ought to have accepted his complaint. The CF grievance process was exhausted because the Chief of the Defence Staff failed to make a decision on the grievance and instead passed it off, requiring the harassment procedures to be initiated. Further, the CF has demonstrated an unwillingness and an inability to handle this complex matter.\", \"The Respondent submits that a review of the reasonableness of the Commission's Decision should include an examination of some or all of the relevant factors ( FRAGMENT_SUPPRESSED), including any delay attributable to the Applicant. It should not include the Applicant's subjective and unsubstantiated view that the harassment process is not reasonably available and inadequate because the Applicant has never engaged the process.\", \"The Respondent further submits that the Commission's Decision that the Applicant ought to have used the appropriate CF internal process, which was reasonably available, falls within a range of possible acceptable outcomes. Any circularity in the process, as alleged by the Applicant, is purely attributable to his own refusal to utilize the process that the CF, the FA and the Commission all found to be the appropriate avenue for redress. Analysis\", \"In this matter the Commission adopted the recommendations made in the Section 40/41 Report, which, in that circumstance, constitute the reasons of the Commission ( FRAGMENT_SUPPRESSED). The Applicant takes no issue with the Commission's form of response.\", \"The Section 40/41 Report stated that s 41(1)(a) and s 42 together mean that the Commission can decide not to deal with a complaint under s 41(1)(a) if it finds that the complainant chose not to finish another process that was reasonably available to him or her. This is not an unreasonable interpretation of these provisions. In FRAGMENT_SUPPRESSED\", \"The Section 40/41 Report then listed factors that may be considered when deciding under s 41(1)(a) whether or not a complainant should keep using the other complaint or grievance process. These were: why the parties did not finish the other process; how much longer that process was likely to take; whether there is new information in that regard; and whether there is anything other than the amount of time the other process may take, such as vulnerability arising from the complainant's current situation or a risk of harm to any participant, that makes it not \\\"reasonably available\\\" to the complainant such that the Commission should deal with the complaint. The Section 40/41 Report also summarized the CF's submissions of February 20, 2014, and the Applicant's submissions of February 21, 2014 and April 10, 2013.\", \"The question of whether the harassment complaint process was reasonably available to the Applicant is largely a factual question. In my view, there is no question that it was available. The only question being was it reasonably so.\", \"In July 2011 the Commission decided not to deal with the complaint at that time under s 41(1)(a); instead, it asked the Applicant to use another complaint or grievance process first. At the end of that process the complainant could come back to the Commission and ask it to reactivate the complaint.\", \"Given this, the Applicant then filed a grievance pursuant to s 29 of the National Defence Act , RSC 1985, c N-5. The process thus engaged is set out in the Queen's Regulations and Orders for the Canadian Forces (QROCF) (online:  ). This envisions consideration and determination of the grievance by the IA (s 7.15). If the complainant is not satisfied with the IA decision, he or she may request that the matter be referred to the FA (s 7.18). The FA for all grievances is the Chief of the Defence Staff ( National Defence Act , s 29) or his delegate (QROCF, s 7.17). Pursuant to s 29.12 of the National Defence Act, the Chief of the Defence Staff shall refer every grievance that is of a type prescribed by regulation to the Grievances Committee for its findings and recommendations before the Chief of the Defence Staff considers and determines the grievance. These types of grievances are set out in s 7.21 of the QROCF. They include any decision of the Chief of the Defence Staff in respect of a particular officer or non-commissioned officer.\", \"In March 2012 the IA decision was issued. As the Applicant was unsatisfied with that decision, in May 2012 he sought an FA decision. As seen from the March 4, 2013 letter from the Chief of the Defence Staff, the grievance was submitted to the CFGB which conducted an independent review of the grievance and provided findings and recommendations which were disclosed to the Applicant.\", \"The Chief of the Defence Staff stated that the CFGB had analysed each of the contentions in the grievance file and found that the administrative investigation on which the IA had based its decision was \\\"fatally flawed\\\" on several levels.\", \"He also stated: The CF's policies on racist conduct and on harassment prevention and resolution are outlined in Canadian Forces Administrative Order 19-43 (Racist Conduct) and Defence Administrative Order and Directive (DAOD) 5012-0 (Harassment Prevention and Resolution). Both are unequivocal in their condemnation of any conduct that causes offence or harm to any person through acts, comments, or displays that demean, belittle, and cause personal humiliation or embarrassment, and any act of intimidation or threat. As much as we in the CF would like to think that we are immune from such behaviours, as a microcosm of the society that we serve, we would be very naïve to think that we, as an organization, would not be subject to instances of this insidious conduct. Accordingly, the CF has a comprehensive framework to deal with such instances in the form of the Harassment Prevention and Resolution Guidelines . The CF policy and guidelines flow directly from and are consistent with the Treasury Board's Policy on Harassment Prevention and Resolution in the Workplace and go beyond the requirements of the Canadian Human Rights Act in their scope.\", \"The Chief of the Defence Staff expressed concern that the Applicant had chosen not to avail himself of the CF harassment process. His lack of cooperation by failing to fulfil the obligation to provide full written details of his complaint was an impediment to the fair, impartial and expedient harassment investigation and contributed to the \\\"procedural morass\\\" that followed. The Chief of the Defence Staff went on to say that there had been important evidentiary omissions and procedural errors that compromised the integrity of the administrative investigation process resulting in what the CFGB had rightly referred to as a fatally flawed administrative investigation; consequently, the IA decision that was based on that investigation was equally flawed.\", \"Although the Chief of the Defence Staff had identified his role as being to determine whether the harassment occurred and whether the Applicant was a victim of mistreatment at the hands of a superior officer, he concluded that in light of the CFGB findings, he could not render a properly informed decision. He therefore decided to order a new harassment investigation.\", \"The Chief of the Defence Staff's reference to a lack of clarity as to the order in which the CF harassment and grievance processes are to be utilized is of note. He stated that the CFGB had highlighted that there was lack of clear policy direction regarding the relationship between the harassment and grievance processes. DAOD 2017-1 states: The following are situations in which other complaint and mechanisms should be used prior to proceeding with a grievance: • a complaint of harassment or abuse of authority\", \"While s 4.10 (Coincidental Complaints and Grievances) of the Harassment Prevention and Resolution Guidelines states: If an individual decides to file a grievance on the same issue as a harassment complaint, the applicable grievance mechanism will apply and the harassment complaint will be closed.\", \"The Chief of the Defence Staff stated that determination of misconduct, as it pertains to harassment or acts of racism, must be ascertained through a procedurally fair and impartial investigation process, such as that outlined in the Harassment Prevention and Resolution Guidelines . The grievance process should be engaged only if the complainant feels they have been wronged as a result of the outcome of the investigation process or if there is an excessive and unjustifiable delay in handling the complaint.\", \"The Chief of Military Personnel (CMP), in consultation with the Assistant Deputy Minister Human Resources - Civilian, \\\"will harmonize the Harassment Prevention and Resolution Guidelines with DAOD 2017-1 so that it is very clear that harassment complaints or grievances containing allegations of harassment must be dealt with first under the harassment policy and only after a proper harassment investigation has been completed\\\".\", \"The Chief of the Defence Staff decided that:\", \"am not prepared to grant the redress that you seek. Yet, I am troubled by your allegations and most particularly, by the lack of a proper investigation into their validity. As I mentioned, the CF takes this issue very seriously and consequently, it is imperative that you initiate the harassment investigation process by submitting a formal harassment complaint to the CO of 4 CFMCU. You have 90 days from the receipt of this letter to make your submission. The CO of 4 CFMCU shall, upon receipt of your harassment complaint, initiate a harassment investigation into the allegations therein. Let me be very clear on this point: an investigation will not commence until there is a formal harassment complaint submitted to the CO of 4 CFMCU and you commit, in writing, that you will fully participate in such an investigation to ensure that a thorough and impartial analysis may determine the legitimacy of your allegations. If you do not respond within 90 days, there will be no further action taken on your alleged complaints. Given the complexities of this case, compounded by the length of time that has elapsed since the alleged incidents, the CO of 4 CFMCU might consider engaging a private consulting firm to assist with the investigation ... When the investigation has been completed, you may grieve its outcome if the conclusions and recommendations are not to your satisfaction.\", \"In my view, regardless of the factual background prior to the Chief of the Defence Staff's decision and regardless of the lack of clarity as to the order in which harassment complaints or grievances containing allegations of harassment were to be dealt with and any delay or confusion that this may have previously caused, as of March 13, 2013, the Applicant was on clear notice of what was required of him, and that was that he initiate a harassment complaint within 90 days. He was also aware that if he failed to do so, there would be no further action taken with respect to the allegations.\", \"The CF's submission, as summarized by the Section 40/41 Report, stated that on May 14, 2013 it received from the Applicant a photocopy of his original grievance, not a formal harassment complaint as required by the Chief of the Defence Staff. On May 22, 2013, the RO sent the Applicant a letter requesting that he redraft his submission, explain each allegation specifically and provide any information that would enable the harassment investigation to be thorough and complete. The RO also notified the Applicant's current CO in order to ensure that an assistant could be appointed for the Applicant to help him with the process. The Applicant did not respond to the RO's request. The RO sent another letter to the Applicant on July 19, 2013 requesting an update on the preparation of this harassment complaint but again did not receive a response.\", \"The Section 40/41 Report noted the other factors that were considered in reaching its recommendation, including the attempt of the Chief of the Defence Staff to address the procedural fairness and other concerns by having the matter dealt with by a formal harassment complaint, with an outside firm being hired to investigate. Further, as to timeliness, that both parties bore some responsibility for the prior delays in the process and that it was not clear that the Commission's process would be more expedient than the CF's harassment policy, if it were still available to the Applicant. However, at this stage, the Commission's process was the only one still available to the Applicant. Further, that the Applicant had provided no information to indicate that he was personally vulnerable.\", \"The Applicant's position is that the harassment complaint process as proposed by the Chief of the Defence Staff cannot be considered to be reasonably available. The Applicant submits that there has already been a considerable delay, that he commenced the grievance process and that, at the end of that process, rather than rendering a decision, the Chief of the Defence Staff instead attempted to effect a harassment investigation. This is \\\"cyclical\\\" because he was required to commence the grievance process, which now requires a harassment investigation, which in turn can be grieved, and that this has the potential of becoming an endless loop.\", \"In my view, this argument cannot succeed. Although the CFGB's report was not in the CTR, it is clear from the Chief of the Defence Staff's letter that there was a lack of clarity regarding the relationship between the harassment and grievance processes. This may well have contributed to the prior delay and procedural missteps in this matter. And, unless the relationship is clarified, it may have the same impact on other matters in the future.\", \"Regardless, in this case, the decision by the Chief of the Defence Staff had the effect of resetting the clock. While it is true that a grievance from the conclusions and recommendations of the harassment investigation would have been available to the Applicant, this did not mean that an endless procedural circle would follow. The problem identified in the original grievance process initiated by the Applicant was that there was insufficient information to deal with the harassment complaint. This led to the Chief of the Defence Staff's decision that a harassment investigation was required. Had that been carried out, any further grievance process then open to the Applicant would have been based on the findings of the harassment investigation, not the absence of one, and would not have resulted in a decision that a harassment investigation was required. Thus, this does not result in an endless cycle, nor does it establish that the process proposed by the Chief of the Defence Staff was not \\\"reasonably available\\\" to the Applicant.\", \"Further, the original grievance process would have been exhausted if the Applicant had participated in the proposed harassment investigation. The Applicant does not submit that it was beyond the authority of the Chief of the Defence Staff to proceed as he did, by requiring the harassment investigation. In addition, he discontinued his judicial review of the FA.\", \"also do not agree with the Applicant's submission that the Chief of the Defence Staff admitted that the CF is unable or unwilling to fully and objectively determine a complaint of racial discrimination when he stated the CO \\\"might consider engaging a private consulting firm to assist the investigation\\\" in his letter given the complexity of the case, compounded by the length of time that had elapsed since the alleged incidents. A suggestion that an independent third party with experience in the investigation of harassment complaints could be utilized, if anything, would seem to suggest the incorporation of an additional measure to ensure procedural fairness in the process.\", \"The Applicant also submits that the Chief of the Defence Staff's statement quoted at paragraph 42 above is an acknowledgment that racism exists within the CF, at least to some degree, and that his inaction to combat racism may mean that it will only continue to exist. However, read in whole, the statement clearly expresses that the CF Harassment Prevention Resolution Guidelines are intended to deal with any instance of racism and harassment.\", \"As to systemic discrimination, the Section 40/41 Report stated that although the Applicant described the complaint as one of systemic discrimination, no policy or practice had been identified in the summary of complaint. A review of the complaint confirms that the complaint clearly pertained to the Applicant's CO and made no allegation of systemic discrimination. At the hearing before me, the Applicant confirmed that this was the case and that systemic discrimination was therefore not pursued as an issue in this matter.\", \"In summary, the Commission considered the factual basis of this matter and found that there was a review procedure that was reasonably available to the Applicant and that had not been exhausted by the Applicant prior to the expiry of the 90 day deadline set by the Chief of the Defence Staff. Based on the record, this finding was reasonably open to it.\", \"The Commission also recognised that this finding would leave the Applicant without recourse. However, as noted by the Respondent, this Court has held that the reasonableness of a decision is not affected by the absence of the \\\"safety net\\\" of renewed access to the Commission ( FRAGMENT_SUPPRESSED). In this case, while the CF review process itself is no longer available to the Applicant, nor is there a safety net, the Commission recognized this outcome when making its decision.\", \"In the circumstance of this matter, the Commission's finding that the Applicant was aware of, but refused to participate in, the CF's internal harassment complaint process and that the failure to exhaust the procedure was solely attributable to the Applicant is to be afforded deference ( FRAGMENT_SUPPRESSED) and also fell within a range of possible acceptable outcomes defensible in respect of the facts and law ( FRAGMENT_SUPPRESSED).\", \"For these reasons the application is dismissed. The parties have agreed to costs in the amount of $2,500.00, accordingly, the Respondent shall have its costs in that amount. JUDGMENT\", \"THIS COURT'S JUDGMENT is that 1. The application for judicial review is dismissed. 2. The Respondent shall have its costs in the amount of $2,500.00. Application dismissed. Editor: Jana A. Andersen/nmg [End of document]\"], \"c_paras\": [\"Russell, J. : This is an application under s. 18.1 of the Federal Courts Act , RSC 1985, c F-7 for judicial review of a decision of the Canadian Human Rights Commission [Commission], dated May 7, 2014 [Decision], which decided not to deal with the Applicant's complaint pursuant to s. 41(1)(d) of the Canadian Human Rights Act , RSC 1985, c H-6 [Act].\", \"The Applicant started working as a Heavy Equipment Operator for Canadian National Railway Company [CN] in August 1981. He was a member of a bargaining unit represented by the National Automobile, Aerospace, Transportation and General Workers Union of Canada (CAW-Canada) [ Union].\", \"On December 19, 2012, the Applicant's employment was terminated because he refused to attend a medical assessment to determine his fitness for work. CN requested the medical assessment because the Applicant's position was a safety sensitive position and there were concerns regarding his behaviour. The Applicant says that he refused to attend the medical assessment because he was receiving treatment for drug dependency and believed that was sufficient to address CN's concerns.\", \"On January 8, 2013, the Union filed a grievance to contest the Applicant's termination.\", \"On February 14, 2013, the Union closed the Applicant's file. The Union said that the Applicant had not responded to their requests for information and it \\\"was not in a position to advance the matter with the limited information at hand.\\\"\", \"In March 2013, the Applicant submitted a complaint to the Commission. He alleged that CN had discriminated against him on the ground of disability by terminating his employment contrary to s. 7 of the Act. The Commission decided not to deal with the complaint pursuant to s. 41(1)(a) because the Applicant had not exhausted the Respondent's grievance process.\", \"On April 19, 2013, CN denied the Union's grievance. The Union did not refer the grievance to arbitration.\", \"On October 1, 2013, the Applicant submitted another complaint to the Commission. He alleged that, again, CN had discriminated against him on the ground of disability by terminating his employment contrary to s. 7 of the Act.\", \"On October 29, 2013, the Applicant was advised that the Commission would be preparing a s. 40/41 report to determine whether it should deal with his complaint. The Applicant was invited to prepare a letter stating his position on whether the Commission should not deal with the issues because \\\"the human rights issues in this complaint may have already been dealt with through another process.\\\" Counsel for the Applicant made submissions to the Commission both in advance of the preparation of the report and after being provided a copy of the s. 40/41 report [Report].\", \"On May 7, 2014, the Commission decided not to deal with the Applicant's complaint pursuant to s. 41(1)(d) of the Act. The Commission adopted the Report's conclusions and decided that the complaint was vexatious under s. 41(1)(d) of the Act (Applicant's Record at 48): The complainant's human rights allegations have been addressed by an alternate decision maker with authority to consider human rights issues. The allegations raised in the complaint before the Commission are the same as those addressed in the final level grievance response. Given that the alternate decision-maker dealt with the human rights issues raised in this complaint, and that process was fair, the Commission must respect the finality of that decision and should not deal with this complaint. It is therefore plain and obvious that this complaint is vexatious within the meaning of section 41(1)(d) of the Act.\", \"The Applicant raises the following issues in this application: 1. Whether the Commission unreasonably refused to exercise its jurisdiction; 2. Whether the Commission erred in law by: a) Unreasonably finding the Applicant's complaint to be vexatious; or, b) Having found the complaint to be vexatious, unreasonably ignoring that justice required it to deal with the complaint anyway; and, 3. Whether the Commission unreasonably based its decision on erroneous findings of fact made without regard to the material before it.\", \"The Supreme Court of Canada in Dunsmuir v New Brunswick , 2008 SCC 9 [ Dunsmuir ] held that a standard of review analysis need not be conducted in every instance. Instead, where the standard of review applicable to a particular question before the court is settled in a satisfactory manner by past jurisprudence, the reviewing court may adopt that standard of review. Only where this search proves fruitless, or where the relevant precedents appear to be inconsistent with new developments in the common law principles of judicial review, must the reviewing court undertake a consideration of the four factors comprising the standard of review analysis: Agraira v Canada (Public Safety and Emergency Preparedness) , 2013 SCC 36 at para 48.\", \"The Applicant submits that decisions under s. 41(1)(d) of the Human Rights Act are reviewed on a standard of reasonableness: Chan v Canada (Attorney General) , 2010 FC 1232 [ Chan ]. The Respondent submits that the Commission's decision not to deal with a complaint under s. 41 of the Human Rights Act is a discretionary decision reviewable on a standard of reasonableness: Exeter v Canada (Attorney General) , 2011 FC 86 at para 19, aff'd 2012 FCA 119 at para 6 [ Exeter ]; Morin v Canada (Attorney General) , 2007 FC 1355 at para 25, aff'd 2008 FCA 269.\", \"All of the issues question the reasonableness of the Commission's decision to not deal with the complaint. The Court agrees that these decisions are reviewable on a standard of reasonableness: Chan , above, at para 15; Exeter , above, at para 6.\", \"When reviewing a decision on the standard of reasonableness, the analysis will be concerned with \\\"the existence of justification, transparency and intelligibility within the decision-making process [and also with] whether the decision falls within a range of possible, acceptable outcomes which are defensible in respect of the facts and law\\\": see Dunsmuir , above, at para 47; Canada (Citizenship and Immigration) v Khosa , 2009 SCC 12 at para 59. Put another way, the Court should intervene only if the Decision was unreasonable in the sense that it falls outside the \\\"range of possible, acceptable outcomes which are defensible in respect of the facts and law.\\\"\", \"A. Applicant\", \"The Applicant submits that the Commission unreasonably refused to exercise its jurisdiction. The Applicant concedes that the Commission is entitled to adopt the Report for its reasons: Chan , above, at paras 39-40. However, the Applicant distinguishes the present proceeding from the Chan case on two grounds. First, the Commission's adoption of the Report was inadequate because it fails to show that the Commission considered the submissions before it and fails to recognize that the human rights issues were not considered in the grievance process: Vancouver International Airport Authority v Public Service Alliance of Canada , 2010 FCA 158. Second, the internal grievance process does not constitute a proper decision-maker. The grievance process is not an independent arbitrator and it failed to provide reasons for its decision on the human rights issues.\", \"The Applicant characterizes the grievance process as an internal negotiation between the Union and CN. If the Commission does not deal with the complaint then he says the Union's Decision to not refer the grievance to arbitration will have denied him the ability to have his human rights issue considered by a decision-maker. The Union's Decision to not proceed to arbitration was based, in part, on the Applicant's refusal to cooperate but the Applicant says that the nature of his disability carries the need for reasonable flexibility regarding deadlines and expectations. The Union's Decision was also based on other factors including time, money and resources.\", \"In the alternative, if the internal grievance process constituted a decision-maker, the Decision is unreasonable because the grievance did not address the human rights issues. It referenced \\\"drug dependency\\\" and concluded there was insufficient evidence.\", \"If the complaint was correctly deemed vexatious, then the Applicant submits that the Commission erred in law by ignoring that justice required the Commission to deal with the complaint anyway. The Decision says the internal grievance process was fair but fails to consider the Applicant's reply submissions.\", \"Finally, the Decision is unreasonable because it relies on the erroneous finding that the Applicant's human rights issues were already addressed by a decision-maker. B. Respondent\", \"The Respondent submits that it was reasonable for the Commission to refuse to deal with the complaint. The Report constitutes the reasons for the Decision: Canada (Attorney General) v Sketchley , 2005 FCA 404 at para 37 [ Sketchley ]; Bergeron v Canada (Attorney General) , 2013 FC 301 at paras 28-29 [ Bergeron ]. The Applicant had the opportunity to address the human rights issues through the Union but he failed to cooperate with the search for accommodation. The Commission may refuse to deal with a complaint if it is obvious that the complaint cannot succeed. A complainant who refuses to collaborate in the search for accommodation will have his or her complaint dismissed: Central Okanogan School District No 23 v Renaud , [1992] 2 SCR 970.\", \"The Commission's Decision to refuse to deal with the complaint is also in accordance with Supreme Court of Canada jurisprudence regarding the importance of permitting administrative tribunals to curb abuse of process: British Columbia (Workers' Compensation Board) v Figliola , 2011 SCC 52. It would be an abuse of process to advance a human rights complaint where the complainant has failed to cooperate with their union to have the same human rights issues addressed.\", \"Contrary to the Applicant's submissions, s. 41(1)(d) does not require that a decision be made by an arbitrator. The Commission is granted great latitude in exercising its discretion and assessing the appropriate factors in performing its screening function: Sketchley , above, at para 38; Bergeron , above, at para 39. Further, the Federal Court has held that s. 41(1)(d) may apply in situations where a union has decided not to pursue a grievance to arbitration: Bergeron , above, at para 38. There is also no evidence that those who decided the Applicant's grievances were not impartial: Bergeron , above, at para 43.\", \"The Report shows that the Investigator turned her mind to the outcome of the grievance process, the Applicant's allegations relating to substance abuse, and the question of reasonable accommodation. The Commission reasonably concluded that the allegations raised in the complaint had already been addressed in the grievance process and that the grievance process was fair.\", \"The Applicant raises three (3) grounds for reviewable error but, in the end, they all come back to the issue of the Applicant's own failure to cooperate in the grievance process. Essentially, the Commission came to the conclusion that the Applicant's complaint was vexatious under s. 41(1)(d) of the Act because the Applicant's human rights allegations had already been addressed by the grievance process.\", \"As the Report found, the Applicant's Union representative filed a grievance on his behalf that raised the same human rights issues as those in the complaint to the Commission. The Union had to close out its grievance file because the Applicant would not cooperate with its attempts at obtaining accommodation for him. The Union concluded that it could not advance the grievance to arbitration because, given Applicant's failure to cooperate in providing the information requested and required for the grievance process, there was insufficient information to advance the matter. In the end, the grievance process was exhausted without going to arbitration because the Applicant failed to cooperate. This was the final decision in the grievance process.\", \"The Applicant attempted to convince the Commission, and he has attempted to convince this Court, that his disability prevented him from providing the materials and cooperation required by the grievance process. In his submissions to the Commission, he alleged as follows (CTR at 14): Given the nature of [the Applicant's] disability, it follows that it would be logical for the Company to have been in closer contact with the Union in order to determine what the correct situation, and prognosis, for [the Applicant] was. It is understandable that an individual with a disability would encounter difficulty in navigating deadlines without reasonable assistance, and it was further understandable that [the Applicant] mistakenly believed the matter was being dealt with by his Union and his doctor.\", \"There was no evidence before the Commission, and there is none before me, to support this bare allegation that the Applicant had difficulties with deadlines and mistaken beliefs because of his disability. The Applicant simply expected the Commission, and now asks the Court, to draw an inference to this effect from the nature of his disability which is drug dependency. It is noticeable that, in the affidavit he has filed with this application, the Applicant says nothing about difficulties with deadlines and mistaken beliefs. Further, his evidence before me clearly indicates the considerable lengths to which the Union went to make clear to the Applicant what was required of him and to encourage him to comply. The letter of April 4, 2013 to the Applicant from Mr. Robert Fitzgerald, the Union's National Representative, sets out the whole picture: To date, none of this information has been provided. The Union only has two pieces of medical documentation. One dated January 24, 2013 stating that you will be seeing an addiction counsellor, but no confirmation that you did. The second one verified that you do not have any disability as it relates to psychiatric issues. Although your doctor invited us to follow up with him providing we had the necessary medical release to do so, you failed to return the release form that the Regional Representative provided to you, enclosed with his letter of February 01, 2013. There have been literally hundreds of phone calls between you and the Union at different levels. However, you have not acknowledged the Union's request for information. You have abated the Union's request in must the same way you have declined to cooperate with the Company. In our opinion, the negative connotation of your actions would not be lost on an Arbitrator. At some point you did advise the Company that you had an addiction problem and that you were seeking help for such. However, there is no evidence that you have been diagnosed with such an addiction nor is there any evidence to show that you are being treated for such. As we said earlier, there is only two pieces of medical documentation on file and neither provide a diagnoses or address treatment. If in fact there was a clinical diagnosis of addiction, treatment and rehabilitation, the Company may well have been obligated to provide accommodation. However, with such an obligation, there also comes an onus on the employees to cooperate with the efforts to accommodate. It was put this way by the arbitrator in CROADR case 3354: The Arbitrator must agree. As confirmed by the Supreme Court of Canada in Central Okanogan School District No. 23 v. Renaud,\", \"2 S.C.R. 970, the obligation of accommodation involves the cooperative participation of the employer, the trade union and the employee . That was reflected in an award of this Office in CROA 3173 : The Arbitrator is satisfied that the approach adopted by the Company is in keeping with its obligations under the Canadian Human Rights Act . It now seems well-established that when an employee seeks accommodation by reason of a status that is protected under the Canadian Human Rights Act, it is incumbent upon the employee concerned to contribute positively to the process , and to accept an offer of reasonable accommodation, even though it might not be the specific accommodation which the employee would prefer. That is reflected, in part, in the decision of the Supreme Court of Canada in Central Okanogan School District No. 23 v. Renaud,\", \"2 S.C.R. 970. In that decision, for a unanimous court, Sopinkla J. wrote as follows : To facilitate the search for an accommodation, the complainant must do his or her part as well . Concomitant with a search for reasonable accommodation is a duty to facilitate the search for such an accommodation. Thus in determining whether the duty of accommodation has been fulfilled the conduct of the complainant must be considered. [Emphasis in original]\", \"On the basis of the record that was before the Commission, and that is before me, the only possible inference is that the Union made every effort to advance the Applicant's grievance but had to abandon the process at step III because of the Applicant's refusal to provide the necessary information, a refusal that has not been linked to his alleged disability. The Commission deals with this matter extensively in the Decision by referring to Mr. Fitzgerald's letter and the Step III Grievance Response dated April 19, 2013. The Applicant provided nothing to counter the information regarding his non-cooperation. It has to be remembered that it was the Applicant who provided the letter from Mr. Fitzgerald so that he was well-aware of what it said about him, and it also has to be borne in mind that his non-cooperation is evidenced by his own Union who had supported him in the grievance process. There was nothing to suggest that the Applicant's failure to cooperate had anything to do with his disability.\", \"It is also noteworthy that the Canadian Industrial Relations Board came to a similar conclusion when the Applicant alleged a violation of s. 37 of the Canada Labour Code , and alleged that the Union breached its duty of fair representation by failing to properly represent him when it decided not to proceed further with his grievance ( Mulligan v National Automobile, Aerospace, Transportation and General Workers Union of Canada (CAW-Canada) (31 July 2013), 29997-C (CIRB):\", \"In this case, the complainant requests that the Board hold a hearing. Section 16.1 of the Code provides that the Board may decide any matter before it without holding an oral hearing. Having reviewed all of the material on file, the Board is satisfied that the documentation before it is sufficient for it to decide the matter without holding an oral hearing. As mentioned above, the complainant alleges that the union acted in an arbitrary manner and in bad faith when it did not properly investigate his grievance, did not contact him and did not seek the proper information from his doctors and counsellor. The complainant also alleges that the union violated his rights with respect to article 23.2 of the collective agreement. Section 37 of the Code reads as follows: 37. A trade union or representative of a trade union that is the bargaining agent for a bargaining unit shall not act in a manner that is arbitrary, discriminatory or in bad faith in the representation of any of the employees in the unit with respect to their rights under the collective agreement that is applicable to them. The Board's role in the context of a duty of fair representation complaint is to examine the union's conduct in handling the employee's grievance (see Bugay , 1999 CIRB 45). A section 37 complaint cannot serve to appeal a union's decision not to refer a grievance to arbitration, or to assess the merits of the grievance, but it is used to assess how the union handled the grievance (see Presseault , 2001 CIRB 138). In a complaint under section 37, the complainant bears the onus of presenting evidence that is sufficient to raise a presumption that the union has breached its duty of fair representation. The Board will normally find that the union has fulfilled its duty of fair representation if it has investigated the circumstances, considered the merits of the grievance, made a reasoned judgment about whether to pursue the issue, and if it advised the employee of the reason for its ultimate decision not to proceed any further. The duty of a member to cooperate with his union is described in the following passage from McRaeJackson , 2004 CIRB 290:\", \"The union's duty of fair representation is predicated on the requirement that employees take the necessary steps to protect their own interests. Employees must make the union aware of potential grievances and ask the union to act on their behalf within the time limits provided in the collective agreement. They must cooperate with their union throughout the grievance procedure, for example by providing the union with the information necessary to investigate a grievance, by attending any medical examinations or other assessments. The evidence on file indicates that the union filed a grievance on behalf of the complainant, processed the grievance to step three of the grievance procedure, sent several letters to the complainant seeking medical information and had numerous telephone conversation with the complainant, with limited success in getting the information needed to further his case. In the Board's opinion, the complainant did not provide any evidence of wrongdoing by the union. The documentation submitted indicates that the complainant brought his termination upon himself by not submitting the information requested by the union. Failure by the complainant to take such action, along with his refusal to cooperate with the union, leads the Board to conclude that the union did not act in an arbitrary manner or in bad faith. Having reviewed the facts submitted, the Board finds that the complainant did not provide sufficient facts to establish that the union has violated its duty of fair representation. For the above reasons, the complaint is dismissed.\", \"This decision by the Canadian Industrial Relations Board was not before the Commission, but it confirms the Commission's conclusions that the Applicant is the one who, for no apparent reason, thwarted the grievance process that the Commission had earlier told him he had to exhaust before bringing his complaint to the Commission.\", \"As the Report makes clear, all of the Applicant's submissions were considered including the \\\"issue of consent and ongoing substance use\\\" that he claims was not addressed by the Respondent, as well as the correspondence from the Applicant's doctor and personnel in the Respondent's Employee Assistance Program.\", \"can find no reviewable error in the Commission's Decision (which includes the Report), which ably sets out the relevant facts and the governing jurisprudence. This is simply a case where the Applicant, for no apparent reason, refused to cooperate in the grievance process that could have dealt with his human rights issues and left his Union with no alternative but to close out the file.\", \"The Commission provides full reasons as to why the complaint was vexatious and why justice did not require the Commission to deal with the complaint.\", \"Subsection 41(1)(d) of the Act does not require a decision by a grievance arbitrator. As Justice Zinn pointed out in Bergeron , above:\", \"The jurisprudence is clear that the Commission is to be afforded great latitude in exercising its judgment and in assessing the appropriate factors when considering the application of paragraph 41(1)(d) of the CHRA and performing this \\\"screening function:\\\" See, e.g., Sketchley at para 38.\", \"Bergeron , above, makes it clear that s. 41(1)(d) of the Act may apply in situations where a union has decided not to pursue a grievance to arbitration. In the present case, as the Union letter makes clear, the Applicant refused, for no reason that is established, to engage in a grievance process that could have provided him with accommodation and arbitration and that could have dealt with his human rights issues. The Union makes it clear that his failure to cooperate meant that there was no point in proceeding to arbitration. Having failed to exhaust a grievance process that could have provided him with the remedy he sought before the Commission, the Applicant then filed his complaint with the Commission. The Applicant failed to show that his complaint could not have reasonably been dealt with by the grievance process. The Commission's Decision should not be disturbed.\", \"The Commission's Decision is transparent, intelligible and justifiable. I can find no reviewable error. It falls within the range of possible acceptable outcomes which are defensible in respect of the facts and the law. JUDGMENT\", \"THIS COURT'S JUDGMENT is that 1. This application is dismissed with costs to the Respondent. Application dismissed. Editor: J. Danielle King/bk [End of document]\"], \"label\": 1}\n",
    "# task1=dict()\n",
    "# task1['guid']='1839d9b_d6f26064d6f489ae6e1a681139e58eb7223837a0'\n",
    "# task1['q_paras']=['Detection of CDE as active WM is platform-spe...','SWT code currently determines the active deskt...']\n",
    "# task1['c_paras']=['quert_1','query_2']\n",
    "# task1['label']=1\n",
    "# task1\n",
    "# with open('case_para_sample.json','w',encoding='utf-8') as f_out:\n",
    "#     for i in range(4):\n",
    "#         f_out.write(str(task1)+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "# 模型需要的输入样例 下面进行模型的测试\n",
    "{\n",
    "\t\"guid\": \"queryID_docID\",\n",
    "\t\"q_paras\": [...], // a list of paragraphs in query case,\n",
    "\t\"c_paras\": [...], // a list of parameters in candidate case,\n",
    "\t\"label\": 0, // 0 or 1, denote the relevance\n",
    " }\n",
    " \n",
    "{\n",
    "\t\"guid\": \"queryID_docID\",\n",
    "\t\"res\": [[],...,[]], // N * 768, result of BertPoolOutMax,\n",
    "\t\"label\": 0, // 0 or 1, denote the relevance\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import functools\n",
    "\n",
    "\n",
    "class ConfigParser:\n",
    "    def __init__(self, *args, **params):\n",
    "        self.default_config = configparser.RawConfigParser(*args, **params)\n",
    "        self.local_config = configparser.RawConfigParser(*args, **params)\n",
    "        self.config = configparser.RawConfigParser(*args, **params)\n",
    "\n",
    "    def read(self, filenames, encoding=None):\n",
    "        if os.path.exists(\"config/default_local.config\"):\n",
    "            self.local_config.read(\"config/default_local.config\", encoding=encoding)\n",
    "        else:\n",
    "            self.local_config.read(\"config/default.config\", encoding=encoding)\n",
    "\n",
    "        self.default_config.read(\"config/default.config\", encoding=encoding)\n",
    "        self.config.read(filenames, encoding=encoding)\n",
    "\n",
    "\n",
    "def _build_func(func_name):\n",
    "    @functools.wraps(getattr(configparser.RawConfigParser, func_name))\n",
    "    def func(self, *args, **kwargs):\n",
    "        try:\n",
    "            return getattr(self.config, func_name)(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                return getattr(self.local_config, func_name)(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                return getattr(self.default_config, func_name)(*args, **kwargs)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def create_config(path):\n",
    "    for func_name in dir(configparser.RawConfigParser):\n",
    "        if not func_name.startswith('_') and func_name != \"read\":\n",
    "            setattr(ConfigParser, func_name, _build_func(func_name))\n",
    "\n",
    "    config = ConfigParser()\n",
    "    config.read(path)\n",
    "\n",
    "    return config\n",
    "config = create_config('/home/hdj/BERT-PLI-IJCAI2020/config/nlp/BertPoint.config')\n",
    "\n",
    "\n",
    "\n",
    "for key,val in config.items():\n",
    "    for k,v in val.items():\n",
    "        print(k,v)\n",
    "    print(key,val,type(val))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AdamW\n",
    "import torch\n",
    "\n",
    "class BertPoolOutMax(nn.Module):\n",
    "    def __init__(self, config, gpu_list, *args, **params):\n",
    "        super(BertPoolOutMax, self).__init__()\n",
    "        self.max_para_c = config.getint('model', 'max_para_c')\n",
    "        self.max_para_q = config.getint('model', 'max_para_q')\n",
    "        self.step = config.getint('model', 'step')\n",
    "        self.max_len = config.getint(\"data\", \"max_seq_length\")\n",
    "#         self.bert = BertModel.from_pretrained(config.get(\"model\", \"bert_path\"))\n",
    "        self.bert = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        # self.maxpool = nn.MaxPool1d(kernel_size=self.max_para_c)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(1, self.max_para_c))\n",
    "\n",
    "    def init_multi_gpu(self, device, config, *args, **params):\n",
    "        self.bert = nn.DataParallel(self.bert, device_ids=device)\n",
    "\n",
    "    def forward(self, data, config, gpu_list, acc_result, mode):\n",
    "        input_ids, attention_mask, token_type_ids = data['input_ids'], data['attention_mask'], data['token_type_ids']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for k in range(input_ids.size()[0]):\n",
    "                q_lst = []\n",
    "                for i in range(0, self.max_para_q, self.step):\n",
    "                    # print(input_ids[k, i:i+self.step].view(-1, self.max_len).size())\n",
    "                    _, lst = self.bert(input_ids[k, i:i+self.step].view(-1, self.max_len),\n",
    "                                       token_type_ids=token_type_ids[k, i:i+self.step].view(-1, self.max_len),\n",
    "                                       attention_mask=attention_mask[k, i:i+self.step].view(-1, self.max_len))\n",
    "                    # print('before view', lst.size())\n",
    "                    lst = lst.view(self.step, self.max_para_c, -1)\n",
    "                    # print('after view', lst.size())\n",
    "                    lst = lst.permute(2, 0, 1)\n",
    "                    # print('after permute', lst.size())\n",
    "                    lst = lst.unsqueeze(0)\n",
    "                    # print('after unsquezze', lst.size())\n",
    "                    max_out = self.maxpool(lst)\n",
    "                    # print('after maxpool', max_out.size())\n",
    "                    max_out = max_out.squeeze()\n",
    "                    # print('after squeeze', max_out.size())\n",
    "                    max_out = max_out.transpose(0, 1)\n",
    "                    q_lst.extend(max_out.cpu().tolist())\n",
    "                    #input('continue?')\n",
    "                # print(len(q_lst))\n",
    "                #exit()\n",
    "                assert (len(q_lst) == self.max_para_q)\n",
    "                output.append([data['guid'][k], q_lst])\n",
    "            return {\"output\": output}\n",
    "        \n",
    "\n",
    "class BertPoint(nn.Module):\n",
    "    def __init__(self, config, gpu_list, *args, **params):\n",
    "        super(BertPoint, self).__init__()\n",
    "\n",
    "        self.output_dim = config.getint(\"model\", \"output_dim\")\n",
    "        self.output_mode = config.get('model', 'output_mode')\n",
    "\n",
    "        # self.bert = BertModel.from_pretrained(config.get(\"model\", \"bert_path\"))\n",
    "        self.bert = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.fc = nn.Linear(768, self.output_dim)\n",
    "        if self.output_mode == 'classification':\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        self.accuracy_function = init_accuracy_function(config, *args, **params)\n",
    "\n",
    "    def init_multi_gpu(self, device, config, *args, **params):\n",
    "        self.bert = nn.DataParallel(self.bert, device_ids=device)\n",
    "\n",
    "    def forward(self, data, config, gpu_list, acc_result, mode):\n",
    "        input_ids, attention_mask, token_type_ids = data['input_ids'], data['attention_mask'], data['token_type_ids']\n",
    "        _, y = self.bert(input_ids,\n",
    "                         # token_type_ids=token_type_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                            # output_all_encoded_layers=False\n",
    "                         )\n",
    "        y = y.view(y.size()[0], -1)\n",
    "\n",
    "        if mode == 'test' and config.getboolean('output', 'pool_out'):\n",
    "            output = []\n",
    "            y = y.cpu().detach().numpy().tolist()\n",
    "            for i, guid in enumerate(data['guid']):\n",
    "                output.append([guid, y[i]])\n",
    "            return {\"output\": output}\n",
    "\n",
    "        y = self.fc(y)\n",
    "        y = y.view(y.size()[0], -1)\n",
    "\n",
    "        if \"label\" in data.keys():\n",
    "            label = data[\"label\"]\n",
    "            loss = self.criterion(y, label.view(-1))\n",
    "            acc_result = self.accuracy_function(y, label, config, acc_result)\n",
    "            return {\"loss\": loss, \"acc_result\": acc_result}\n",
    "\n",
    "        else:\n",
    "            output = []\n",
    "            y = y.cpu().detach().numpy().tolist()\n",
    "            for i, guid in enumerate(data['guid']):\n",
    "                output.append([guid, y[i]])\n",
    "            return {\"output\": output}\n",
    "\n",
    "def init_accuracy_function(config, *args, **params):\n",
    "    name = config.get(\"output\", \"accuracy_method\")\n",
    "    if name in accuracy_function_dic:\n",
    "        return accuracy_function_dic[name]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def get_prf(res):\n",
    "    # According to https://github.com/dice-group/gerbil/wiki/Precision,-Recall-and-F1-measure\n",
    "    if res[\"TP\"] == 0:\n",
    "        if res[\"FP\"] == 0 and res[\"FN\"] == 0:\n",
    "            precision = 1.0\n",
    "            recall = 1.0\n",
    "            f1 = 1.0\n",
    "        else:\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            f1 = 0.0\n",
    "    else:\n",
    "        precision = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FP\"])\n",
    "        recall = 1.0 * res[\"TP\"] / (res[\"TP\"] + res[\"FN\"])\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def gen_micro_macro_result(res):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    total = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0}\n",
    "    for a in range(0, len(res)):\n",
    "        total[\"TP\"] += res[a][\"TP\"]\n",
    "        total[\"FP\"] += res[a][\"FP\"]\n",
    "        total[\"FN\"] += res[a][\"FN\"]\n",
    "        total[\"TN\"] += res[a][\"TN\"]\n",
    "\n",
    "        p, r, f = get_prf(res[a])\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    micro_precision, micro_recall, micro_f1 = get_prf(total)\n",
    "\n",
    "    macro_precision = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    for a in range(0, len(f1)):\n",
    "        macro_precision += precision[a]\n",
    "        macro_recall += recall[a]\n",
    "        macro_f1 += f1[a]\n",
    "\n",
    "    macro_precision /= len(f1)\n",
    "    macro_recall /= len(f1)\n",
    "    macro_f1 /= len(f1)\n",
    "\n",
    "    return {\n",
    "        \"micro_precision\": round(micro_precision, 3),\n",
    "        \"micro_recall\": round(micro_recall, 3),\n",
    "        \"micro_f1\": round(micro_f1, 3),\n",
    "        \"macro_precision\": round(macro_precision, 3),\n",
    "        \"macro_recall\": round(macro_recall, 3),\n",
    "        \"macro_f1\": round(macro_f1, 3)\n",
    "    }\n",
    "\n",
    "\n",
    "def null_accuracy_function(outputs, label, config, result=None):\n",
    "    return None\n",
    "\n",
    "\n",
    "def single_label_top1_accuracy(outputs, label, config, result=None):\n",
    "    if result is None:\n",
    "        result = []\n",
    "    id1 = torch.max(outputs, dim=1)[1]\n",
    "    # id2 = torch.max(label, dim=1)[1]\n",
    "    id2 = label\n",
    "    nr_classes = outputs.size(1)\n",
    "    while len(result) < nr_classes:\n",
    "        result.append({\"TP\": 0, \"FN\": 0, \"FP\": 0, \"TN\": 0})\n",
    "    for a in range(0, len(id1)):\n",
    "        # if len(result) < a:\n",
    "        #    result.append({\"TP\": 0, \"FN\": 0, \"FP\": 0, \"TN\": 0})\n",
    "\n",
    "        it_is = int(id1[a])\n",
    "        should_be = int(id2[a])\n",
    "        if it_is == should_be:\n",
    "            result[it_is][\"TP\"] += 1\n",
    "        else:\n",
    "            result[it_is][\"FP\"] += 1\n",
    "            result[should_be][\"FN\"] += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def multi_label_accuracy(outputs, label, config, result=None):\n",
    "    if len(label[0]) != len(outputs[0]):\n",
    "        raise ValueError('Input dimensions of labels and outputs must match.')\n",
    "\n",
    "    outputs = outputs.data\n",
    "    labels = label.data\n",
    "\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    total = 0\n",
    "    nr_classes = outputs.size(1)\n",
    "\n",
    "    while len(result) < nr_classes:\n",
    "        result.append({\"TP\": 0, \"FN\": 0, \"FP\": 0, \"TN\": 0})\n",
    "\n",
    "    for i in range(nr_classes):\n",
    "        outputs1 = (outputs[:, i] >= 0.5).long()\n",
    "        labels1 = (labels[:, i].float() >= 0.5).long()\n",
    "        total += int((labels1 * outputs1).sum())\n",
    "        total += int(((1 - labels1) * (1 - outputs1)).sum())\n",
    "\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        # if len(result) < i:\n",
    "        #    result.append({\"TP\": 0, \"FN\": 0, \"FP\": 0, \"TN\": 0})\n",
    "\n",
    "        result[i][\"TP\"] += int((labels1 * outputs1).sum())\n",
    "        result[i][\"FN\"] += int((labels1 * (1 - outputs1)).sum())\n",
    "        result[i][\"FP\"] += int(((1 - labels1) * outputs1).sum())\n",
    "        result[i][\"TN\"] += int(((1 - labels1) * (1 - outputs1)).sum())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def single_label_top2_accuracy(outputs, label, config, result=None):\n",
    "    raise NotImplementedError\n",
    "    # still bug here\n",
    "\n",
    "    if result is None:\n",
    "        result = []\n",
    "        # print(label)\n",
    "\n",
    "    id1 = torch.max(outputs, dim=1)[1]\n",
    "    # id2 = torch.max(label, dim=1)[1]\n",
    "    id2 = label\n",
    "    nr_classes = outputs.size(1)\n",
    "    while len(result) < nr_classes:\n",
    "        result.append({\"TP\": 0, \"FN\": 0, \"FP\": 0, \"TN\": 0})\n",
    "    for a in range(0, len(id1)):\n",
    "        # if len(result) < a:\n",
    "        #    result.append({\"TP\": 0, \"FN\": 0, \"FP\": 0, \"TN\": 0})\n",
    "\n",
    "        it_is = int(id1[a])\n",
    "        should_be = int(id2[a])\n",
    "        if it_is == should_be:\n",
    "            result[it_is][\"TP\"] += 1\n",
    "        else:\n",
    "            result[it_is][\"FP\"] += 1\n",
    "            result[should_be][\"FN\"] += 1\n",
    "\n",
    "    _, prediction = torch.topk(outputs, 2, 1, largest=True)\n",
    "    prediction1 = prediction[:, 0:1]\n",
    "    prediction2 = prediction[:, 1:]\n",
    "\n",
    "    prediction1 = prediction1.view(-1)\n",
    "    prediction2 = prediction2.view(-1)\n",
    "\n",
    "    return result\n",
    "accuracy_function_dic = {\n",
    "    \"SingleLabelTop1\": single_label_top1_accuracy,\n",
    "    \"MultiLabel\": multi_label_accuracy,\n",
    "    \"Null\": null_accuracy_function\n",
    "}\n",
    "\n",
    "model =BertPoint(config, 0,'', \"BertPoint\")\n",
    "\n",
    "\n",
    "model_list = {\n",
    "    \"BertPoint\": BertPoint,\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model_name):\n",
    "    if model_name in model_list.keys():\n",
    "        return model_list[model_name]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "model = get_model('BertPoint')(config, 0)\n",
    "\n",
    "parameters = torch.load('/home/hdj/BERT-PLI-IJCAI2020/output/model/task2/1_save.pkl')\n",
    "\n",
    "def load_state_keywise(model, pretrained_dict):\n",
    "    model_dict = model.state_dict()\n",
    "    # print(model_dict.keys())\n",
    "    # input(\"continue?\")\n",
    "    tmp_cnt = 0\n",
    "    for k, v in pretrained_dict.items():\n",
    "        # kk = k.replace(\"module.\", \"\")\n",
    "        # print('k=', k)\n",
    "        # input(\"continue?\")\n",
    "        # print('kk=', kk)\n",
    "        # input(\"continue?\")\n",
    "        \n",
    "        if k in model_dict and v.size() == model_dict[k].size():\n",
    "            model_dict[k] = v\n",
    "            tmp_cnt += 1\n",
    "            \n",
    "            print('here',k)\n",
    "        else:\n",
    "            continue\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "model = load_state_keywise(model, parameters[\"model\"])\n",
    "\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model\n",
    "# if hasattr(model, 'module'):\n",
    "#     print('这个是存在module的****')\n",
    "# elif hasattr(model,'modules'):\n",
    "#     print('这个是存在modules的////')\n",
    "# else:\n",
    "#     print('什么都么有++++')\n",
    "# for key, val in model_to_save.state_dict().items():\n",
    "#     print('&'*10,key)\n",
    "\n",
    "#TODO \n",
    "parameters = torch.load('/home/hdj/BERT-PLI-IJCAI2020/output/model/task2/1.pkl')\n",
    "\n",
    "for key,val in parameters.items():\n",
    "    print(key)\n",
    "\n",
    "if hasattr(model, 'module'):\n",
    "    print('has module')\n",
    "else:\n",
    "    print('model')\n",
    "\n",
    "# for key,val in model.state_dict().items():\n",
    "#     print(key)\n",
    "\n",
    "# for key in model.modules().state_dict():\n",
    "#     print(key)\n",
    "\n",
    "# model_dict = model.state_dict()\n",
    "# for (key1,val1),(key2,val2) in zip(model_dict.items(),parameters['model'].items()):\n",
    "#     print(key1,' *** ',key2,' // ',key1.endswith(key2) )\n",
    "# # for key,val in model.children:\n",
    "# #     print(key)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"/data/hdj/data/CodeBERT/codesearch/java_model_codesearch/512_0202_del_import_0.62_0.14_0.18/checkpoint-best\")\n",
    "\n",
    "output=None\n",
    "for i in range(2):\n",
    "    a=torch.tensor([[1,1,1],[2,2,2]])\n",
    "    print(a.shape)\n",
    "    # b=torch.tensor([[3,3,3],[4,4,4]])\n",
    "    if   output==None:\n",
    "        output=a.reshape(1,-1)\n",
    "    else:\n",
    "        \n",
    "        output=torch.cat([output,a.view(1,-1)],axis=0)\n",
    "#     output.append()\n",
    "output\n",
    "\n",
    "a=torch.tensor([[1,1,2,2,2]])\n",
    "output1=torch.cat([a,a],axis=0)\n",
    "output1\n",
    "\n",
    "# if hasattr(model, 'module'):\n",
    "#     print('t')\n",
    "# else:\n",
    "#     print('f')\n",
    "# for key,val in model.state_dict().items():\n",
    "#     print(key)\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model\n",
    "# save_params = {\n",
    "#         \"model\": model_to_save.state_dict(),\n",
    "#         \"optimizer_name\": 'no_exists',\n",
    "#         \"optimizer\": 'optimizer.state_dict()',\n",
    "#         \"trained_epoch\": 3,\n",
    "#         \"global_step\":1\n",
    "# }\n",
    "# torch.save(save_params, '/home/hdj/BERT-PLI-IJCAI2020/output/filenamed.pkl')\n",
    "\n",
    "filename=torch.load('/home/hdj/BERT-PLI-IJCAI2020/output/filenamed.pkl')\n",
    "filenameModel=filename['model']\n",
    "\n",
    "\n",
    "# fine_tune=torch.load('/home/hdj/BERT-PLI-IJCAI2020/output/model/task2/1.pkl')\n",
    "# fine_tuneModel=fine_tune['model']\n",
    "\n",
    "# model.state_dict()\n",
    "\n",
    "# filenameModel\n",
    "\n",
    "# for key,val in filename['model'].items():\n",
    "#     print(key)\n",
    "\n",
    "# for key,val in parameters.items():\n",
    "#     print(key)\n",
    "\n",
    "\n",
    "# 模型需要的样例结束\n",
    "\n",
    "# tomcat_ast_file = pickle.loads(tomcat_ast_cache_collection_db['5d3a94c94ec7de55d2d83b0c8ba4ca4689626309'])\n",
    "# tomcat_ast_file['methodContent']\n",
    "\n",
    "# 根据bid 和fid获取相应的字符结束\n",
    "\n",
    "all_results_df_top_k.index.get_level_values(0).unique().shape[0]\n",
    "3645/500\n",
    "\n",
    "all_results_df_top_k.loc[500:999].shape,all_results_df_top_k.loc[1000:1499]\n",
    "\n",
    "print(all_results_df_top_k.loc[4143].shape)\n",
    "all_results_df_top_k.loc[4143]\n",
    "\n",
    "top_k=sorted_df[:50]\n",
    "\n",
    "# print(top_k.head())\n",
    "print(top_k.shape)\n",
    "top_k_pos=top_k['used_in_fix'].sum()\n",
    "all_pos=sorted_df['used_in_fix'].sum()\n",
    "print('top_k :',top_k_pos)\n",
    "print('all :',all_pos)\n",
    "print('recall :',top_k_pos/all_pos)\n",
    "\n",
    "# 计算前k个的recall，将前k个文件保存起来结束\n",
    "\n",
    "sum_val=0\n",
    "average_precision_per_bug_report = []\n",
    "reciprocal_ranks = []\n",
    "k_range=range(1, 21)\n",
    "bug_report_number = 0\n",
    "accuracy_at_k = dict.fromkeys(k_range, 0)\n",
    "for i,(bug_report, bug_report_files_dataframe)  in enumerate(all_results_df_before.groupby(level=0, sort=False)):\n",
    "    print('index :',i,bug_report,bug_report_files_dataframe.shape[0])\n",
    "    sum_val+=bug_report_files_dataframe.shape[0]\n",
    "#     print(bug_report_files_dataframe)\n",
    "    min_fix_result = bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix'] == 1.0]['result'].min()\n",
    "    bug_report_files_dataframe2 = bug_report_files_dataframe[bug_report_files_dataframe[\"result\"] >= min_fix_result]\n",
    "#     print('取fix得分最小值之前的所有文件 ',bug_report_files_dataframe2.shape,' 过滤前大小 :',bug_report_files_dataframe.shape,\n",
    "#           ' min_fix_result :',min_fix_result)\n",
    "    sorted_df = bug_report_files_dataframe2.sort_values(ascending=False, by=['result'])\n",
    "    #假如过滤后没有值，直接用原始的dataframe\n",
    "    if sorted_df.shape[0] == 0:\n",
    "        sorted_df = bug_report_files_dataframe.copy().sort_values(ascending=False, by=['result'])\n",
    "    #为了代码的健壮性\n",
    "    \n",
    "    #保存在k位置正例的比例\n",
    "    precision_at_k = []\n",
    "    \n",
    "    # precision per k in range\n",
    "    #给原始数据增加一列position，使用tmp保存\n",
    "    tmp = sorted_df\n",
    "    a = range(1, tmp.shape[0] + 1)\n",
    "    tmp['position'] = pd.Series(a, index=tmp.index)\n",
    "#     print(tmp)\n",
    "    #保存的是fix的排序位置\n",
    "    large_k_p = tmp[(tmp['used_in_fix'] == 1.0)]['position'].tolist()\n",
    "#     print('large_k_p :',large_k_p)\n",
    "    #保存的是result的升序排列\n",
    "    unique_results = sorted_df['result'].unique().tolist()\n",
    "    unique_results.sort()\n",
    "#     print('unique_results :',unique_results)\n",
    "    #计算在k位置前有多少正例,计算的是MAP的值\n",
    "    for fk in large_k_p:#[4, 186]\n",
    "        k = int(fk)\n",
    "        #取\n",
    "        k_largest = unique_results[-k:]\n",
    "        largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "        real_fixes_at_k = (largest_at_k['used_in_fix'] == 1.0).sum()\n",
    "        #正例个数/位置k\n",
    "        p = float(real_fixes_at_k) / float(k)\n",
    "        precision_at_k.append(p)\n",
    "#     print('precision_at_k :',precision_at_k)\n",
    "    average_precision = pd.Series(precision_at_k).mean()\n",
    "    average_precision_per_bug_report.append(average_precision)\n",
    "#     print('average_precision_per_bug_report :',average_precision_per_bug_report)\n",
    "    #保存MAP完毕\n",
    "    \n",
    "    #计算TOP20\n",
    "    for k in k_range:\n",
    "        k_largest = unique_results[-k:]\n",
    "        largest_at_k = sorted_df[sorted_df['result'] >= min(k_largest)]\n",
    "        real_fixes_at_k = largest_at_k['used_in_fix'][(largest_at_k['used_in_fix'] == 1.0)].count()\n",
    "        if real_fixes_at_k >= 1:\n",
    "            accuracy_at_k[k] += 1\n",
    "    #计算TOP20完毕\n",
    "    \n",
    "    #计算MRR\n",
    "    indexes_of_fixes = np.flatnonzero(sorted_df['used_in_fix'] == 1.0)\n",
    "#     print('indexes_of_fixes :',indexes_of_fixes)\n",
    "    if indexes_of_fixes.size == 0:\n",
    "        reciprocal_ranks.append(0.0)\n",
    "    else:\n",
    "        first_index = indexes_of_fixes[0]\n",
    "        reciprocal_rank = 1.0 / (first_index + 1)\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "#     print('reciprocal_ranks :',reciprocal_ranks)\n",
    "    #计算MRR完毕\n",
    "    bug_report_number += 1\n",
    "#     print('*'*10)\n",
    "#     if i==10:\n",
    "#         break\n",
    "\n",
    "mean_average_precision = pd.Series(average_precision_per_bug_report).mean()\n",
    "    # print('mean average precision', mean_average_precision)\n",
    "mean_reciprocal_rank = pd.Series(reciprocal_ranks).mean()\n",
    "# print('mean reciprocal rank', mean_reciprocal_rank)\n",
    "for k in k_range:\n",
    "    accuracy_at_k[k] = accuracy_at_k[k] / bug_report_number\n",
    "\n",
    "mean_average_precision,mean_reciprocal_rank,accuracy_at_k\n",
    "\n",
    "sum([0.25, 0.010752688172043012])/2\n",
    "\n",
    "# 指标计算结束\n",
    "\n",
    "all_results_df_before.index.get_level_values(0).unique().shape[0]\n",
    "\n",
    "fold_test_1 = pd.read_pickle('/data/hdj/tracking_buggy_files/swt/swt_normalized_testing_fold_1')\n",
    "\n",
    "from skopt import *\n",
    "fold_number, fold_testing, fold_training = load('/data/hdj/tracking_buggy_files/joblib_memmap_swt/swt/data_memmap', mmap_mode=\"r\")\n",
    "\n",
    "fold_number,len(fold_testing)\n",
    "\n",
    "# all_rows=0\n",
    "# for i in range(1,9):\n",
    "#     print(fold_testing[i].shape[0])\n",
    "#     all_rows+=fold_testing[i].shape[0]\n",
    "# print(all_rows)\n",
    "\n",
    "# for i in range(500,1000):\n",
    "#     print(fold_testing[1].loc[i].shape,all_results_df_before.loc[i].shape)\n",
    "\n",
    "all_results_df_after.shape,type(all_results_df_after)\n",
    "\n",
    "all_results_df_after.to_csv('/data/hdj/tracking_buggy_files/joblib_memmap_tomcat/after.csv')\n",
    "\n",
    "all_results_df_after_check=pd.read_csv('/data/hdj/tracking_buggy_files/joblib_memmap_tomcat/after.csv')\n",
    "all_results_df_after_check.shape\n",
    "\n",
    "type(queries),type(corpus),len(queries),len(corpus)\n",
    "\n",
    "# corpus.keys()\n",
    "\n",
    "# # aspectj_test=pd.read_csv('/data/hdj/SourceFile/data/sourceFile_aspectj/aspectj_test.csv')\n",
    "# all_data=pd.read_csv('/data/hdj/SourceFile/data/sourceFile_aspectj/all_data.csv')\n",
    "# all_data.head()\n",
    "# # aspectj_test[['bug_id','files']].head()\n",
    "\n",
    "all_fixed_files=[]\n",
    "for file in all_data['files'].values:\n",
    "    all_fixed_files.extend(eval(file))\n",
    "\n",
    "len(all_fixed_files)\n",
    "# all_fixed_files\n",
    "with open('/data/hdj/SourceFile/data/sourceFile_aspectj/fixed_files.txt','w',encoding='utf-8') as f_out:\n",
    "    for file in all_fixed_files:\n",
    "        f_out.write(file+'\\n')\n",
    "\n",
    "src_addresses = glob.glob(str('/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj') + '/**/*.java', recursive=True)\n",
    "len(src_addresses)\n",
    "\n",
    "# aspectj_test_pos_neg['path'][1000],aspectj_test_pos_neg['path'][2]\n",
    "\n",
    "# aspectj_test_pos_neg=pd.read_csv('/data/hdj/SourceFile/data/sourceFile_aspectj/aspectj_test_pos_neg.csv')\n",
    "# aspectj_test_pos_neg.head(1002)\n",
    "\n",
    "aspectj_test=pd.read_csv('/data/hdj/SourceFile/data/sourceFile_aspectj/aspectj_test.csv')\n",
    "# aspectj_test[['bug_id','files']].head()\n",
    "bug_ids=aspectj_test['bug_id'].values\n",
    "files=aspectj_test['files'].values\n",
    "for i,(bid,file) in enumerate(zip(bug_ids,files)):\n",
    "    print(i,bid,file)\n",
    "    if i==10:\n",
    "        break\n",
    "tests/bugs163/pr251326/pkgA/Target.java\n",
    "\n",
    "# #测试DictVectorizer()如何使用\n",
    "# import pandas as pd\n",
    "# from collections import Counter\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# data=[]\n",
    "# def convert(dict_list):\n",
    "#     counter_list = []\n",
    "#     for d in dict_list:\n",
    "#         counter_list.append(Counter(d))\n",
    "#     return counter_list\n",
    "# test_dict={'a':1,'b':2,'d':3}\n",
    "# data.extend(convert([test_dict]))\n",
    "\n",
    "# test_list=[{'a':1,'b':2,'e':3},{'a':1,'b':2,'f':3},{'a':1,'b':2,'g':3}]\n",
    "# data.extend(convert(test_list))\n",
    "\n",
    "# vectorizer = DictVectorizer()\n",
    "# vectorized_data = vectorizer.fit_transform(data)\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "# feature_names_lenghts_dict = {}\n",
    "# for i, feature_name in enumerate(feature_names):\n",
    "#     feature_names_lenghts_dict[i] = feature_name\n",
    "# feature_names_lenghts_dict\n",
    "# # vectorized_data.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
