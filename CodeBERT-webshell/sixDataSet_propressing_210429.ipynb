{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inflection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3f5c31f7fb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minflection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inflection'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import inflection\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from assets import stop_words, java_keywords\n",
    "from parsers import Parser\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi, BM25Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8efdb444d6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/hdj/tracking_buggy_files/joblib_memmap_aspectj/code_urls.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "b = np.load(\"/data/hdj/tracking_buggy_files/joblib_memmap_aspectj/code_urls.npy\")\n",
    "a=np.load(\"/data/hdj/tracking_buggy_files/joblib_memmap_aspectj/nl_urls.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0aa0b027fcb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src='/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj'\n",
    "src_addresses = glob.glob(src + '/**/*.java', recursive=True)\n",
    "len(src_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/data/hdj/cross_project_trans/report/aspectj_report_clean.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_id</th>\n",
       "      <th>fixed_files</th>\n",
       "      <th>pos_tagged_descriptions</th>\n",
       "      <th>pos_tagged_summarys</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>summarys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>70773</td>\n",
       "      <td>['org.aspectj/modules/org.aspectj.ajdt.core/sr...</td>\n",
       "      <td>{'stemmed': ['aspectj', 'eclips', 'build', 'id...</td>\n",
       "      <td>{'stemmed': ['aspectj', 'error', 'conncurrentm...</td>\n",
       "      <td>{'stemmed': ['aspectj', 'eclips', 'build', 'id...</td>\n",
       "      <td>{'stemmed': ['aspectj', 'error', 'conncurrentm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>125810</td>\n",
       "      <td>['org.aspectj/modules/org.aspectj.ajdt.core/sr...</td>\n",
       "      <td>{'stemmed': ['error', 'issu', 'implement', 'po...</td>\n",
       "      <td>{'stemmed': ['miss', 'error', 'inherit', 'poin...</td>\n",
       "      <td>{'stemmed': ['error', 'issu', 'sub', 'aspect',...</td>\n",
       "      <td>{'stemmed': ['miss', 'error', 'inherit', 'poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>128655</td>\n",
       "      <td>['org.aspectj/modules/org.aspectj.ajdt.core/sr...</td>\n",
       "      <td>{'stemmed': ['version', 'aspectj', 'version', ...</td>\n",
       "      <td>{'stemmed': ['annot', 'compil', 'crash', 'decl...</td>\n",
       "      <td>{'stemmed': ['ajdt', 'version', 'aspectj', 've...</td>\n",
       "      <td>{'stemmed': ['annot', 'compil', 'crash', 'poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>148536</td>\n",
       "      <td>['org.aspectj/modules/org.aspectj.ajdt.core/sr...</td>\n",
       "      <td>{'stemmed': ['aspect', 'bug', 'declar', 'type'...</td>\n",
       "      <td>{'stemmed': ['nullpointerexcept', 'declar', 'a...</td>\n",
       "      <td>{'stemmed': ['aspect', 'bug', 'declar', 'type'...</td>\n",
       "      <td>{'stemmed': ['nullpointerexcept', 'declar', 'i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bug_id                                        fixed_files  \\\n",
       "82    70773  ['org.aspectj/modules/org.aspectj.ajdt.core/sr...   \n",
       "184  125810  ['org.aspectj/modules/org.aspectj.ajdt.core/sr...   \n",
       "192  128655  ['org.aspectj/modules/org.aspectj.ajdt.core/sr...   \n",
       "235  148536  ['org.aspectj/modules/org.aspectj.ajdt.core/sr...   \n",
       "\n",
       "                               pos_tagged_descriptions  \\\n",
       "82   {'stemmed': ['aspectj', 'eclips', 'build', 'id...   \n",
       "184  {'stemmed': ['error', 'issu', 'implement', 'po...   \n",
       "192  {'stemmed': ['version', 'aspectj', 'version', ...   \n",
       "235  {'stemmed': ['aspect', 'bug', 'declar', 'type'...   \n",
       "\n",
       "                                   pos_tagged_summarys  \\\n",
       "82   {'stemmed': ['aspectj', 'error', 'conncurrentm...   \n",
       "184  {'stemmed': ['miss', 'error', 'inherit', 'poin...   \n",
       "192  {'stemmed': ['annot', 'compil', 'crash', 'decl...   \n",
       "235  {'stemmed': ['nullpointerexcept', 'declar', 'a...   \n",
       "\n",
       "                                          descriptions  \\\n",
       "82   {'stemmed': ['aspectj', 'eclips', 'build', 'id...   \n",
       "184  {'stemmed': ['error', 'issu', 'sub', 'aspect',...   \n",
       "192  {'stemmed': ['ajdt', 'version', 'aspectj', 've...   \n",
       "235  {'stemmed': ['aspect', 'bug', 'declar', 'type'...   \n",
       "\n",
       "                                              summarys  \n",
       "82   {'stemmed': ['aspectj', 'error', 'conncurrentm...  \n",
       "184  {'stemmed': ['miss', 'error', 'inherit', 'poin...  \n",
       "192  {'stemmed': ['annot', 'compil', 'crash', 'poss...  \n",
       "235  {'stemmed': ['nullpointerexcept', 'declar', 'i...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['fixed_files']==\"['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/compiler/lookup/AjLookupEnvironment.java']\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len : 286 264\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/bcel/LazyMethodGen.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjState.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/tools/ajc/Main.java']\n",
      "已经出现了 : ['org.aspectj/modules/runtime/src/org/aspectj/lang/SoftException.java']\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/bcel/BcelShadow.java']\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/AsmRelationshipProvider.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/EclipseAdapterUtils.java']\n",
      "已经出现了 : ['org.aspectj/modules/asm/src/org/aspectj/asm/internal/ProgramElement.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/compiler/ast/AspectDeclaration.java']\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/tools/WeavingAdaptor.java']\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/UnresolvedType.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/compiler/lookup/AjLookupEnvironment.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjBuildManager.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/compiler/lookup/AjLookupEnvironment.java']\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/bcel/ClassPathManager.java']\n",
      "已经出现了 : ['org.aspectj/modules/taskdefs/src/org/aspectj/tools/ant/taskdefs/AjcTask.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjBuildManager.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjBuildManager.java']\n",
      "已经出现了 : ['org.aspectj/modules/taskdefs/src/org/aspectj/tools/ant/taskdefs/AjcTask.java', 'org.aspectj/modules/taskdefs/testsrc/org/aspectj/tools/ant/taskdefs/AjcTaskTest.java']\n",
      "已经出现了 : ['org.aspectj/modules/org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/compiler/lookup/AjLookupEnvironment.java']\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/tools/WeavingAdaptor.java']\n",
      "已经出现了 : ['org.aspectj/modules/weaver/src/org/aspectj/weaver/bcel/BcelShadow.java']\n"
     ]
    }
   ],
   "source": [
    "not_duplicate=data['fixed_files'].values\n",
    "duplicate=set(data['fixed_files'].values)\n",
    "print('len :',len(not_duplicate),len(duplicate))\n",
    "has_exist=set()\n",
    "for string in not_duplicate:\n",
    "#     print('not dup :',string)\n",
    "    if string not in has_exist:\n",
    "#         print(string)\n",
    "        has_exist.add(string)\n",
    "    else:\n",
    "        print('已经出现了 :',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286, 264)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data['bug_id'].values)),len(set(data['fixed_files'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SrcPreprocessing_fulltext:\n",
    "    def __init__(self,code_dir):\n",
    "        self.src_files=self.get_repo_source_files(code_dir)\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"Tokenizing source codes into tokens\"\"\"\n",
    "        for key,vals in self.src_files.items():\n",
    "            tmp=[]\n",
    "            for val in vals:\n",
    "                tmp.append(nltk.wordpunct_tokenize(val))\n",
    "            self.src_files[key]= tmp\n",
    "\n",
    "    def _split_camelcase(self,tokens):\n",
    "\n",
    "        # Copy tokens\n",
    "        returning_tokens = tokens[:]\n",
    "\n",
    "        for token in tokens:\n",
    "            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n",
    "\n",
    "            # If token is split into some other tokens\n",
    "            if len(split_tokens) > 1:\n",
    "                returning_tokens.remove(token)\n",
    "                # Camel case detection for new tokens\n",
    "                for st in split_tokens:\n",
    "                    camel_split = inflection.underscore(st).split('_')\n",
    "                    if len(camel_split) > 1:\n",
    "                        returning_tokens.append(st)\n",
    "                        returning_tokens += camel_split\n",
    "                    else:\n",
    "                        returning_tokens.append(st)\n",
    "            else:\n",
    "                camel_split = inflection.underscore(token).split('_')\n",
    "                if len(camel_split) > 1:\n",
    "                    returning_tokens += camel_split\n",
    "\n",
    "        return returning_tokens\n",
    "\n",
    "    def split_camelcase(self):\n",
    "        \"\"\"Split CamelCase identifiers\"\"\"\n",
    "\n",
    "        for key,val_list in self.src_files.items():\n",
    "            tmp = []\n",
    "            # print('vals',val_list)\n",
    "            for vals in val_list:\n",
    "                # print('vals',vals)\n",
    "                tmp.append(self._split_camelcase(vals))\n",
    "            self.src_files[key] = tmp\n",
    "    def normalize(self):\n",
    "        \"\"\"Removing punctuation, numbers and also lowercase conversion\"\"\"\n",
    "\n",
    "        # Building a translate table for punctuation and number removal\n",
    "        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n",
    "\n",
    "        for key,vals in self.src_files.items():\n",
    "            tmp = []\n",
    "            for val in vals:\n",
    "                # print(val)\n",
    "                content_punctnum_rem = [token.translate(punctnum_table)for token in val]\n",
    "                tmp.append([token.lower() for token in content_punctnum_rem if token])\n",
    "            self.src_files[key] = tmp\n",
    "    def remove_stopwords(self):\n",
    "        \"\"\"Removing stop words from tokens\"\"\"\n",
    "\n",
    "        for key, val_list in self.src_files.items():\n",
    "            tmp=[]\n",
    "            for val in val_list:\n",
    "                after_removed=[token for token in val if token not in stop_words]\n",
    "                if len(after_removed)>0:\n",
    "                    tmp.append(after_removed)\n",
    "            self.src_files[key] = tmp\n",
    "    def remove_java_keywords(self):\n",
    "        \"\"\"Removing Java language keywords from tokens\"\"\"\n",
    "\n",
    "        for key, val_list in self.src_files.items():\n",
    "            tmp=[]\n",
    "            for val in val_list:\n",
    "                after_removed =[token for token in val  if token not in java_keywords]\n",
    "                if len(after_removed) > 0:\n",
    "                    tmp.append(after_removed)\n",
    "            self.src_files[key] = tmp\n",
    "\n",
    "    def stem(self):\n",
    "        \"\"\"Stemming tokens\"\"\"\n",
    "\n",
    "        # Stemmer instance\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        for key, val_list in self.src_files.items():\n",
    "            tmp = []\n",
    "            for val in val_list:\n",
    "                tmp.append(dict(zip(['stemmed', 'unstemmed'],[[stemmer.stem(token) for token in val], val])))\n",
    "            self.src_files[key] = tmp\n",
    "    def preprocess(self):\n",
    "        \"\"\"Run the preprocessing\"\"\"\n",
    "\n",
    "#         self.pos_tagging()\n",
    "        self.tokenize()\n",
    "#         self.split_camelcase()\n",
    "#         self.normalize()\n",
    "        self.remove_stopwords()\n",
    "#         self.remove_java_keywords()\n",
    "        self.stem()\n",
    "    def get_repo_source_files(self,code_dir):\n",
    "        files = dict()\n",
    "        start_dir = os.path.normpath(code_dir)  # 规范path字符串形式\n",
    "        for dir_, dir_names, file_names in os.walk(start_dir):  # os.walk()方法用于通过在目录树中游走输出在目录中的文件名，向上或者向下\n",
    "            for filename in [f for f in file_names if f.endswith(\".java\")]:\n",
    "                src_name = os.path.join(dir_, filename)  # 所有java文件名\n",
    "                try:\n",
    "                    with open(src_name, encoding='utf-8', mode='r') as src_file:\n",
    "                        lines = src_file.readlines()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "                file_key = src_name.split(start_dir)[1]\n",
    "                file_key = file_key[len(os.sep):]  # os.sep 不用考虑是linux 的‘/’ 或者是windows的 ‘\\’\n",
    "\n",
    "                # 过滤掉 copyright\n",
    "                new_lines = []\n",
    "                beforeTag = True\n",
    "                for line in lines:\n",
    "                    line = line.lstrip().replace('\\n', '')\n",
    "                    if 'package' in line or 'import' in line or 'class' in line:\n",
    "                        beforeTag = False\n",
    "                    if beforeTag == False:\n",
    "                        new_lines.append(line)\n",
    "\n",
    "                # 分离code和description\n",
    "                source = []\n",
    "                # if file_key == 'org.aspectj\\modules\\\\ajde\\\\testdata\\examples\\coverage\\ModelCoverage.java':\n",
    "                #     print('newline: ',new_lines)\n",
    "                for line in new_lines:\n",
    "                    # if file_key == 'org.aspectj\\modules\\\\ajde\\\\testdata\\examples\\coverage\\ModelCoverage.java':\n",
    "                    #     print(line)\n",
    "                    if line == \"\": continue\n",
    "                    if line.startswith(\"@\"): continue\n",
    "                    if line.find('/*') >= 0: continue\n",
    "                    if line.startswith('*'): continue\n",
    "                    if line.find('*/') >= 0: continue\n",
    "                    # if line.find('import')>=0:continue\n",
    "                    if line.startswith('//'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if line.find('//') >= 0:\n",
    "                            index = line.index('//')\n",
    "                            line = line[:index]\n",
    "                        # source += (line + '\\n')\n",
    "                        source.append(line + '\\n')\n",
    "\n",
    "                # print(source,'\\n******************************\\n')\n",
    "                # print(file_key)\n",
    "                # if file_key=='org.aspectj\\modules\\\\ajde\\\\testdata\\examples\\coverage\\ModelCoverage.java':\n",
    "                #     print('here ',file_key,source)\n",
    "                files[file_key] = source\n",
    "                # break\n",
    "        # self.src_files=files\n",
    "\n",
    "        # for path,code in files.items():\n",
    "        #     f = open('hdj.java','w',encoding='utf-8')\n",
    "        #     # print(code,type(code))\n",
    "        #     f.write(''.join(code))\n",
    "            # break\n",
    "        # out = pd.DataFrame()\n",
    "        # out['path'] = files.keys()\n",
    "        # out['source'] = files.values()\n",
    "        # out.to_csv('code.csv', index=False)  #\n",
    "        # return out\n",
    "        return  files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'utf-8' codec can't decode byte 0xe9 in position 426: invalid continuation byte\n",
      "总共java文件数 ，处理后java文件数 2394 2393\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/decp/9052d5d AspectX.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs170/pr376351/be063b8 RAj.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs172/pr398588/96ebaae AspectB.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/multiIncremental/PR192877/inc1/src/9b68a31 Test.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/multiIncremental/PR192877/base/src/9b68a31 Test.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/multiIncremental/PR192877/base/src/9b68a31 Foo.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/multiIncremental/pr117209/base/src/551b9ca P.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/multiIncremental/pr117209/base/src/551b9ca DefaultInterfaceImplementationRecipe.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/multiIncremental/pr117209/base/src/551b9ca MyInterface_ch16.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/45bce91 EllipsesStar.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/fbc0aa3 DecwClassCastException.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/5e08765 BogusMessage.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/7322131 StaticInterfaceMethods.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/StringToString/0521e79 X.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/perCflowAndJar/5a07dce PerCFlowCompileFromJar.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/java5/arrayCloning/b5e6307 A.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs/throwsSignature/8660cc1 ExceptionAspect.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs153/pr148409/369de87 X.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs153/PR148219/07c2189 MyMessages.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs153/pr153845/70ae0f8 Interface.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs153/pr153845/70ae0f8 Aspect2.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/harness/047173e XLintcflow.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs174/pr368046/0c0adc5 Azpect.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs174/pr419279/b2cd5fa Code.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs163/pr249710/cffe291 Foo.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs150/87e5c2e PR109486.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs150/pr117681/11ab99f Audit.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs150/pr108902/2505485 Observer.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs151/pr124803/8676df9 TestAspect2.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs151/pr124803/8676df9 TestAspect.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs151/pr128744/7a2087e World.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/features151/serialveruid/6e6658a AnAspect.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs152/pr147801/13dde4e PreparedStatement.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs152/pr135068/9ffc63b A.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs152/pr135001/1a6f695 AbstractAspect.java  is null\n",
      "/data/hdj/SourceFile/data/clean/sourceFile_aspectj/org.aspectj/tests/bugs152/pr135001/1a6f695 ConcreteAspect.java  is null\n",
      "org.aspectj 共有 36 个文件被过滤掉\n"
     ]
    }
   ],
   "source": [
    "def mkdir(path):\n",
    "    index = path.rindex('/')\n",
    "    dir=path[:index]\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "def pathNorm(a,reg):\n",
    "    index = a.index(reg)\n",
    "    path = a[index:]\n",
    "    # path = '.'.join(path.split('/'))\n",
    "    return path\n",
    "def test2():\n",
    "    srcs = {'/data/hdj/SourceFile/data/sourceFile_aspectj/': 'org.aspectj',\n",
    "           }\n",
    "            # '/home/hdj/bug_localization/data/ZXing/ZXing-1.6': 'com/google/zxing'}\n",
    "    clean_path=['/data/hdj/SourceFile/data/clean/sourceFile_aspectj']\n",
    "    i = 0\n",
    "    for src,reg in srcs.items():\n",
    "        src_addresses = glob.glob(src + '/**/*.java', recursive=True)\n",
    "        code_prep = SrcPreprocessing_fulltext(src)\n",
    "        code_prep.preprocess()\n",
    "        print('总共java文件数 ，处理后java文件数',len(src_addresses),len(code_prep.src_files))\n",
    "        j=0\n",
    "        for path,val_list in code_prep.src_files.items():\n",
    "            path=pathNorm(path,reg)\n",
    "            path=os.path.join(clean_path[i],path)\n",
    "            # path=os.path.join(,path)\n",
    "            mkdir(path)\n",
    "            if len(val_list)==0:\n",
    "                print(path,' is null')\n",
    "                j+=1\n",
    "                continue\n",
    "            with open(path, 'w', encoding='utf-8') as f_out:\n",
    "                for val in val_list:\n",
    "                    f_out.write(' '.join(val['stemmed']) + '\\n')\n",
    "        print(reg,'共有',j,'个文件被过滤掉')\n",
    "        i+=1\n",
    "            #分为stemmed 和unstemmed文件写入，暂时一切从简\n",
    "            # print(path)\n",
    "            # index = path.rindex('.')\n",
    "            # with open(path[:index] + '_Stemmed.java', 'w', encoding='utf-8') as f_out:\n",
    "            #     for val in val_list:\n",
    "            #         f_out.write(' '.join(val['stemmed']) + '\\n')\n",
    "            # with open(path[:index] + '_Unstemmed.java', 'w', encoding='utf-8') as f_out:\n",
    "            #     for val in val_list:\n",
    "            #         f_out.write(' '.join(val['unstemmed']) + '\\n')\n",
    "            # print(key,val)\n",
    "            # break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # main()\n",
    "    test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理report信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t423257\tBug 423257 LTW - java.lang.VerifyError: Bad return type with generics and local variables\t\t2013-12-04 19:43:22\t1386200000\tresolved fixed\tdd88d21\t1386350000\torg.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjState.java tests/bugs175/pr423257/AspectX.java tests/bugs175/pr423257/Test.java tests/src/org/aspectj/systemtest/AllTests17.java tests/src/org/aspectj/systemtest/ajc175/Ajc175Tests.java tests/src/org/aspectj/systemtest/ajc175/AllTestsAspectJ175.java tests/src/org/aspectj/systemtest/incremental/tools/MultiProjectIncrementalTests.java weaver/src/org/aspectj/weaver/bcel/asm/StackMapAdder.java\t52:tests/src/org/aspectj/systemtest/incremental/tools/MultiProjectIncrementalTests.java 129:org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjState.java 179:weaver/src/org/aspectj/weaver/bcel/asm/StackMapAdder.java 258:tests/src/org/aspectj/systemtest/AllTests17.java\n",
      "\n",
      "['1', '423257', 'Bug 423257 LTW - java.lang.VerifyError: Bad return type with generics and local variables', '', '2013-12-04 19:43:22', '1386200000', 'resolved fixed', 'dd88d21', '1386350000', 'org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjState.java tests/bugs175/pr423257/AspectX.java tests/bugs175/pr423257/Test.java tests/src/org/aspectj/systemtest/AllTests17.java tests/src/org/aspectj/systemtest/ajc175/Ajc175Tests.java tests/src/org/aspectj/systemtest/ajc175/AllTestsAspectJ175.java tests/src/org/aspectj/systemtest/incremental/tools/MultiProjectIncrementalTests.java weaver/src/org/aspectj/weaver/bcel/asm/StackMapAdder.java', '52:tests/src/org/aspectj/systemtest/incremental/tools/MultiProjectIncrementalTests.java 129:org.aspectj.ajdt.core/src/org/aspectj/ajdt/internal/core/builder/AjState.java 179:weaver/src/org/aspectj/weaver/bcel/asm/StackMapAdder.java 258:tests/src/org/aspectj/systemtest/AllTests17.java\\n']\n"
     ]
    }
   ],
   "source": [
    "ids=[]\n",
    "bug_id=[]\n",
    "summary=[]\n",
    "description=[]\n",
    "report_time=[]\n",
    "report_timestamp=[]\n",
    "status=[]\n",
    "commit=[]\n",
    "commit_timestamp=[]\n",
    "files=[]\n",
    "with open('/data/hdj/SourceFile/data/sourceFile_aspectj/bugreport.txt','r',encoding='utf-8') as f_in:\n",
    "    for i,line in enumerate(f_in):\n",
    "        if i==0:\n",
    "            continue\n",
    "        print(line)\n",
    "        contents=line.split('\\t')\n",
    "        #id\tbug_id\tsummary\tdescription\treport_time\treport_timestamp\tstatus\tcommit\tcommit_timestamp\tfiles\n",
    "        print(contents)\n",
    "        ids.append(contents[0])\n",
    "        bug_id.append(contents[1])\n",
    "        summary.append(contents[2])\n",
    "        description.append(contents[3])\n",
    "        files.append(list(files))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GHM_Loss(nn.Module):\n",
    "    def __init__(self, bins, alpha):\n",
    "        super(GHM_Loss, self).__init__()\n",
    "        self._bins = bins\n",
    "        self._alpha = alpha\n",
    "        self._last_bin_count = None\n",
    "\n",
    "    def _g2bin(self, g):\n",
    "        return torch.floor(g * (self._bins - 0.0001)).long()\n",
    "\n",
    "    def _custom_loss(self, x, target, weight):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        g = torch.abs(self._custom_loss_grad(x, target)).detach()\n",
    "\n",
    "        bin_idx = self._g2bin(g)\n",
    "\n",
    "        bin_count = torch.zeros((self._bins))\n",
    "        for i in range(self._bins):\n",
    "            bin_count[i] = (bin_idx == i).sum().item()\n",
    "\n",
    "        N = (x.size(0) * x.size(1))\n",
    "\n",
    "        if self._last_bin_count is None:\n",
    "            self._last_bin_count = bin_count\n",
    "        else:\n",
    "            bin_count = self._alpha * self._last_bin_count + (1 - self._alpha) * bin_count\n",
    "            self._last_bin_count = bin_count\n",
    "\n",
    "        nonempty_bins = (bin_count > 0).sum().item()\n",
    "\n",
    "        gd = bin_count * nonempty_bins\n",
    "        gd = torch.clamp(gd, min=0.0001)\n",
    "        beta = N / gd\n",
    "\n",
    "        return self._custom_loss(x, target, beta[bin_idx])\n",
    "\n",
    "\n",
    "class GHMC_Loss(GHM_Loss):\n",
    "    def __init__(self, bins, alpha):\n",
    "        super(GHMC_Loss, self).__init__(bins, alpha)\n",
    "\n",
    "    def _custom_loss(self, x, target, weight):\n",
    "        return F.binary_cross_entropy_with_logits(x, target, weight=weight)\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        return torch.sigmoid(x).detach() - target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghm=GHMC_Loss(bins=30,alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 8\n",
    "class_num = 2\n",
    "label = np.random.randint(0,class_num,size=(batch_size,1))\n",
    "label = torch.LongTensor(label)\n",
    "print(label)\n",
    "y_one_hot = torch.zeros(batch_size,class_num).scatter_(1,label,1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([3, 2])\n",
      "tensor([[ 0.2826, -0.2776],\n",
      "        [ 0.5978,  0.8595],\n",
      "        [-1.0763,  0.5448]], device='cuda:0', grad_fn=<CopyBackwards>)\n",
      "tensor([[0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "pred = torch.randn((3,2), requires_grad=True)\n",
    "target=torch.tensor([[0.0,1.0],[1.0,0.0],[1.0,0.0]])\n",
    "\n",
    "# loss = F.binary_cross_entropy_with_logits(input, target)\n",
    "# loss.backward()\n",
    "pred=pred.to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "target=target.to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "print(pred.shape,target.shape)\n",
    "print(pred)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghm=ghm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.GHMC_Loss'>\n"
     ]
    }
   ],
   "source": [
    "print(type(ghm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss=ghm(pred,target)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "pred=torch.tensor([[0.2,0.8],[0.1,0.9],[0.4,0.6]], requires_grad=True)\n",
    "# pred=torch.tensor([[0.2],[0.9],[0.6]])\n",
    "# target=torch.tensor([[0.0,1.0],[1.0,0.0],[1.0,0.0]])\n",
    "target=torch.tensor([[0.0,1.],[1.,0.],[1.,1.]])\n",
    "print(pred.shape,target.shape)\n",
    "# loss=ghm(pred,target)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "loss = F.binary_cross_entropy_with_logits(input, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-2.2548, -0.7794, -0.3030], requires_grad=True), tensor([0., 0., 1.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input.shape,target.shape)\n",
    "input,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class focal_loss(nn.Module):\n",
    "    def __init__(self, alpha=[0.25,0.75], gamma=2, num_classes = 2, size_average=False):\n",
    "        \"\"\"\n",
    "        focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi)\n",
    "        步骤详细的实现了 focal_loss损失函数.\n",
    "        :param alpha: 阿尔法α,类别权重.      \n",
    "                    当α是列表时,为各类别权重；\n",
    "                    当α为常数时,类别权重为[α, 1-α, 1-α, ....],\n",
    "                    常用于目标检测算法中抑制背景类, \n",
    "                    retainnet中设置为0.25\n",
    "        :param gamma: 伽马γ,难易样本调节参数. retainnet中设置为2\n",
    "        :param num_classes: 类别数量\n",
    "        :param size_average: 损失计算方式,默认取均值\n",
    "        \"\"\"\n",
    "        super(focal_loss,self).__init__()\n",
    "        self.size_average = size_average\n",
    "        if isinstance(alpha,list):\n",
    "            assert len(alpha)==num_classes   \n",
    "            # α可以以list方式输入,\n",
    "            # size:[num_classes] 用于对不同类别精细地赋予权重\n",
    "            print(\"Focal_loss alpha = {}, 将对每一类权重进行精细化赋值\".format(alpha))\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        else:\n",
    "            assert alpha<1  #如果α为一个常数,则降低第一类的影响,在目标检测中为第一类\n",
    "            print(\"Focal_loss alpha = {} ,将对背景类进行衰减,请在目标检测任务中使用.\".format(alpha))\n",
    "            self.alpha = torch.zeros(num_classes)\n",
    "            self.alpha[0] += alpha\n",
    "            self.alpha[1:] += (1-alpha) # α 最终为 [ α, 1-α, 1-α, 1-α, 1-α, ...] size:[num_classes]\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        \"\"\"\n",
    "        focal_loss损失计算\n",
    "        :param preds: 预测类别. size:[B,N,C] or [B,C]    分\n",
    "                别对应与检测与分类任务, B 批次, N检测框数, C类别数\n",
    "        :param labels:  实际类别. size:[B,N] or [B]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # assert preds.dim()==2 and labels.dim()==1\n",
    "        preds = preds.view(-1,preds.size(-1))\n",
    "#         print('preds :',preds)\n",
    "        self.alpha = self.alpha.to(preds.device)\n",
    "        # 这里并没有直接使用log_softmax, 因为后面会用到softmax的结果(当然也可以使用log_softmax,然后进行exp操作)\n",
    "        preds_softmax = F.softmax(preds, dim=1) \n",
    "#         print('preds_softmax :',preds_softmax)\n",
    "        preds_logsoft = torch.log(preds_softmax)\n",
    "#         print('preds_logsoft :',preds_logsoft)        \n",
    "        # 这部分实现nll_loss ( crossempty = log_softmax + nll )\n",
    "        preds_softmax = preds_softmax.gather(1,labels.view(-1,1))   \n",
    "#         print('preds_softmax :',preds_softmax) \n",
    "        preds_logsoft = preds_logsoft.gather(1,labels.view(-1,1))\n",
    "#         print('preds_logsoft :',preds_logsoft) \n",
    "        self.alpha = self.alpha.gather(0,labels.view(-1))\n",
    "        loss = -torch.mul(torch.pow((1-preds_softmax), self.gamma), preds_logsoft)  \n",
    "        # torch.pow((1-preds_softmax), self.gamma) 为focal loss中 (1-pt)**γ\n",
    "\n",
    "        loss = torch.mul(self.alpha, loss.t())\n",
    "        if self.size_average:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.8000, 1.0000],\n",
       "        [0.1000, 0.9000, 2.0000],\n",
       "        [0.4000, 0.6000, 3.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "bert=torch.tensor([[0.2,0.8],[0.1,0.9],[0.4,0.6]])\n",
    "fea=torch.tensor([[1],[2],[3]])\n",
    "out=torch.cat((bert,fea),axis=1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=torch.tensor([[0.2,0.8],[0.1,0.9],[0.4,0.6]], requires_grad=True)\n",
    "# pred=torch.tensor([[0.2],[0.9],[0.6]])\n",
    "# target=torch.tensor([[0.0,1.0],[1.0,0.0],[1.0,0.0]])\n",
    "target=torch.tensor([[0],[1],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=[0.25,0.75], gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = torch.Tensor([gamma])\n",
    "        self.size_average = size_average\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            if self.alpha > 1:\n",
    "                raise ValueError('Not supported value, alpha should be small than 1.0')\n",
    "            else:\n",
    "                self.alpha = torch.Tensor([alpha, 1.0 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.alpha /= torch.sum(self.alpha)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # [N,C,H,W]->[N,C,H*W] ([N,C,D,H,W]->[N,C,D*H*W])\n",
    "        # target\n",
    "        # [N,1,D,H,W] ->[N*D*H*W,1]\n",
    "        if self.alpha.device != input.device:\n",
    "            self.alpha = torch.tensor(self.alpha, device=input.device)\n",
    "        target = target.view(-1, 1)\n",
    "        logpt = torch.log(input + 1e-10)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1, 1)\n",
    "        pt = torch.exp(logpt)\n",
    "        alpha = self.alpha.gather(0, target.view(-1))\n",
    "\n",
    "        gamma = self.gamma\n",
    "\n",
    "        if not self.gamma.device == input.device:\n",
    "            gamma = torch.tensor(self.gamma, device=input.device)\n",
    "\n",
    "        loss = -1 * alpha * torch.pow((1 - pt), gamma) * logpt\n",
    "        if self.size_average:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2164, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floss=FocalLoss()\n",
    "loss=floss(pred,target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal_loss alpha = [0.25, 0.75], 将对每一类权重进行精细化赋值\n"
     ]
    }
   ],
   "source": [
    "floss=focal_loss()\n",
    "loss=floss(pred,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2258, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
