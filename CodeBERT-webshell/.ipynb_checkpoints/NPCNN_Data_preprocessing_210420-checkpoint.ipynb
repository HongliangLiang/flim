{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里是NPCNN的数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys, re\n",
    "import os\n",
    "import csv\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from unqlite import UnQLite\n",
    "import string\n",
    "import re\n",
    "import inflection\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from assets import stop_words, java_keywords\n",
    "from parsers import Parser\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "# dataTest=pd.DataFrame()\n",
    "# dataTest['id']=[1,2,3]\n",
    "# dataTest['name']=['hdj','gzh','cxl']\n",
    "# dataTest.to_pickle('/data/hdj/cross_project_trans/report/aspectj_test_middle/student.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>used_in_fix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">b067e67</th>\n",
       "      <th>dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d</th>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.037999</td>\n",
       "      <td>0.272661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.011659</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.161792</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e4cac7453e1315a26c8542e13899a688fc7d265</th>\n",
       "      <td>0.018465</td>\n",
       "      <td>0.037999</td>\n",
       "      <td>0.272661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.161792</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8d3e3a67e2b54cef78b5348755252586d51bfaa</th>\n",
       "      <td>0.021243</td>\n",
       "      <td>0.037098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152437</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.050641</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.016424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065427</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dc52a6a4701375236460a7e752eaebfe1f363e0e</th>\n",
       "      <td>0.065562</td>\n",
       "      <td>0.037098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>0.095327</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.020406</td>\n",
       "      <td>0.007911</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4</th>\n",
       "      <td>0.019104</td>\n",
       "      <td>0.034676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141786</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>0.010049</td>\n",
       "      <td>0.089488</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.045398</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.110790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        f1        f2  \\\n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.016591  0.037999   \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.018465  0.037999   \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.021243  0.037098   \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.065562  0.037098   \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.019104  0.034676   \n",
       "\n",
       "                                                        f3   f4        f5  \\\n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.272661  0.0  0.142857   \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.272661  0.0  0.142857   \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.000000  0.0  0.000000   \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.000000  0.0  0.000000   \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.000000  0.0  0.000000   \n",
       "\n",
       "                                                     f6   f7   f8   f9  f10  \\\n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.025  0.0  0.0  0.0  0.0   \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.025  0.0  0.0  0.0  0.0   \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.000  0.0  0.0  0.0  0.0   \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.000  0.0  0.0  0.0  0.0   \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.000  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                                       f11       f12  \\\n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.235599  0.005200   \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.235599  0.005200   \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.152437  0.002281   \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.219169  0.095327   \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.141786  0.027632   \n",
       "\n",
       "                                                       f13       f14  \\\n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.011659  0.089488   \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.006148  0.089488   \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.050641  0.089488   \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.000691  0.089488   \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.010049  0.089488   \n",
       "\n",
       "                                                       f15       f16  \\\n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.004310  0.093458   \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.004310  0.093458   \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.000000  0.046729   \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.021552  0.042056   \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.034483  0.046729   \n",
       "\n",
       "                                                       f17       f18  \\\n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.017198  0.002109   \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.017198  0.002109   \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.016424  0.000000   \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.020406  0.007911   \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.045398  0.008755   \n",
       "\n",
       "                                                       f19  used_in_fix  \n",
       "b067e67 dcb5e1189cf30f98c28e3f7e02f3bd61f286ce5d  0.161792          0.0  \n",
       "        3e4cac7453e1315a26c8542e13899a688fc7d265  0.161792          0.0  \n",
       "        e8d3e3a67e2b54cef78b5348755252586d51bfaa  0.065427          0.0  \n",
       "        dc52a6a4701375236460a7e752eaebfe1f363e0e  0.007665          0.0  \n",
       "        a1ed589a9f443d1ce5cb0abbb25aa06d42d0c1a4  0.110790          0.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_dataframe=pd.read_pickle('/data/hdj/tracking_buggy_files/'+project+'/'+project+'_normalized_training_fold_'+str(0)+'_raw')\n",
    "train_all_dataframe.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_report_files_dataframe=train_all_dataframe.loc['b067e67']\n",
    "pos=bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix']==1.0][['used_in_fix']]\n",
    "neg=bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix']==0.0][['used_in_fix']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>used_in_fix</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94065bf02a63710434075209886f06f9464ed3a1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8d7063afbecfe86cb9ec5be3e059633a488a1c4f</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          used_in_fix\n",
       "fid                                                  \n",
       "94065bf02a63710434075209886f06f9464ed3a1          1.0\n",
       "8d7063afbecfe86cb9ec5be3e059633a488a1c4f          1.0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.index.name='fid'\n",
    "# bug_report_files_dataframe\n",
    "pos\n",
    "# bug_report_files_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>used_in_fix</th>\n",
       "      <th>bid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94065bf02a63710434075209886f06f9464ed3a1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8d7063afbecfe86cb9ec5be3e059633a488a1c4f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        fid  used_in_fix bid\n",
       "0  94065bf02a63710434075209886f06f9464ed3a1          1.0   a\n",
       "1  8d7063afbecfe86cb9ec5be3e059633a488a1c4f          1.0   a"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg.shape\n",
    "pos=pos.reset_index()\n",
    "pos['bid']='a'\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train=[]\n",
    "neg_train=[]\n",
    "\n",
    "for i,(bug_report, bug_report_files_dataframe) in enumerate(train_all_dataframe.groupby(level=0, sort=False)):\n",
    "    pos=bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix']==1.0][['used_in_fix']]\n",
    "    neg=bug_report_files_dataframe[bug_report_files_dataframe['used_in_fix']==0.0][['used_in_fix']]\n",
    "    pos[[\"used_in_fix\"]] = pos[[\"used_in_fix\"]].astype(int)\n",
    "    neg[[\"used_in_fix\"]] = neg[[\"used_in_fix\"]].astype(int)\n",
    "#     print(bug_report_files_dataframe)\n",
    "    pos=pos.reset_index()\n",
    "    neg=neg.reset_index()\n",
    "#     print(pos)\n",
    "    pos_train.append(pos)\n",
    "    neg_train.append(neg)\n",
    "#     if i==2:\n",
    "#         break\n",
    "pos_dataframe=pd.concat(pos_train,axis=0)\n",
    "neg_dataframe=pd.concat(neg_train,axis=0)\n",
    "pos_dataframe.rename(columns={'level_0':'bid','level_1':'fid'},inplace=True)\n",
    "neg_dataframe.rename(columns={'level_0':'bid','level_1':'fid'},inplace=True)\n",
    "pos_train_path=os.path.join(outpath,project+'_80_training_pos.csv')\n",
    "neg_train_path=os.path.join(outpath,project+'_80_training_neg.csv')\n",
    "pos_dataframe.to_csv(pos_train_path,index=False)\n",
    "neg_dataframe.to_csv(neg_train_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008199918000819992"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "820/100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid</th>\n",
       "      <th>fid</th>\n",
       "      <th>used_in_fix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1839d9b</td>\n",
       "      <td>8640d87eebe944043533a62885e33b04d5475d53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5ce168a</td>\n",
       "      <td>77adc4b41a8fc3eed6d7c62292be018cda642669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ce168a</td>\n",
       "      <td>6cbed3de32cbfb7970ff47cd2ae634138c2e5d8a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5ce168a</td>\n",
       "      <td>f61551cda8113971da3ea148a77d208ff4b89c5a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83e8870</td>\n",
       "      <td>4ad29aee46529969cf18949646ed741ae6703f10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bid                                       fid  used_in_fix\n",
       "0  1839d9b  8640d87eebe944043533a62885e33b04d5475d53            1\n",
       "0  5ce168a  77adc4b41a8fc3eed6d7c62292be018cda642669            1\n",
       "1  5ce168a  6cbed3de32cbfb7970ff47cd2ae634138c2e5d8a            1\n",
       "2  5ce168a  f61551cda8113971da3ea148a77d208ff4b89c5a            1\n",
       "0  83e8870  4ad29aee46529969cf18949646ed741ae6703f10            1"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dataframe.rename(columns={'level_0':'bid','level_1':'fid'},inplace=True)\n",
    "neg_dataframe.rename(columns={'level_0':'bid','level_1':'fid'},inplace=True)\n",
    "pos_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 3), (600, 3))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_dataframe.head()\n",
    "pos_dataframe.shape,neg_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>used_in_fix</th>\n",
       "      <th>f1</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid</th>\n",
       "      <th>fid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">a0c9251</th>\n",
       "      <th>aa245982fa51518c22c1817fd56ac88b4b921f80</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030239</td>\n",
       "      <td>0.588582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38b760ae592520a29eecbc29ac4b6665c9e9bf61</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.556395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebde7ae63aa647dc84eb1c29f1fa2133b4d1e705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026023</td>\n",
       "      <td>0.549675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b6789bd6c4f2ad5b82d0e4800e6c3924bc45589</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026685</td>\n",
       "      <td>0.550011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc6b590ab782fc46728d0561c084f3c5fa843a68</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>0.600155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  used_in_fix        f1  \\\n",
       "bid     fid                                                               \n",
       "a0c9251 aa245982fa51518c22c1817fd56ac88b4b921f80          0.0  0.030239   \n",
       "        38b760ae592520a29eecbc29ac4b6665c9e9bf61          0.0  0.010100   \n",
       "        ebde7ae63aa647dc84eb1c29f1fa2133b4d1e705          0.0  0.026023   \n",
       "        0b6789bd6c4f2ad5b82d0e4800e6c3924bc45589          0.0  0.026685   \n",
       "        bc6b590ab782fc46728d0561c084f3c5fa843a68          0.0  0.040819   \n",
       "\n",
       "                                                    result  \n",
       "bid     fid                                                 \n",
       "a0c9251 aa245982fa51518c22c1817fd56ac88b4b921f80  0.588582  \n",
       "        38b760ae592520a29eecbc29ac4b6665c9e9bf61  0.556395  \n",
       "        ebde7ae63aa647dc84eb1c29f1fa2133b4d1e705  0.549675  \n",
       "        0b6789bd6c4f2ad5b82d0e4800e6c3924bc45589  0.550011  \n",
       "        bc6b590ab782fc46728d0561c084f3c5fa843a68  0.600155  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df_before=pd.read_pickle('/data/hdj/tracking_buggy_files/joblib_memmap_'+project+'/all_results_df_before.pickle')\n",
    "all_results_df_before.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "project='swt'\n",
    "test_all_dataframe=pd.read_pickle('/data/hdj/tracking_buggy_files/'+project+'/'+project+'_normalized_testing_fold_'+str(1))\n",
    "test_all_dataframe.head()\n",
    "outpath='/data/hdj/cross_project_trans/report/bert/'\n",
    "for i,(bug_report, bug_report_files_dataframe) in enumerate(test_all_dataframe.groupby(level=0, sort=False)):\n",
    "    \n",
    "    result=all_results_df_before.loc[bug_report]\n",
    "    sorted_df = result.sort_values(ascending=False, by=['result'])\n",
    "    sorted_df=sorted_df.head(300)\n",
    "    sorted_df=sorted_df.reset_index()\n",
    "    sorted_df.head()\n",
    "    writePath=os.path.join(outpath,project+'_test','test'+str(i)+'.csv')\n",
    "#     print(sum(bug_report_files_dataframe['used_in_fix']))\n",
    "    sorted_df=sorted_df[['fid','used_in_fix']]\n",
    "    sorted_df['bid']=bug_report\n",
    "    sorted_df = sorted_df[['bid','fid','used_in_fix']]\n",
    "    sorted_df.to_csv(writePath,index=False)\n",
    "#     print(i,bug_report,sorted_df.shape)\n",
    "    #过滤掉topk之外的文件 取300\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid</th>\n",
       "      <th>fid</th>\n",
       "      <th>used_in_fix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0026caf</td>\n",
       "      <td>000f528d2d9c0bfefd8a7ea672093a0b21cabbf9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0026caf</td>\n",
       "      <td>004566737fca05244d8f3a9fd7b403ec6bddc453</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0026caf</td>\n",
       "      <td>00be184c4d31b50fa6a9305ae77a3c8846d2a4a7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0026caf</td>\n",
       "      <td>010a7b6426963ede4625b51d6a16f26ba12a327f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0026caf</td>\n",
       "      <td>011d765225ccde0aa890339b7a542502fe54d342</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bid                                       fid  used_in_fix\n",
       "0  0026caf  000f528d2d9c0bfefd8a7ea672093a0b21cabbf9          0.0\n",
       "1  0026caf  004566737fca05244d8f3a9fd7b403ec6bddc453          0.0\n",
       "2  0026caf  00be184c4d31b50fa6a9305ae77a3c8846d2a4a7          0.0\n",
       "3  0026caf  010a7b6426963ede4625b51d6a16f26ba12a327f          0.0\n",
       "4  0026caf  011d765225ccde0aa890339b7a542502fe54d342          0.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_report_files_dataframe.shape,result.shape\n",
    "bug_report_files_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(result['fid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_report_files_dataframe_filter=pd.merge(sorted_df,bug_report_files_dataframe,how='left',on=['fid','bid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 6)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_report_files_dataframe_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1019, 6), (1003, 5), (1374, 3))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_report_files_dataframe_filter.shape,result.shape,bug_report_files_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = result.sort_values(ascending=False, by=['result'])\n",
    "sorted_df=sorted_df.head(300)\n",
    "# result=result.reset_index()\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportIds=all_results_df_before.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, TREC=False):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?;\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip() if TREC else string.strip().lower()\n",
    "\n",
    "#\n",
    "# def load_bin_vec(fname, vocab):\n",
    "#     word_vecs = {}\n",
    "#     model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)#模型的导入\n",
    "#     for word in vocab:\n",
    "#         if word in model.vocab:\n",
    "#             word_vecs[word] = model[word]\n",
    "#     return word_vecs\n",
    "#\n",
    "#\n",
    "# def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "#     \"\"\"\n",
    "#     For words that occur in at least min_df documents, create a separate word vector.\n",
    "#     0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "#     \"\"\"\n",
    "#     # for word in vocab:\n",
    "#     #     if word not in word_vecs and vocab[word] >= min_df:\n",
    "#     #         word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "#     i = 0  # 统计多少不在预训练词表里\n",
    "#     for word in vocab:\n",
    "#         if word not in word_vecs and vocab[word] >= min_df:\n",
    "#             print('不在词表里词示例 ', word)\n",
    "#             i += 1\n",
    "#             word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "#     print('一共 ', i, ' 个词不在词表里')\n",
    "\n",
    "def load_bin_vec(google,hdj, vocab,hdjWord2Vec):\n",
    "    word_vecs = {}\n",
    "    if hdjWord2Vec:\n",
    "        w2v = Word2Vec.load(hdj)#模型的导入\n",
    "    else:\n",
    "        w2v=gensim.models.KeyedVectors.load_word2vec_format(hdj, binary=True)\n",
    "    # w2v = gensim.models.KeyedVectors.load_word2vec_format(hdj, binary=True)#模型的导入\n",
    "    vocab_emd=w2v.wv.vocab\n",
    "    print('max_token : ', (w2v.wv.syn0.shape[0]))\n",
    "    # hdj_vocab = w2v.wv.vocab\n",
    "    # max_token = w2v.wv.syn0.shape[0]  # 字典的单词总量\n",
    "    # print('max_token :',max_token)\n",
    "    # goo_model = gensim.models.KeyedVectors.load_word2vec_format(google, binary=True)#模型的导入\n",
    "    for word in vocab:\n",
    "        if word in vocab_emd:\n",
    "            word_vecs[word] = w2v[word]\n",
    "        # else:\n",
    "            # print(word,' not in 词向量')\n",
    "    print('load_bin_vec :',len(word_vecs))\n",
    "    return word_vecs\n",
    "\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=2, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.\n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    # for word in vocab:\n",
    "    #     if word not in word_vecs and vocab[word] >= min_df:\n",
    "    #         word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "    i = 0  # 统计多少不在预训练词表里\n",
    "    print(type(word_vecs),type(vocab))\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            # print('不在词表里词示例 ', word)\n",
    "            i += 1\n",
    "            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "    print('一共 ', i, ' 个词不在词表里')\n",
    "def get_W(word_vecs, k=300):\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size + 1, k), dtype='float32')\n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word,emb in word_vecs.items():\n",
    "        # print(type(word),word,emb)\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    print('get W ',len(W),len(word_idx_map))\n",
    "    return W, word_idx_map\n",
    "\n",
    "def getIdxfrom_sent(sent, word_idx_map, code_maxk):\n",
    "    x = []\n",
    "    #    pad = filter_h - 1\n",
    "    #    for i in xrange(pad):\n",
    "    #        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    if len(x) <= code_maxk:\n",
    "        while len(x) < code_maxk:\n",
    "            x.append(0)\n",
    "    if len(x) >= code_maxk:\n",
    "        while len(x) > code_maxk:\n",
    "            x.pop()\n",
    "    return x\n",
    "\n",
    "\n",
    "def getIdxfrom_sent_n(sent, max_l, word_idx_map, filter_h=5):\n",
    "    #TODO 为什么先pad 0 后再pad 0 ?\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in range(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l + 2 * pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_str_sst(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "# def get_W(word_vecs, k=300):\n",
    "#     vocab_size = len(word_vecs)\n",
    "#     word_idx_map = dict()\n",
    "#     W = np.zeros(shape=(vocab_size + 1, k), dtype='float32')\n",
    "#     W[0] = np.zeros(k, dtype='float32')\n",
    "#     i = 1\n",
    "#     for word in word_vecs:\n",
    "#         W[i] = word_vecs[word]\n",
    "#         word_idx_map[word] = i\n",
    "#         i += 1\n",
    "#     return W, word_idx_map\n",
    "\n",
    "\n",
    "def alignData(data, code_maxl):\n",
    "    mm = data.shape[0]\n",
    "    nn = data.shape[1]\n",
    "    if mm < code_maxl:\n",
    "        tt = code_maxl - mm\n",
    "        aa = np.zeros(nn * tt, dtype=\"int\").reshape(tt, nn)\n",
    "        new_data = np.vstack((data, aa))\n",
    "    else:\n",
    "        new_data = data[:code_maxl, :]\n",
    "    return new_data\n",
    "\n",
    "def random_permutation(train_report, train_code, labels):\n",
    "    labels = np.asarray(labels)\n",
    "    labels = labels.reshape(len(labels))\n",
    "    datasets_size = len(labels)\n",
    "    labels_rand = np.zeros(datasets_size, dtype=\"int\")\n",
    "    zz = np.random.permutation(np.arange(datasets_size))\n",
    "    train_report_rand, train_code_rand = [], []\n",
    "    for i in range(datasets_size):\n",
    "        labels_rand[i] = labels[zz[i]]\n",
    "        train_report_rand.append(train_report[zz[i]])\n",
    "        train_code_rand.append(train_code[zz[i]])\n",
    "    return train_report_rand, train_code_rand, labels_rand\n",
    "\n",
    "def getReport(bug_id,report_data, maxlen,tag):\n",
    "    # print(type(bug_id),type(report_data['bug_id'][0]))\n",
    "    summary=None\n",
    "    description=None\n",
    "    # print('hhh: ',bug_id,tag)\n",
    "    try:\n",
    "        description=eval(report_data[report_data['bug_id'] == int(bug_id)]['pos_tagged_descriptions'].values[0])['stemmed']#['unstemmed']\n",
    "        # print(description)\n",
    "        summary=eval(report_data[report_data['bug_id'] == int(bug_id)]['pos_tagged_summarys'].values[0])['stemmed']#['unstemmed']\n",
    "    except Exception as e:\n",
    "        print('except : ',bug_id)\n",
    "    # print(summary)\n",
    "    report=None\n",
    "    if description!=None and summary!=None:\n",
    "        summary.extend(description)\n",
    "        if len(summary) > maxlen:\n",
    "            cut_words = []\n",
    "            for i in range(maxlen):\n",
    "                cut_words.append(summary[i])\n",
    "            report = \" \".join(cut_words)\n",
    "        else:\n",
    "            report = \" \".join(summary)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成report开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportPreprocessing:\n",
    "    \"\"\"Class to preprocess bug reports\"\"\"\n",
    "\n",
    "    __slots__ = ['bug_reports']\n",
    "\n",
    "    def __init__(self, bug_reports):\n",
    "        self.bug_reports = bug_reports\n",
    "\n",
    "\n",
    "    def pos_tagging(self):\n",
    "        \"\"\"Extracing specific pos tags from bug reports' summary and description\"\"\"\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            # Tokenizing using word_tokeize for more accurate pos-tagging\n",
    "            summ_tok = nltk.word_tokenize(report.summary)\n",
    "            try:\n",
    "                desc_tok = nltk.word_tokenize(report.description)\n",
    "            except Exception as e:\n",
    "                print('出问题了 ',report.description)\n",
    "            sum_pos = nltk.pos_tag(summ_tok)\n",
    "            desc_pos = nltk.pos_tag(desc_tok)\n",
    "\n",
    "            report.pos_tagged_summary = [token for token, pos in sum_pos\n",
    "                                         if 'NN' in pos or 'VB' in pos]\n",
    "            report.pos_tagged_description = [token for token, pos in desc_pos\n",
    "                                             if 'NN' in pos or 'VB' in pos]\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"Tokenizing bug reports into tokens\"\"\"\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            report.summary = nltk.wordpunct_tokenize(report.summary)\n",
    "            report.description = nltk.wordpunct_tokenize(report.description)\n",
    "\n",
    "    def _split_camelcase(self, tokens):\n",
    "\n",
    "        # Copy tokens\n",
    "        returning_tokens = tokens[:]\n",
    "\n",
    "        for token in tokens:\n",
    "            split_tokens = re.split(fr'[{string.punctuation}]+', token)\n",
    "\n",
    "            # If token is split into some other tokens\n",
    "            if len(split_tokens) > 1:\n",
    "                returning_tokens.remove(token)\n",
    "                # Camel case detection for new tokens\n",
    "                for st in split_tokens:\n",
    "                    camel_split = inflection.underscore(st).split('_')\n",
    "                    if len(camel_split) > 1:\n",
    "                        returning_tokens.append(st)\n",
    "                        returning_tokens += camel_split\n",
    "                    else:\n",
    "                        returning_tokens.append(st)\n",
    "            else:\n",
    "                camel_split = inflection.underscore(token).split('_')\n",
    "                if len(camel_split) > 1:\n",
    "                    returning_tokens += camel_split\n",
    "\n",
    "        return returning_tokens\n",
    "\n",
    "    def split_camelcase(self):\n",
    "        \"\"\"Split CamelCase identifiers\"\"\"\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            report.summary = self._split_camelcase(report.summary)\n",
    "            report.description = self._split_camelcase(report.description)\n",
    "            report.pos_tagged_summary = self._split_camelcase(report.pos_tagged_summary)\n",
    "            report.pos_tagged_description = self._split_camelcase(report.pos_tagged_description)\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Removing punctuation, numbers and also lowercase conversion\"\"\"\n",
    "\n",
    "        # Building a translate table for punctuation and number removal\n",
    "        punctnum_table = str.maketrans({c: None for c in string.punctuation + string.digits})\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            summary_punctnum_rem = [token.translate(punctnum_table)\n",
    "                                    for token in report.summary]\n",
    "            desc_punctnum_rem = [token.translate(punctnum_table)\n",
    "                                 for token in report.description]\n",
    "            pos_sum_punctnum_rem = [token.translate(punctnum_table)\n",
    "                                    for token in report.pos_tagged_summary]\n",
    "            pos_desc_punctnum_rem = [token.translate(punctnum_table)\n",
    "                                     for token in report.pos_tagged_description]\n",
    "\n",
    "            report.summary = [token.lower() for token\n",
    "                              in summary_punctnum_rem if token]\n",
    "            report.description = [token.lower() for token\n",
    "                                  in desc_punctnum_rem if token]\n",
    "            report.pos_tagged_summary = [token.lower() for token\n",
    "                                         in pos_sum_punctnum_rem if token]\n",
    "            report.pos_tagged_description = [token.lower() for token\n",
    "                                             in pos_desc_punctnum_rem if token]\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        \"\"\"Removing stop words from tokens\"\"\"\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            report.summary = [token for token in report.summary\n",
    "                              if token not in stop_words]\n",
    "            report.description = [token for token in report.description\n",
    "                                  if token not in stop_words]\n",
    "            report.pos_tagged_summary = [token for token in report.pos_tagged_summary\n",
    "                                         if token not in stop_words]\n",
    "            report.pos_tagged_description = [token for token in report.pos_tagged_description\n",
    "                                             if token not in stop_words]\n",
    "\n",
    "    def remove_java_keywords(self):\n",
    "        \"\"\"Removing Java language keywords from tokens\"\"\"\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            report.summary = [token for token in report.summary\n",
    "                              if token not in java_keywords]\n",
    "            report.description = [token for token in report.description\n",
    "                                  if token not in java_keywords]\n",
    "            report.pos_tagged_summary = [token for token in report.pos_tagged_summary\n",
    "                                         if token not in java_keywords]\n",
    "            report.pos_tagged_description = [token for token in report.pos_tagged_description\n",
    "                                             if token not in java_keywords]\n",
    "\n",
    "    def stem(self):\n",
    "        \"\"\"Stemming tokens\"\"\"\n",
    "\n",
    "        # Stemmer instance\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        for report in self.bug_reports.values():\n",
    "            report.summary = dict(zip(['stemmed', 'unstemmed'],\n",
    "                                      [[stemmer.stem(token) for token in report.summary],\n",
    "                                       report.summary]))\n",
    "\n",
    "            report.description = dict(zip(['stemmed', 'unstemmed'],\n",
    "                                          [[stemmer.stem(token) for token in report.description],\n",
    "                                           report.description]))\n",
    "\n",
    "            report.pos_tagged_summary = dict(zip(['stemmed', 'unstemmed'],\n",
    "                                                 [[stemmer.stem(token) for token\n",
    "                                                   in report.pos_tagged_summary],\n",
    "                                                  report.pos_tagged_summary]))\n",
    "\n",
    "            report.pos_tagged_description = dict(zip(['stemmed', 'unstemmed'],\n",
    "                                                     [[stemmer.stem(token) for token\n",
    "                                                       in report.pos_tagged_description],\n",
    "                                                      report.pos_tagged_description]))\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Run the preprocessing\"\"\"\n",
    "\n",
    "        self.pos_tagging()\n",
    "        self.tokenize()\n",
    "        self.split_camelcase()\n",
    "        self.normalize()\n",
    "        self.remove_stopwords()\n",
    "        self.remove_java_keywords()\n",
    "        self.stem()\n",
    "class BugReport:\n",
    "    \"\"\"Class representing each bug report\"\"\" \n",
    "    \n",
    "    __slots__ = ['summary', 'description',\n",
    "                 'pos_tagged_summary', 'pos_tagged_description']\n",
    "    \n",
    "    def __init__(self, summary, description):\n",
    "        self.summary = summary\n",
    "        self.description = description\n",
    "        self.pos_tagged_summary = None\n",
    "        self.pos_tagged_description = None\n",
    "def load_bug_reports(bug_report_file_path):\n",
    "    \"\"\"load bug report file (the one generated from xml)\"\"\"\n",
    "    with open(bug_report_file_path) as bug_report_file:\n",
    "        bug_reports = json.load(bug_report_file)\n",
    "        return bug_reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug_report_file_path='/data/hdj/tracking_buggy_files/'+swt+'.json'\n",
    "# bug_report_file_path='/data/hdj/tracking_buggy_files/eclipse/eclipse_platform_ui.json'\n",
    "project='swt'\n",
    "bug_report_file_path='/data/hdj/tracking_buggy_files/'+project+'/'+project+'.json'\n",
    "bug_reports = load_bug_reports(bug_report_file_path)\n",
    "bug_reports_new = dict()\n",
    "\n",
    "for i,(key,val)  in enumerate(bug_reports.items()):\n",
    "    bug_reports_new[key] = BugReport(\n",
    "        ' '.join(val['bug_report']['summary'].split()[2:]),\n",
    "        '' if  val['bug_report']['description']==None else val['bug_report']['description']\n",
    "    )\n",
    "report_prep = ReportPreprocessing(bug_reports_new)\n",
    "report_prep.preprocess()\n",
    "summarys=[]\n",
    "descriptions=[]\n",
    "pos_tagged_summarys=[]\n",
    "pos_tagged_descriptions=[]\n",
    "bug_ids=[]\n",
    "for key,report in zip(report_prep.bug_reports.keys(),report_prep.bug_reports.values()):\n",
    "    summarys.append(report.summary)\n",
    "    descriptions.append(report.description)\n",
    "    pos_tagged_summarys.append(report.pos_tagged_summary)\n",
    "    pos_tagged_descriptions.append(report.pos_tagged_description)\n",
    "    bug_ids.append(key)\n",
    "report=pd.DataFrame()\n",
    "report['bug_id']=bug_ids\n",
    "report['pos_tagged_descriptions']=pos_tagged_descriptions\n",
    "report['pos_tagged_summarys']=pos_tagged_summarys\n",
    "report['descriptions']=descriptions\n",
    "report['summarys']=summarys\n",
    "report.to_csv('/data/hdj/cross_project_trans/report/bert/'+project+'_report_clean.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_id</th>\n",
       "      <th>pos_tagged_descriptions</th>\n",
       "      <th>pos_tagged_summarys</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>summarys</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bae8b23</td>\n",
       "      <td>{'stemmed': ['red', 'hat', 'eclips', 'open', '...</td>\n",
       "      <td>{'stemmed': ['dcr', 'need', 'display', 'getsys...</td>\n",
       "      <td>{'stemmed': ['red', 'hat', 'eclips', 'open', '...</td>\n",
       "      <td>{'stemmed': ['dcr', 'need', 'display', 'getsys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32055ba</td>\n",
       "      <td>{'stemmed': ['computes', 'method', 'appear', '...</td>\n",
       "      <td>{'stemmed': ['label', 'requir', 'draw'], 'unst...</td>\n",
       "      <td>{'stemmed': ['computes', 'method', 'appear', '...</td>\n",
       "      <td>{'stemmed': ['label', 'requir', 'border', 'dra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9d45068</td>\n",
       "      <td>{'stemmed': ['run', 'eclips', 'grab', 'sash', ...</td>\n",
       "      <td>{'stemmed': ['sash', 'cursor', 'chang', 'drag'...</td>\n",
       "      <td>{'stemmed': ['run', 'eclips', 'grab', 'sash', ...</td>\n",
       "      <td>{'stemmed': ['sash', 'cursor', 'chang', 'drag'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ee8264</td>\n",
       "      <td>{'stemmed': ['build', 'identifi', 'notic', 'wo...</td>\n",
       "      <td>{'stemmed': ['window', 'render', 'restor', 'mi...</td>\n",
       "      <td>{'stemmed': ['build', 'identifi', 'notic', 'wo...</td>\n",
       "      <td>{'stemmed': ['workbench', 'window', 'render', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f35602d</td>\n",
       "      <td>{'stemmed': ['isnt', 'copi', 'past', 'text', '...</td>\n",
       "      <td>{'stemmed': ['browser', 'copi', 'past', 'text'...</td>\n",
       "      <td>{'stemmed': ['isnt', 'possibl', 'copi', 'past'...</td>\n",
       "      <td>{'stemmed': ['browser', 'unabl', 'copi', 'past...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bug_id                            pos_tagged_descriptions  \\\n",
       "0  bae8b23  {'stemmed': ['red', 'hat', 'eclips', 'open', '...   \n",
       "1  32055ba  {'stemmed': ['computes', 'method', 'appear', '...   \n",
       "2  9d45068  {'stemmed': ['run', 'eclips', 'grab', 'sash', ...   \n",
       "3  1ee8264  {'stemmed': ['build', 'identifi', 'notic', 'wo...   \n",
       "4  f35602d  {'stemmed': ['isnt', 'copi', 'past', 'text', '...   \n",
       "\n",
       "                                 pos_tagged_summarys  \\\n",
       "0  {'stemmed': ['dcr', 'need', 'display', 'getsys...   \n",
       "1  {'stemmed': ['label', 'requir', 'draw'], 'unst...   \n",
       "2  {'stemmed': ['sash', 'cursor', 'chang', 'drag'...   \n",
       "3  {'stemmed': ['window', 'render', 'restor', 'mi...   \n",
       "4  {'stemmed': ['browser', 'copi', 'past', 'text'...   \n",
       "\n",
       "                                        descriptions  \\\n",
       "0  {'stemmed': ['red', 'hat', 'eclips', 'open', '...   \n",
       "1  {'stemmed': ['computes', 'method', 'appear', '...   \n",
       "2  {'stemmed': ['run', 'eclips', 'grab', 'sash', ...   \n",
       "3  {'stemmed': ['build', 'identifi', 'notic', 'wo...   \n",
       "4  {'stemmed': ['isnt', 'possibl', 'copi', 'past'...   \n",
       "\n",
       "                                            summarys  \n",
       "0  {'stemmed': ['dcr', 'need', 'display', 'getsys...  \n",
       "1  {'stemmed': ['label', 'requir', 'border', 'dra...  \n",
       "2  {'stemmed': ['sash', 'cursor', 'chang', 'drag'...  \n",
       "3  {'stemmed': ['workbench', 'window', 'render', ...  \n",
       "4  {'stemmed': ['browser', 'unabl', 'copi', 'past...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#验证一下正确性\n",
    "report_data=pd.read_csv('/data/hdj/cross_project_trans/report/bert/'+project+'_report_clean.csv')\n",
    "report_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成report信息结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成code字典开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string_code(string):\n",
    "    m = re.compile(r'/\\*.*?\\*/', re.S)\n",
    "    outstring = re.sub(m, '', string)\n",
    "    m = re.compile(r'package*', re.S)\n",
    "    outstring = re.sub(m, '', outstring)\n",
    "    m = re.compile(r'import*', re.S)\n",
    "    outstring = re.sub(m, '', outstring)\n",
    "    m = re.compile(r'//.*')\n",
    "    outtmp = re.sub(m, '', outstring)\n",
    "    \n",
    "    return outtmp\n",
    "fid='7dbde289a9c63fbe8464de0e913442e6889b57bf'\n",
    "ast_raw_file = pickle.loads(ast_cache_collection_db['7dbde289a9c63fbe8464de0e913442e6889b57bf'])\n",
    "rawContent=clean_string_code(ast_raw_file['rawSourceContent'])\n",
    "codeWritePath=os.path.join(codePath,fid+'.java')\n",
    "with open(codeWritePath,'w',encoding='utf-8') as f_out:\n",
    "    f_out.write(rawContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7dbde289a9c63fbe8464de0e913442e6889b57bf'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fidList.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7186\n",
      "17324\n"
     ]
    }
   ],
   "source": [
    "ast_cache_collection_db=UnQLite(\"/data/hdj/tracking_buggy_files/\"+project+'/'+project+\"_ast_cache_collection_db\",flags=0x00000100 | 0x00000001)\n",
    "outPath='/data/hdj/cross_project_trans/report/'\n",
    "train_pos=pd.read_csv('/data/hdj/cross_project_trans/report/bert/'+project+'_80_training_pos.csv')\n",
    "train_neg=pd.read_csv('/data/hdj/cross_project_trans/report/bert/'+project+'_80_training_neg.csv')\n",
    "fidList=set()\n",
    "fidList=fidList.union(train_pos['fid'].values)\n",
    "fidList=fidList.union(train_neg['fid'].values)\n",
    "print(len(fidList))\n",
    "testNum=len(os.listdir('/data/hdj/cross_project_trans/report/swt_test/'))\n",
    "for i in range(testNum):\n",
    "    filePath=os.path.join(outPath,project+'_test','test'+str(i)+'.csv')\n",
    "#     print(filePath)\n",
    "    data=pd.read_csv(filePath)\n",
    "    fidList=fidList.union(data['fid'].values)\n",
    "print(len(fidList))    \n",
    "# codeDict=dict()\n",
    "codePath='/data/hdj/cross_project_trans/report/bert/'+project+'_code'\n",
    "for fid in fidList:\n",
    "#     print(fid)\n",
    "    ast_raw_file = pickle.loads(ast_cache_collection_db[fid])\n",
    "#     codeDict[fid]=ast_raw_file['rawSourceContent']\n",
    "    rawContent=clean_string_code(ast_raw_file['rawSourceContent'])\n",
    "    codeWritePath=os.path.join(codePath,fid)\n",
    "    with open(codeWritePath,'w',encoding='utf-8') as f_out:\n",
    "        f_out.write(rawContent)\n",
    "#     break\n",
    "\n",
    "# for fid,rawContent in codeDict.items():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, TREC=False):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?;\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip() if TREC else string.strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lresult wmscrollchild \\\\( int long wparam , int long lparam \\\\)'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_str('LRESULT wmScrollChild (int /*long*/ wParam, int /*long*/ lParam) {')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成code字典结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data( code_maxl, code_maxk, report_maxl, w2v_file, report_path,project_name,more_project_name,less_project_name,clean_path,hdjWord2vec):\n",
    "    '''\n",
    "        code_maxl:代码的最大行数 100\n",
    "        code_maxk:代码每行最大单词数 40 \n",
    "        report_maxl:report最大的单词数 308\n",
    "        w2c_file: 词向量所在位置/data/hdj/cross_project_trans/embedding/word_w2v_100\n",
    "        report_path:/data/hdj/cross_project_trans/report/bert 文件保存所在位置\n",
    "        project_name:将要处理的项目aspectj_\n",
    "        more_project_name:aspectj_80_\n",
    "        less_project_name:aspectj_20_\n",
    "        clean_path:清洗过后的项目路径/data/hdj/cross_project_trans/report/bert/swt_code\n",
    "        \n",
    "    '''\n",
    "    less_report_ids=[]\n",
    "    #revs:保存report的dict信息\n",
    "    '''\n",
    "    #revs_code:保存一个code文件的所有行 [\n",
    "                            [{\"y\": 1,#这是一行的数据\n",
    "                             \"text\": orig_rev,\n",
    "                             \"num_words\": len(orig_rev.split()),\n",
    "                             \"test\": -1,\n",
    "                             \"target\": -1,\n",
    "                             \"path\": source_code_name,\n",
    "                             'bug_id': bug_report_id},{},{}这是很多行\n",
    "                         ]\n",
    "                        ]\n",
    "    '''\n",
    "    \n",
    "    report_ids,revs, revs_code = [], [],[]\n",
    "    report, code, train_path = [], [], []\n",
    "    revs_pre = []\n",
    "    vocab = defaultdict(float)\n",
    "    #拼接成的路径 /data/hdj/cross_project_trans/report/aspectj_report_clean.csv\n",
    "    #/data/hdj/cross_project_trans/report/bert/swt_report_clean.csv\n",
    "    report_data=pd.read_csv(report_path+project_name+'report_clean.csv')\n",
    "    print('读取80% training_pos的数据')#读取80% training_pos的数据/data/hdj/cross_project_trans/report/aspectj_80_training_pos.csv\n",
    "    spamreader = csv.reader(open(report_path+more_project_name+\"training_pos.csv\", newline=''))\n",
    "    i=0\n",
    "    for row in spamreader:\n",
    "        if i==0:\n",
    "            i+=1\n",
    "            continue\n",
    "        #根据bugid获得对应report\n",
    "        bug_report_id = row[0].strip()\n",
    "        report_ids.append(bug_report_id)#TODO 记录report_id\n",
    "        bug_description = getReport(bug_report_id,report_data, report_maxl,'ll')\n",
    "        # report信息获得完成\n",
    "        # 统计词表信息\n",
    "        orig_rev = bug_description.strip().lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        #统计完成\n",
    "        #将report相关信息保存\n",
    "        datum = {\"y\": 1,#是正例\n",
    "                 \"text\": orig_rev,#report内容\n",
    "                 \"num_words\": len(orig_rev.split()),#report长度\n",
    "                 \"test\": -1,#不是测试数据\n",
    "                 \"target\": -1,\n",
    "                 'bug_id': bug_report_id}\n",
    "        revs.append(datum)#TODO 记录report\n",
    "        #保存完成\n",
    "\n",
    "        source_code_name = row[1].strip()\n",
    "        #clean_path+source_code_name='/home/hdj/bug_localization/data/ZXing/clean/' + 'com/google/zxing/client/rim/util/Log.java'\n",
    "        #/data/hdj/cross_project_trans/report/bert/jdt_code\n",
    "        with open(os.path.join(clean_path,source_code_name), \"r\") as f:\n",
    "            revs_single = []#保存一个java文件所有行[[import pandas as pd],[],[]]\n",
    "            for line in f:\n",
    "                orig_rev = clean_str(str(line.strip()))  # TODO 更改代码的正则化\n",
    "                if len(orig_rev) <= 1: continue#这里过滤了数量小于1的行\n",
    "                words = set(orig_rev.split())\n",
    "                for word in words:\n",
    "                    vocab[word] += 1\n",
    "                datum = {\"y\": 1,\n",
    "                         \"text\": orig_rev,\n",
    "                         \"num_words\": len(orig_rev.split()),\n",
    "                         \"test\": -1,\n",
    "                         \"target\": -1,\n",
    "                         \"path\": source_code_name,\n",
    "                         'bug_id': bug_report_id}\n",
    "                revs_single.append(datum)\n",
    "\n",
    "            revs_code.append(revs_single)#保存所有的java文件内容 [[revs_single],[]] #TODO 记录该report对应的code\n",
    "    # 读取80% training_pos的数据完成\n",
    "    print('读取80% training_pos的数据完成')\n",
    "\n",
    "#     print('读取20% training_pos的数据')\n",
    "#     # 读取20% training_pos的数据\n",
    "#     spamreader = csv.reader(open(report_path + less_project_name + \"training_pos.csv\", newline=''))\n",
    "#     i = 0\n",
    "#     for row in spamreader:\n",
    "#         if i == 0:\n",
    "#             i += 1\n",
    "#             continue\n",
    "#         # 根据bugid获得对应report\n",
    "#         bug_report_id = row[0].strip()\n",
    "#         less_report_ids.append(bug_report_id)  # TODO 记录report_id\n",
    "#         bug_description = getReport(bug_report_id, report_data, report_maxl, 'll')\n",
    "#         # report信息获得完成\n",
    "#         # 统计词表信息\n",
    "#         orig_rev = bug_description.strip().lower()\n",
    "#         words = set(orig_rev.split())\n",
    "#         for word in words:\n",
    "#             vocab[word] += 1\n",
    "#         # 统计完成\n",
    "#         # 将report相关信息保存\n",
    "#         datum = {\"y\": 1,  # 是正例\n",
    "#                  \"text\": orig_rev,  # report内容\n",
    "#                  \"num_words\": len(orig_rev.split()),  # report长度\n",
    "#                  \"test\": -2,  # -1 代表80%的数据   -2 代表20%的数据\n",
    "#                  \"target\": -1,\n",
    "#                  'bug_id': bug_report_id}\n",
    "#         revs.append(datum)  # TODO 记录report\n",
    "#         # 保存完成\n",
    "\n",
    "#         source_code_name = row[1].strip()\n",
    "#         # clean_path+source_code_name='/home/hdj/bug_localization/data/ZXing/clean/' + 'com/google/zxing/client/rim/util/Log.java'\n",
    "#         with open(os.path.join(clean_path, source_code_name), \"r\") as f:\n",
    "#             revs_single = []  # 保存一个java文件所有行[[import pandas as pd],[],[]]\n",
    "#             for line in f:\n",
    "#                 orig_rev = clean_str(str(line.strip()))  # TODO 更改代码的正则化\n",
    "#                 if len(orig_rev) <= 1: continue\n",
    "#                 words = set(orig_rev.split())\n",
    "#                 for word in words:\n",
    "#                     vocab[word] += 1\n",
    "#                 datum = {\"y\": 1,\n",
    "#                          \"text\": orig_rev,\n",
    "#                          \"num_words\": len(orig_rev.split()),\n",
    "#                          \"test\": -2,\n",
    "#                          \"target\": -1,\n",
    "#                          \"path\": source_code_name,\n",
    "#                          'bug_id': bug_report_id}\n",
    "#                 revs_single.append(datum)\n",
    "\n",
    "#             revs_code.append(revs_single)  # 保存所有的java文件内容 [[revs_single],[]] #TODO 记录该report对应的code\n",
    "#     # 读取20% training_pos的数据完成\n",
    "#     print('读取20% training_pos的数据完成')\n",
    "\n",
    "    print('读取80% training negative 的数据')\n",
    "    #读取80% training negative 的数据\n",
    "    spamreader = csv.reader(open(report_path+more_project_name+\"training_neg.csv\", newline=''))\n",
    "    i=0\n",
    "    for row in spamreader:\n",
    "        if i==0:\n",
    "            i+=1\n",
    "            continue\n",
    "       # bug_report_name = \"JDT_CNN/BugCorpus/BugCorpus/\" + row[0].strip()\n",
    "        bug_report_id = row[0].strip()\n",
    "        report_ids.append(bug_report_id)#TODO 记录report_id\n",
    "\n",
    "        #统计词频\n",
    "        bug_description = getReport(bug_report_id, report_data, report_maxl,'aa')\n",
    "        # rev = []\n",
    "        # rev.append(bug_description.strip())\n",
    "        # orig_rev = \" \".join(rev).lower()\n",
    "        orig_rev = bug_description.strip().lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        #统计完成\n",
    "\n",
    "\n",
    "        datum = {\"y\": 0,\n",
    "                 \"text\": orig_rev,\n",
    "                 \"num_words\": len(orig_rev.split()),\n",
    "                 \"test\": -1,\n",
    "                 \"target\": -1,\n",
    "                 \"bug_id\":bug_report_id}\n",
    "        revs.append(datum)#TODO 记录report\n",
    "\n",
    "        source_code_name = row[1].strip()\n",
    "        with open(os.path.join(clean_path,source_code_name), \"r\") as f:\n",
    "            revs_single = []\n",
    "            for line in f:\n",
    "                orig_rev = clean_str(str(line.strip()))  # TODO 更改代码的正则化\n",
    "                if len(orig_rev) <= 1: continue\n",
    "                words = set(orig_rev.split())\n",
    "                for word in words:\n",
    "                    vocab[word] += 1\n",
    "                datum = {\"y\": 0,\n",
    "                         \"text\": orig_rev,\n",
    "                         \"num_words\": len(orig_rev.split()),\n",
    "                         \"test\": -1,\n",
    "                         \"target\": -1,\n",
    "                        \"path\":source_code_name,\n",
    "                         \"bug_id\": bug_report_id}\n",
    "                revs_single.append(datum)\n",
    "            if len(revs_single)==0:#TODO 注意 过滤后的java文件不应该为空，否则后续处理有问题\n",
    "                print('this is the bug')\n",
    "            revs_code.append(revs_single)#TODO 记录该report对应的code\n",
    "    # 读取80% training negative 的数据完成\n",
    "    print('读取80% training negative 的数据完成')\n",
    "\n",
    "#     print('读取20% training negative 的数据')\n",
    "#     # 读取20% training negative 的数据\n",
    "#     spamreader = csv.reader(open(report_path + less_project_name + \"training_neg.csv\", newline=''))\n",
    "#     i = 0\n",
    "#     for row in spamreader:\n",
    "#         if i == 0:\n",
    "#             i += 1\n",
    "#             continue\n",
    "#             # bug_report_name = \"JDT_CNN/BugCorpus/BugCorpus/\" + row[0].strip()\n",
    "#         bug_report_id = row[0].strip()\n",
    "#         less_report_ids.append(bug_report_id)  # TODO 记录report_id\n",
    "#         # 统计词频\n",
    "#         bug_description = getReport(bug_report_id, report_data, report_maxl, 'aa')\n",
    "#         # rev = []\n",
    "#         # rev.append(bug_description.strip())\n",
    "#         # orig_rev = \" \".join(rev).lower()\n",
    "#         orig_rev = bug_description.strip().lower()\n",
    "#         words = set(orig_rev.split())\n",
    "#         for word in words:\n",
    "#             vocab[word] += 1\n",
    "#         # 统计完成\n",
    "#         datum = {\"y\": 0,\n",
    "#                  \"text\": orig_rev,\n",
    "#                  \"num_words\": len(orig_rev.split()),\n",
    "#                  \"test\": -2,\n",
    "#                  \"target\": -1,\n",
    "#                  \"bug_id\": bug_report_id}\n",
    "#         revs.append(datum)  # TODO 记录report\n",
    "#         source_code_name = row[1].strip()\n",
    "#         with open(os.path.join(clean_path, source_code_name), \"r\") as f:\n",
    "#             revs_single = []\n",
    "#             for line in f:\n",
    "#                 orig_rev = clean_str(str(line.strip()))  # TODO 更改代码的正则化\n",
    "#                 if len(orig_rev) <= 1: continue\n",
    "#                 words = set(orig_rev.split())\n",
    "#                 for word in words:\n",
    "#                     vocab[word] += 1\n",
    "#                 datum = {\"y\": 0,\n",
    "#                          \"text\": orig_rev,\n",
    "#                          \"num_words\": len(orig_rev.split()),\n",
    "#                          \"test\": -2,\n",
    "#                          \"target\": -1,\n",
    "#                          \"path\": source_code_name,\n",
    "#                          \"bug_id\": bug_report_id}\n",
    "#                 revs_single.append(datum)\n",
    "#             if len(revs_single) == 0:  # TODO 注意 过滤后的java文件不应该为空，否则后续处理有问题\n",
    "#                 print('this is the bug')\n",
    "#             revs_code.append(revs_single)  # TODO 记录该report对应的code\n",
    "#     # 读取20% training negative 的数据完成\n",
    "#     print('读取20% training negative 的数据完成')\n",
    "    # 读取test数据  test_num ：number of test bug reports #TODO 数量不固定，提前传个参数\n",
    "    #test_path=report_path+project_name+'test/test'+str(index)+'.csv'\n",
    "    #  例如    '/home/hdj/cross_project_trans/report/'+'aspectj_'+'test/test'+'0'+'.csv'\n",
    "    test_path=report_path+project_name+'test/'\n",
    "    test_num=len(os.listdir(test_path))\n",
    "    print('读取test数据',test_num)\n",
    "    for test_index in range(test_num):\n",
    "        spamreader = csv.reader(open(report_path+project_name+'test/test' + str(test_index) + \".csv\", newline=''))\n",
    "        i=0\n",
    "        for row in spamreader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            label = row[2]\n",
    "            # bug_report_name = \"JDT_CNN/BugCorpus/BugCorpus/\" + row[0].strip()\n",
    "            bug_report_id = row[0].strip()\n",
    "            # report_ids.append(bug_report_id)#TODO 记录test 的report_id\n",
    "\n",
    "            bug_description = getReport(bug_report_id, report_data, report_maxl,'dd')\n",
    "            orig_rev = bug_description.strip().lower()\n",
    "            words = set(orig_rev.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "            if label.strip() == \"1\":\n",
    "                datum = {\"y\": 1,\n",
    "                         \"text\": orig_rev,\n",
    "                         \"num_words\": len(orig_rev.split()),\n",
    "                         \"test\": test_index,#用来记录test索引 每个测试report都单独处理\n",
    "                         \"target\": 1,\n",
    "                         \"bug_id\": bug_report_id}\n",
    "            else:\n",
    "                datum = {\"y\": 0,\n",
    "                         \"text\": orig_rev,\n",
    "                         \"num_words\": len(orig_rev.split()),\n",
    "                         \"test\": test_index,\n",
    "                         \"target\": 1,\n",
    "                         \"bug_id\": bug_report_id}\n",
    "            #revs 存放report信息\n",
    "            revs.append(datum)#TODO 记录test report\n",
    "\n",
    "          #  source_file_name = \"JDT_CNN/sources/sources/JDT_4_5/\" + row[1].strip()\n",
    "            source_code_name = row[1].strip()\n",
    "            # print(os.path.join(pre, source_file_name))\n",
    "            with open(os.path.join(clean_path,source_code_name), \"r\") as f:\n",
    "                revs_single = []\n",
    "                for line in f:\n",
    "                    orig_rev = clean_str(str(line.strip()))  # TODO 更改代码的正则化\n",
    "                    if len(orig_rev) <= 1:\n",
    "                        orig_rev = \"test\"\n",
    "                    words = set(orig_rev.split())\n",
    "                    for word in words:\n",
    "                        vocab[word] += 1\n",
    "                    if label.strip() == \"1\":\n",
    "                        datum = {\"y\": 1,\n",
    "                                 \"text\": orig_rev,\n",
    "                                 \"num_words\": len(orig_rev.split()),\n",
    "                                 \"test\": test_index,\n",
    "                                 \"target\": 1,\n",
    "                                 \"path\":source_code_name,\n",
    "                                 \"bug_id\": bug_report_id}\n",
    "                    else:\n",
    "                        datum = {\"y\": 0,\n",
    "                                 \"text\": orig_rev,\n",
    "                                 \"num_words\": len(orig_rev.split()),\n",
    "                                 \"test\": test_index,\n",
    "                                 \"target\": 1,\n",
    "                                 \"path\": source_code_name,\n",
    "                                 \"bug_id\": bug_report_id}#TODO 根据这个区匹配对应的AST\n",
    "                    revs_single.append(datum)\n",
    "                if len(revs_single) == 0:\n",
    "                    revs_single = revs_pre\n",
    "                if len(revs_single)==0:\n",
    "                    print(os.path.join(clean_path, source_code_name))\n",
    "                    print('here null :',revs_single,' llllll')\n",
    "                revs_code.append(revs_single) #TODO 记录该report对应的code\n",
    "                revs_pre = revs_single\n",
    "    print('读取test数据 ', test_num,' 完成')\n",
    "    w2v = load_bin_vec('',w2v_file, vocab,hdjWord2vec)\n",
    "    if hdjWord2vec:\n",
    "        add_unknown_words(w2v, vocab,k=100)\n",
    "        W, word_idx_map = get_W(w2v,k=100)\n",
    "    else:\n",
    "        add_unknown_words(w2v, vocab)\n",
    "        W, word_idx_map = get_W(w2v)\n",
    "    print('这里显示向量真实大小')\n",
    "    print(len(W),len(W[0]))\n",
    "    #对train80% 数据进行整理\n",
    "    print('对train80% 数据进行整理')\n",
    "    train_id,train_report, train_code, train_labels = [], [], [],[]\n",
    "    #1. 对report信息进行操作\n",
    "    for rev in revs:\n",
    "        \n",
    "        if rev[\"test\"] == -1:\n",
    "            sent = getIdxfrom_sent_n(rev[\"text\"], report_maxl, word_idx_map, filter_h=5)\n",
    "            train_id.append(rev[\"bug_id\"])\n",
    "            train_report.append(sent)\n",
    "            report.append(rev['text'])\n",
    "    #2. 对code信息进行操作\n",
    "    for revs_single in revs_code:\n",
    "        data_single = []\n",
    "        label_single = []\n",
    "        if len(revs_single)==0:\n",
    "            print('null ',revs_single,' kl')\n",
    "        for rev in revs_single:\n",
    "                sent = getIdxfrom_sent(rev[\"text\"], word_idx_map, code_maxk=code_maxk)\n",
    "                data_single.append(sent)\n",
    "        if rev[\"test\"] == -1:\n",
    "            \n",
    "            data_single = np.array(data_single, dtype=\"int\")\n",
    "            if len(data_single)==0:\n",
    "                print(type(data_single),data_single,revs_single)\n",
    "            data_single = alignData(data_single, code_maxl)\n",
    "            train_code.append(data_single)\n",
    "            label_single.append(rev[\"y\"])\n",
    "            train_labels.append(label_single)\n",
    "            code.append(revs_single)\n",
    "            train_path.append(rev['path'])\n",
    "\n",
    "    train_labels = np.array(train_labels, dtype=\"int\")\n",
    "    #train80% 数据整理完成\n",
    "    print('对train80% 数据进行整理完成')\n",
    "\n",
    "    #train 20% 数据整理\n",
    "#     print('train 20% 数据整理')\n",
    "#     less_train_id, less_train_report, less_train_code, less_train_labels = [], [], [], []\n",
    "#     less_report, less_code, less_train_path = [], [], []\n",
    "#     # 1. 对report信息进行操作\n",
    "#     for rev in revs:\n",
    "#         if rev[\"test\"] == -2:#TODO -2 代表20%的数据\n",
    "#             sent = getIdxfrom_sent_n(rev[\"text\"], report_maxl, word_idx_map, filter_h=5)\n",
    "#             less_train_id.append(rev[\"bug_id\"])\n",
    "#             less_train_report.append(sent)\n",
    "#             less_report.append(rev['text'])\n",
    "#     # 2. 对code信息进行操作\n",
    "#     for revs_single in revs_code:\n",
    "#         data_single = []\n",
    "#         label_single = []\n",
    "#         if len(revs_single) == 0:\n",
    "#             print('null ', revs_single, ' kl')\n",
    "#         for rev in revs_single:\n",
    "#                 sent = getIdxfrom_sent(rev[\"text\"], word_idx_map, code_maxk=code_maxk)\n",
    "#                 data_single.append(sent)\n",
    "#         if rev[\"test\"] == -2:\n",
    "#             data_single = np.array(data_single, dtype=\"int\")\n",
    "#             if len(data_single) == 0:\n",
    "#                 print(type(data_single), data_single, revs_single)\n",
    "#             data_single = alignData(data_single, code_maxl)\n",
    "#             less_train_code.append(data_single)\n",
    "#             label_single.append(rev[\"y\"])\n",
    "#             less_train_labels.append(label_single)\n",
    "#             less_code.append(revs_single)\n",
    "#             less_train_path.append(rev['path'])\n",
    "\n",
    "#     less_train_labels = np.array(less_train_labels, dtype=\"int\")\n",
    "#     #train 20% 整理完成\n",
    "#     print('train 20% 数据整理完成')\n",
    "    #对测试集进行整理\n",
    "    print('对测试集进行整理')\n",
    "    test_report, test_code, test_path, test_bug_id, test_raw_code, test_row_report = [], [], [], [], [], []\n",
    "    test_labels = []\n",
    "    #TODO 这里test_num也需要根据项目来指定\n",
    "    for i in range(test_num):\n",
    "        test_report.append([])\n",
    "        test_code.append([])\n",
    "        test_labels.append([])\n",
    "        test_path.append([])\n",
    "        test_bug_id.append([])\n",
    "        test_raw_code.append([])\n",
    "        test_row_report.append([])\n",
    "    #1. report进行整理\n",
    "    for rev in revs:\n",
    "        # sent = []\n",
    "        index = rev[\"test\"]\n",
    "        if index >= 0:\n",
    "            sent = getIdxfrom_sent_n(rev[\"text\"], report_maxl, word_idx_map, filter_h=5)\n",
    "            test_bug_id[index].append(rev['bug_id'])\n",
    "            test_report[index].append(sent)\n",
    "            test_row_report[index].append(rev['text'])\n",
    "    #report 整理完成\n",
    "\n",
    "    #2. 对test code进行整理\n",
    "    for revs_single in revs_code:\n",
    "        data_single = []\n",
    "        label_single = []\n",
    "        for rev in revs_single:\n",
    "            sent = getIdxfrom_sent(rev[\"text\"], word_idx_map, code_maxk=code_maxk)\n",
    "            data_single.append(sent)\n",
    "\n",
    "        index = rev[\"test\"]\n",
    "        if index >= 0:\n",
    "            label_single.append(rev[\"y\"])\n",
    "            data_single = np.array(data_single, dtype=\"int\")\n",
    "            data_single = alignData(data_single, code_maxl)\n",
    "            test_code[index].append(data_single)\n",
    "            test_labels[index].append(label_single)\n",
    "            test_path[index].append(rev['path'])\n",
    "            test_raw_code[index].append(revs_single)\n",
    "#     for revs_single in revs_code:\n",
    "#         index = rev[\"test\"]\n",
    "#         if index >= 0:\n",
    "#             data_single = []\n",
    "#             label_single = []\n",
    "#             for rev in revs_single:\n",
    "#                 sent = getIdxfrom_sent(rev[\"text\"], word_idx_map, code_maxk=code_maxk)\n",
    "#                 data_single.append(sent)\n",
    "#             label_single.append(rev[\"y\"])\n",
    "#             data_single = np.array(data_single, dtype=\"int\")\n",
    "#             data_single = alignData(data_single, code_maxl)\n",
    "#             test_code[index].append(data_single)\n",
    "#             test_labels[index].append(label_single)\n",
    "#             test_path[index].append(rev['path'])\n",
    "#             test_raw_code[index].append(revs_single)\n",
    "    #test code整理完成\n",
    "\n",
    "    #single_path='singleProject/'\n",
    "    single_path=''\n",
    "    #3. 将test数据保存\n",
    "    print('*' * 10)\n",
    "    print('test bug id len', len(test_bug_id), ' test report : ', len(test_report), ' test code ', len(test_code),\n",
    "          ' test label ', len(test_labels), ' test path ', len(test_path), 'test_row_report ', len(test_row_report)\n",
    "          , ' test_raw_code ', len(test_raw_code))\n",
    "#     ast_path='/data/hdj/cross_project_trans/out/'+project_name+'tree_blocks.pkl'\n",
    "#     ast_file = pd.read_pickle(ast_path)\n",
    "#     ast_file = ast_file[['path', 'ast_code']]#TODO 将ast生成时字段改为ast_code\n",
    "    for i in range(test_num):\n",
    "        test_labels[i] = np.array(test_labels[i], dtype=\"int\")\n",
    "        test_data = pd.DataFrame()\n",
    "        #300 300 0 0 0 300 0\n",
    "        print(len(test_bug_id[i]),len(test_report[i]),len(test_code[i]),len(test_labels[i]),len(test_path[i]),len(test_row_report[i]),len(test_raw_code[i]))\n",
    "        test_data['bug_id'] = test_bug_id[i]\n",
    "        test_data['cnn_report_idx'] = test_report[i]\n",
    "        test_data['code_idx'] = test_code[i]\n",
    "        test_data['label'] = test_labels[i]\n",
    "        test_data['path'] = test_path[i]\n",
    "        test_data['raw_report'] = test_row_report[i]\n",
    "        test_data['raw_code'] = test_raw_code[i]\n",
    "        # 合并ast数据\n",
    "#         test_data = pd.merge(test_data, ast_file, on='path', how='left')\n",
    "        test_data.to_pickle(report_path+single_path+project_name+'test_middle/test' + str(i) + '.pkl')\n",
    "        # print('bug_id ', test_bug_id[i])\n",
    "        # print('test_report ', test_report[i])\n",
    "        # print('test_code ', test_code[i])\n",
    "        # print('test_labels ', test_labels[i])\n",
    "        # print('test_path ', test_path[i])\n",
    "        # print('test_row_report ', test_row_report[i])\n",
    "        # print('test_raw_code ', test_raw_code[i])\n",
    "    #test数据保存完成\n",
    "    #测试集整理完成\n",
    "\n",
    "    #    random permutation; print to parameters.in\n",
    "    # train_report, train_code, train_labels = random_permutation(train_report, train_code, train_labels)\n",
    "\n",
    "    #80% train数据保存\n",
    "    print('vaocab ', len(vocab))\n",
    "    print('word_idx_map : ', len(word_idx_map))\n",
    "    # print(report_ids[0])\n",
    "    # print(train_report[0])\n",
    "    # print(train_code[0])\n",
    "    # print(train_labels[0])\n",
    "    # print(report[0])\n",
    "    # print(code[0])\n",
    "    # print(train_path[0])\n",
    "    train_data = pd.DataFrame()\n",
    "    print('数据长度 : ', len(report_ids), len(train_report), len(train_code), len(train_labels), len(report), len(code),\n",
    "          len(train_path))\n",
    "    train_data['bug_id'] = report_ids\n",
    "    train_data['cnn_report_idx'] = train_report\n",
    "    train_data['code_idx'] = train_code\n",
    "    train_data['label'] = train_labels\n",
    "    train_data['raw_report'] = report\n",
    "    train_data['raw_code'] = code\n",
    "    train_data['path'] = train_path\n",
    "#     train_data = pd.merge(train_data, ast_file, on='path', how='left')\n",
    "    train_data.to_pickle(report_path+single_path+project_name+'more_train_data.pkl')\n",
    "    #80% 数据保存完成\n",
    "    #20%数据保存\n",
    "    # print(less_report_ids[0])\n",
    "    # print(less_train_report[0])\n",
    "    # print(less_train_code[0])\n",
    "    # print(less_train_labels[0])\n",
    "    # print(less_report[0])\n",
    "    # print(less_code[0])\n",
    "    # print(less_train_path[0])\n",
    "#     less_train_data = pd.DataFrame()\n",
    "#     print('数据长度 : ', len(less_report_ids), len(less_train_report), len(less_train_code), len(less_train_labels), len(less_report), len(less_code),\n",
    "#           len(less_train_path))\n",
    "#     less_train_data['bug_id'] = less_report_ids\n",
    "#     less_train_data['cnn_report_idx'] = less_train_report\n",
    "#     less_train_data['code_idx'] = less_train_code\n",
    "#     less_train_data['label'] = less_train_labels\n",
    "#     less_train_data['raw_report'] = less_report\n",
    "#     less_train_data['raw_code'] = less_code\n",
    "#     less_train_data['path'] = less_train_path\n",
    "# #     less_train_data = pd.merge(less_train_data, ast_file, on='path', how='left')\n",
    "#     less_train_data.to_pickle(report_path +single_path+ project_name + 'less_train_data.pkl')\n",
    "    #20%数据保存完成\n",
    "    print(\"Finish loading\")\n",
    "\n",
    "    pickle.dump([W], open(report_path+single_path + project_name + 'test_middle/'+\"w_parameters.in\", \"wb\"))\n",
    "    return train_report, train_code, train_labels, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_code_name ='org.aspectj/modules/asm/src/org/aspectj/asm/AsmManager.java'\n",
    "# # print(os.path.join(pre, source_file_name))\n",
    "# clean_path='/home/hdj/bug_localization/data/AspectJ/AspectJ-1.5/clean'\n",
    "# vocab=defaultdict(float)\n",
    "# project_name='aspectj_'\n",
    "# test_index=0\n",
    "# spamreader = csv.reader(open(report_path+project_name+'test/test' + str(test_index) + \".csv\", newline=''))\n",
    "# i=0\n",
    "# revs_code=[]\n",
    "# for row in spamreader:\n",
    "#     if i == 0:\n",
    "#         i += 1\n",
    "#         continue\n",
    "#     label = row[2]\n",
    "#     source_code_name=row[1]\n",
    "#     bug_report_id=row[0]\n",
    "#     with open(os.path.join(clean_path,source_code_name), \"r\") as f:\n",
    "#         revs_single = []\n",
    "#         for line in f:\n",
    "#             orig_rev = clean_str(str(line.strip()))  # TODO 更改代码的正则化\n",
    "#             if len(orig_rev) <= 1:\n",
    "#                 orig_rev = \"test\"\n",
    "#             words = set(orig_rev.split())\n",
    "#             for word in words:\n",
    "#                 vocab[word] += 1\n",
    "#             if label.strip() == \"1\":\n",
    "#                 datum = {\"y\": 1,\n",
    "#                          \"text\": orig_rev,\n",
    "#                          \"num_words\": len(orig_rev.split()),\n",
    "#                          \"test\": test_index,\n",
    "#                          \"target\": 1,\n",
    "#                          \"path\":source_code_name,\n",
    "#                          \"bug_id\": bug_report_id}\n",
    "#             else:\n",
    "#                 datum = {\"y\": 0,\n",
    "#                          \"text\": orig_rev,\n",
    "#                          \"num_words\": len(orig_rev.split()),\n",
    "#                          \"test\": test_index,\n",
    "#                          \"target\": 1,\n",
    "#                          \"path\": source_code_name,\n",
    "#                          \"bug_id\": bug_report_id}#TODO 根据这个区匹配对应的AST\n",
    "#             revs_single.append(datum)\n",
    "#         if len(revs_single) == 0:\n",
    "#             revs_single = revs_pre\n",
    "#         if len(revs_single)==0:\n",
    "#             print(os.path.join(clean_path, source_code_name))\n",
    "#             print('here null :',revs_single,' llllll')\n",
    "#         revs_code.append(revs_single) #TODO 记录该report对应的code\n",
    "#         revs_pre = revs_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revs_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test0=pd.read_pickle('/data/hdj/cross_project_trans/report/aspectj_less_train_data.pkl')\n",
    "# type(test0)\n",
    "# test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取80% training_pos的数据\n",
      "读取80% training_pos的数据完成\n",
      "读取20% training_pos的数据\n",
      "读取20% training_pos的数据完成\n",
      "读取80% training negative 的数据\n",
      "读取80% training negative 的数据完成\n",
      "读取20% training negative 的数据\n",
      "读取20% training negative 的数据完成\n",
      "读取test数据 58\n",
      "读取test数据  58  完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token :  3000000\n",
      "load_bin_vec : 2848\n",
      "<class 'dict'> <class 'collections.defaultdict'>\n",
      "一共  24892  个词不在词表里\n",
      "get W  27741 27740\n",
      "这里显示向量真实大小\n",
      "27741 300\n",
      "对train80% 数据进行整理\n",
      "对train80% 数据进行整理完成\n",
      "train 20% 数据整理\n",
      "train 20% 数据整理完成\n",
      "对测试集进行整理\n",
      "**********\n",
      "test bug id len 58  test report :  58  test code  58  test label  58  test path  58 test_row_report  58  test_raw_code  58\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "vaocab  28208\n",
      "word_idx_map :  27740\n",
      "数据长度 :  1668 1668 1668 1668 1668 1668 1668\n",
      "数据长度 :  400 400 400 400 400 400 400\n",
      "Finish loading\n",
      "Finish processing!\n",
      "读取80% training_pos的数据\n",
      "读取80% training_pos的数据完成\n",
      "读取20% training_pos的数据\n",
      "读取20% training_pos的数据完成\n",
      "读取80% training negative 的数据\n",
      "读取80% training negative 的数据完成\n",
      "读取20% training negative 的数据\n",
      "读取20% training negative 的数据完成\n",
      "读取test数据 20\n",
      "读取test数据  20  完成\n",
      "max_token :  3000000\n",
      "load_bin_vec : 1602\n",
      "<class 'dict'> <class 'collections.defaultdict'>\n",
      "一共  8087  个词不在词表里\n",
      "get W  9690 9689\n",
      "这里显示向量真实大小\n",
      "9690 300\n",
      "对train80% 数据进行整理\n",
      "对train80% 数据进行整理完成\n",
      "train 20% 数据整理\n",
      "train 20% 数据整理完成\n",
      "对测试集进行整理\n",
      "**********\n",
      "test bug id len 20  test report :  20  test code  20  test label  20  test path  20 test_row_report  20  test_raw_code  20\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "vaocab  9689\n",
      "word_idx_map :  9689\n",
      "数据长度 :  320 320 320 320 320 320 320\n",
      "数据长度 :  210 210 210 210 210 210 210\n",
      "Finish loading\n",
      "Finish processing!\n",
      "读取80% training_pos的数据\n",
      "读取80% training_pos的数据完成\n",
      "读取20% training_pos的数据\n",
      "读取20% training_pos的数据完成\n",
      "读取80% training negative 的数据\n",
      "读取80% training negative 的数据完成\n",
      "读取20% training negative 的数据\n",
      "读取20% training negative 的数据完成\n",
      "读取test数据 4\n",
      "读取test数据  4  完成\n",
      "max_token :  3000000\n",
      "load_bin_vec : 1503\n",
      "<class 'dict'> <class 'collections.defaultdict'>\n",
      "一共  4020  个词不在词表里\n",
      "get W  5524 5523\n",
      "这里显示向量真实大小\n",
      "5524 300\n",
      "对train80% 数据进行整理\n",
      "对train80% 数据进行整理完成\n",
      "train 20% 数据整理\n",
      "train 20% 数据整理完成\n",
      "对测试集进行整理\n",
      "**********\n",
      "test bug id len 4  test report :  4  test code  4  test label  4  test path  4 test_row_report  4  test_raw_code  4\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "300 300 300 300 300 300 300\n",
      "vaocab  5556\n",
      "word_idx_map :  5523\n",
      "数据长度 :  46 46 46 46 46 46 46\n",
      "数据长度 :  20 20 20 20 20 20 20\n",
      "Finish loading\n",
      "Finish processing!\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # test2()\n",
    "    # test_bug_id()\n",
    "    project_names=['swt_']\n",
    "    # project_names=['swt_','zxing_']\n",
    "    #已经移到了/data/hdj/cross_project_trans/clen_data\n",
    "#     code_clean=['/data/hdj/cross_project_trans/clen_data/aspectj/','/data/hdj/cross_project_trans/clen_data/swt/','/data/hdj/cross_project_trans/clen_data/zxing/']\n",
    "#     code_clean=['/home/hdj/bug_localization/data/AspectJ/AspectJ-1.5/clean','/home/hdj/bug_localization/data/SWT/clean/','/home/hdj/bug_localization/data/ZXing/clean/']\n",
    "#     code_clean_dir=''\n",
    "    #这些目录已经移到了下/data/hdj/cross_project_trans/report\n",
    "    report_path='/data/hdj/cross_project_trans/report/bert/'\n",
    "#     report_path='/data/hdj/cross_project_trans/report/'\n",
    "#     w2v_file = \"/home/hdj/GoogleNews-vectors-negative300.bin\"\n",
    "    #/data/hdj/cross_project_trans/embedding\n",
    "#     w2v_file = \"/home/hdj/cross_project_trans/embedding/word_w2v_100\"\n",
    "    w2v_file='/data/hdj/GoogleNews-vectors-negative300.bin'\n",
    "    index_num = 1\n",
    "    # for repeat_index in range(index_num):\n",
    "    i=0\n",
    "    for project_name in project_names:\n",
    "        code_maxl = 100  # code_maxl max statements per file\n",
    "        code_maxk = 40  # code_maxk max words per statement\n",
    "\n",
    "        # test_num = 58\n",
    "        report_maxl = 300  # max report length\n",
    "        test_c = 1000  # random choose 300 candidate source code\n",
    "        code_clean=os.path.join(report_path,project+'_code')\n",
    "        train_report, train_code, train_labels, W = load_data(code_maxl, code_maxk, report_maxl,w2v_file, report_path,project_name,project_name+'80_',project_name+'20_',code_clean,hdjWord2vec=False)\n",
    "        i+=1\n",
    "        # pickle.dump([train_report, train_code, train_labels, W],open(\"parameters.in\", \"wb\"))\n",
    "        #pickle.dump([train_report, train_code, train_labels, W],open(\"JDT_CNN/parameters.in\", \"wb\"))\n",
    "        print(\"Finish processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=gensim.models.KeyedVectors.load_word2vec_format('/data/hdj/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# w2v = Word2Vec.load('/home/hdj/cross_project_trans/embedding/word_w2v_100')#模型的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0=pd.read_pickle('/home/hdj/cross_project_trans/report/aspectj_test_middle/test0.pkl')\n",
    "aspectj_more_train_data=pd.read_pickle('/home/hdj/cross_project_trans/report/aspectj_more_train_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_id</th>\n",
       "      <th>cnn_report_idx</th>\n",
       "      <th>code_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>raw_report</th>\n",
       "      <th>raw_code</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28974</td>\n",
       "      <td>[0, 0, 0, 0, 36, 12, 12973, 57, 42, 17, 23, 12...</td>\n",
       "      <td>[[62, 61, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>compiler error introducing field aspect fails ...</td>\n",
       "      <td>[{'y': 1, 'text': 'org aspectj weaver', 'num_w...</td>\n",
       "      <td>org.aspectj/modules/weaver/src/org/aspectj/wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28919</td>\n",
       "      <td>[0, 0, 0, 0, 13030, 284, 289, 295, 273, 276, 2...</td>\n",
       "      <td>[[62, 61, 63, 316, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>waever tries weave methods find exception bug ...</td>\n",
       "      <td>[{'y': 1, 'text': 'org aspectj weaver bcel', '...</td>\n",
       "      <td>org.aspectj/modules/weaver/src/org/aspectj/wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29186</td>\n",
       "      <td>[0, 0, 0, 0, 49, 684, 13106, 253, 13107, 692, ...</td>\n",
       "      <td>[[62, 61, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>ajc emacssym chokes pointcut includes intertyp...</td>\n",
       "      <td>[{'y': 1, 'text': 'org aspectj weaver', 'num_w...</td>\n",
       "      <td>org.aspectj/modules/weaver/src/org/aspectj/wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29186</td>\n",
       "      <td>[0, 0, 0, 0, 49, 684, 13106, 253, 13107, 692, ...</td>\n",
       "      <td>[[62, 61, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>ajc emacssym chokes pointcut includes intertyp...</td>\n",
       "      <td>[{'y': 1, 'text': 'org aspectj weaver', 'num_w...</td>\n",
       "      <td>org.aspectj/modules/weaver/src/org/aspectj/wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29186</td>\n",
       "      <td>[0, 0, 0, 0, 49, 684, 13106, 253, 13107, 692, ...</td>\n",
       "      <td>[[62, 61, 63, 316, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ajc emacssym chokes pointcut includes intertyp...</td>\n",
       "      <td>[{'y': 1, 'text': 'org aspectj weaver bcel', '...</td>\n",
       "      <td>org.aspectj/modules/weaver/src/org/aspectj/wea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bug_id                                     cnn_report_idx  \\\n",
       "0  28974  [0, 0, 0, 0, 36, 12, 12973, 57, 42, 17, 23, 12...   \n",
       "1  28919  [0, 0, 0, 0, 13030, 284, 289, 295, 273, 276, 2...   \n",
       "2  29186  [0, 0, 0, 0, 49, 684, 13106, 253, 13107, 692, ...   \n",
       "3  29186  [0, 0, 0, 0, 49, 684, 13106, 253, 13107, 692, ...   \n",
       "4  29186  [0, 0, 0, 0, 49, 684, 13106, 253, 13107, 692, ...   \n",
       "\n",
       "                                            code_idx  label  \\\n",
       "0  [[62, 61, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      1   \n",
       "1  [[62, 61, 63, 316, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1   \n",
       "2  [[62, 61, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      1   \n",
       "3  [[62, 61, 63, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      1   \n",
       "4  [[62, 61, 63, 316, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1   \n",
       "\n",
       "                                          raw_report  \\\n",
       "0  compiler error introducing field aspect fails ...   \n",
       "1  waever tries weave methods find exception bug ...   \n",
       "2  ajc emacssym chokes pointcut includes intertyp...   \n",
       "3  ajc emacssym chokes pointcut includes intertyp...   \n",
       "4  ajc emacssym chokes pointcut includes intertyp...   \n",
       "\n",
       "                                            raw_code  \\\n",
       "0  [{'y': 1, 'text': 'org aspectj weaver', 'num_w...   \n",
       "1  [{'y': 1, 'text': 'org aspectj weaver bcel', '...   \n",
       "2  [{'y': 1, 'text': 'org aspectj weaver', 'num_w...   \n",
       "3  [{'y': 1, 'text': 'org aspectj weaver', 'num_w...   \n",
       "4  [{'y': 1, 'text': 'org aspectj weaver bcel', '...   \n",
       "\n",
       "                                                path  \n",
       "0  org.aspectj/modules/weaver/src/org/aspectj/wea...  \n",
       "1  org.aspectj/modules/weaver/src/org/aspectj/wea...  \n",
       "2  org.aspectj/modules/weaver/src/org/aspectj/wea...  \n",
       "3  org.aspectj/modules/weaver/src/org/aspectj/wea...  \n",
       "4  org.aspectj/modules/weaver/src/org/aspectj/wea...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspectj_more_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AdamW\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要为数据的code生成文本形式\n",
    "def code_merge(raw_code_list):\n",
    "    code_list=[]\n",
    "    for raw_code in raw_code_list:\n",
    "        code_list.append(raw_code['text'])\n",
    "    code_text=' '.join(code_list)\n",
    "    return code_text\n",
    "\n",
    "def data_tensor_generate(data,maxLength=512,ttype='train'):\n",
    "    '''\n",
    "        输入是dataframe 包含bug_id\tcnn_report_idx\tcode_idx\tlabel\tpath\traw_report\traw_code\n",
    "    '''\n",
    "    #将raw_code里dict形式合并生成code_text\n",
    "    data['raw_code_text']=data['raw_code'].apply(code_merge)\n",
    "    #生成完成\n",
    "    all_labels=list(data['label'])\n",
    "    all_cnn_report_idx=list(data['cnn_report_idx'])\n",
    "    all_code_idx=list(data['code_idx'])\n",
    "    raw_report=list(data['raw_report'])\n",
    "    raw_code=list(data['raw_code_text'])\n",
    "    reportInputs=tokenizer.batch_encode_plus(raw_report,add_special_tokens=True,max_length=maxLength,pad_to_max_length=True,return_token_type_ids=True,truncation=True)\n",
    "    codeInputs=tokenizer.batch_encode_plus(raw_code,add_special_tokens=True,max_length=maxLength,pad_to_max_length=True,return_token_type_ids=True,truncation=True)\n",
    "    all_report_input_ids=reportInputs['input_ids']\n",
    "    all_report_input_mask=reportInputs['attention_mask']\n",
    "    all_code_input_ids=codeInputs['input_ids']\n",
    "    all_code_input_mask=codeInputs['attention_mask']\n",
    "    \n",
    "    all_report_input_ids = torch.tensor(all_report_input_ids, dtype=torch.long)\n",
    "    all_report_input_mask = torch.tensor(all_report_input_mask, dtype=torch.long)\n",
    "    all_code_input_ids = torch.tensor(all_code_input_ids, dtype=torch.long)\n",
    "    all_code_input_mask = torch.tensor(all_code_input_mask, dtype=torch.long)\n",
    "    \n",
    "    all_cnn_report_idx=torch.tensor(all_cnn_report_idx, dtype=torch.long)\n",
    "    all_code_idx=torch.tensor(all_code_idx, dtype=torch.long)\n",
    "    all_labels = torch.tensor(all_labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_report_input_ids, all_report_input_mask, all_code_input_ids, all_code_input_mask,all_cnn_report_idx,\n",
    "                          all_code_idx, all_labels)\n",
    "    if ttype=='train':\n",
    "        return dataset\n",
    "    else:\n",
    "        return dataset,data\n",
    "dataset=data_tensor_generate(aspectj_more_train_data)\n",
    "\n",
    "    #\n",
    "# raw_report[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1668, 512]),\n",
       " torch.Size([1668, 512]),\n",
       " torch.Size([1668, 512]),\n",
       " torch.Size([1668, 512]),\n",
       " torch.Size([1668, 208]),\n",
       " torch.Size([1668, 100, 40]),\n",
       " torch.Size([1668]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tensors[0].shape,dataset.tensors[1].shape,dataset.tensors[2].shape,dataset.tensors[3].shape,dataset.tensors[4].shape,dataset.tensors[5].shape,dataset.tensors[6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectj_w= pickle.load(open('/home/hdj/cross_project_trans/report/aspectj_test_middle/w_parameters.in','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29864, 100)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspectj_w[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnn_report_idx=dataset.tensors[4]\n",
    "all_code_idx=dataset.tensors[5]\n",
    "aspectj_w= pickle.load(open('/home/hdj/cross_project_trans/report/aspectj_test_middle/w_parameters.in','rb'))\n",
    "embedding = nn.Embedding.from_pretrained(torch.tensor(aspectj_w[0]))#从已有的weight导入embedding\n",
    "train_code = embedding(all_cnn_report_idx)\n",
    "train_report =embedding(all_code_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1668, 208, 100]), torch.Size([1668, 100, 40, 100]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_code.shape,train_report.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report=['a a a a a a b b b b','a a a a a a b b b b']\n",
    "report=raw_report\n",
    "inputs=tokenizer.batch_encode_plus(report,add_special_tokens=True,max_length=10,pad_to_max_length=True,return_token_type_ids=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 'Ġa', '<s>', '</s>')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('a'),tokenizer.convert_ids_to_tokens(10),tokenizer.convert_ids_to_tokens(0),tokenizer.convert_ids_to_tokens(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1668"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aspectj_more_train_data['raw_code_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aspectj_more_train_data['raw_code'][0])\n",
    "\n",
    "for index, row in aspectj_more_train_data.iterrows():\n",
    "    all_code_idx.append(row['code_idx'])\n",
    "    all_cnn_report_idx.append(row['cnn_report_idx'])\n",
    "    all_labels.append(row['label'])\n",
    "    #处理report编码\n",
    "    reportInputs=tokenizer.encode_plus(row['raw_report'],add_special_tokens=True,max_length=512,pad_to_max_length=True,return_token_type_ids=True,truncation=True)\n",
    "    all_report_input_ids.append(reportInputs['input_ids'])\n",
    "    all_report_input_mask.append(reportInputs['attention_mask'])\n",
    "    #处理report编码完成\n",
    "    #处理code编码\n",
    "    code_list=[]\n",
    "    for raw_code in row['raw_code']:\n",
    "        code_list.append(raw_code['text'])\n",
    "    code_text=' '.join(code_list)\n",
    "    codeInputs=tokenizer.encode_plus(code_text,add_special_tokens=True,max_length=10,pad_to_max_length=True,return_token_type_ids=True,truncation=True)\n",
    "    all_code_input_ids.append(codeInputs['input_ids'])\n",
    "    all_code_input_mask.append(codeInputs['attention_mask'])\n",
    "    #处理code编码完成\n",
    "#     print(type(aspectj_more_train_data),row['label'],row['cnn_report_idx'])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 44904, 5849, 10345, 882, 6659, 10578, 33172, 5591, 2]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[0, 31118, 6659, 267, 52, 9903, 31118, 6659, 267, 2]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[1]\n",
      "[[0, 0, 0, 0, 36, 12, 12973, 57, 42, 17, 23, 12972, 50, 12, 49, 55, 28, 22, 43, 55, 28, 22, 47, 43, 29, 44, 11, 12986, 39, 12977, 55, 28, 22, 12976, 43, 8, 46, 41, 57, 55, 28, 22, 19, 49, 13, 58, 25, 31, 48, 37, 12, 55, 28, 22, 55, 28, 22, 47, 12978, 18, 52, 3, 9, 20, 56, 12985, 51, 12980, 30, 27, 43, 12982, 38, 12987, 59, 53, 32, 4, 30, 27, 43, 24, 12984, 42, 12976, 45, 40, 19, 12978, 14, 7, 12975, 19, 25, 21, 19, 12983, 19, 12979, 25, 2, 26, 12979, 1, 15, 6, 53, 16, 54, 12974, 35, 12981, 33, 34, 60, 47, 34, 60, 6, 57, 28, 22, 5, 60, 34, 60, 47, 34, 60, 5, 60, 34, 60, 34, 60, 34, 60, 47, 10, 47, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[array([[ 62,  61,  63, ...,   0,   0,   0],\n",
      "       [ 62,  61,  65, ...,   0,   0,   0],\n",
      "       [ 62,  61,  65, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [100,  95,  74, ...,   0,   0,   0],\n",
      "       [ 74,  97,   0, ...,   0,   0,   0],\n",
      "       [ 94,  98,  53, ...,   0,   0,   0]])]\n"
     ]
    }
   ],
   "source": [
    "print(all_report_input_ids)\n",
    "print(all_report_input_mask)\n",
    "print(all_code_input_ids)\n",
    "print(all_code_input_mask)\n",
    "print(all_labels)\n",
    "print(all_cnn_report_idx)\n",
    "print(all_code_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_id</th>\n",
       "      <th>cnn_report_idx</th>\n",
       "      <th>code_idx</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>raw_report</th>\n",
       "      <th>raw_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146546</td>\n",
       "      <td>[0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...</td>\n",
       "      <td>[[62, 61, 828, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>org.aspectj/modules/asm/src/org/aspectj/asm/As...</td>\n",
       "      <td>remove hard coded dependency getfilename metho...</td>\n",
       "      <td>[{'y': 1, 'text': 'org aspectj asm', 'num_word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146546</td>\n",
       "      <td>[0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...</td>\n",
       "      <td>[[62, 61, 828, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>org.aspectj/modules/asm/src/org/aspectj/asm/in...</td>\n",
       "      <td>remove hard coded dependency getfilename metho...</td>\n",
       "      <td>[{'y': 1, 'text': 'org aspectj asm intern', 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146546</td>\n",
       "      <td>[0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...</td>\n",
       "      <td>[[309, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>org.aspectj/modules/tests/base/test135/Driver....</td>\n",
       "      <td>remove hard coded dependency getfilename metho...</td>\n",
       "      <td>[{'y': 0, 'text': 'test', 'num_words': 1, 'tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146546</td>\n",
       "      <td>[0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...</td>\n",
       "      <td>[[62, 61, 309, 997, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "      <td>org.aspectj/modules/aspectj-attic/testing-src/...</td>\n",
       "      <td>remove hard coded dependency getfilename metho...</td>\n",
       "      <td>[{'y': 0, 'text': 'org aspectj test compar', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>146546</td>\n",
       "      <td>[0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...</td>\n",
       "      <td>[[43, 325, 706, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>org.aspectj/modules/tests/bugs150/pr113947/cas...</td>\n",
       "      <td>remove hard coded dependency getfilename metho...</td>\n",
       "      <td>[{'y': 0, 'text': 'java util collect', 'num_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bug_id                                     cnn_report_idx  \\\n",
       "0  146546  [0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...   \n",
       "1  146546  [0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...   \n",
       "2  146546  [0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...   \n",
       "3  146546  [0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...   \n",
       "4  146546  [0, 0, 0, 0, 1937, 4128, 19718, 4679, 19720, 2...   \n",
       "\n",
       "                                            code_idx  label  \\\n",
       "0  [[62, 61, 828, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...      1   \n",
       "1  [[62, 61, 828, 87, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1   \n",
       "2  [[309, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0   \n",
       "3  [[62, 61, 309, 997, 0, 0, 0, 0, 0, 0, 0, 0, 0,...      0   \n",
       "4  [[43, 325, 706, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      0   \n",
       "\n",
       "                                                path  \\\n",
       "0  org.aspectj/modules/asm/src/org/aspectj/asm/As...   \n",
       "1  org.aspectj/modules/asm/src/org/aspectj/asm/in...   \n",
       "2  org.aspectj/modules/tests/base/test135/Driver....   \n",
       "3  org.aspectj/modules/aspectj-attic/testing-src/...   \n",
       "4  org.aspectj/modules/tests/bugs150/pr113947/cas...   \n",
       "\n",
       "                                          raw_report  \\\n",
       "0  remove hard coded dependency getfilename metho...   \n",
       "1  remove hard coded dependency getfilename metho...   \n",
       "2  remove hard coded dependency getfilename metho...   \n",
       "3  remove hard coded dependency getfilename metho...   \n",
       "4  remove hard coded dependency getfilename metho...   \n",
       "\n",
       "                                            raw_code  \n",
       "0  [{'y': 1, 'text': 'org aspectj asm', 'num_word...  \n",
       "1  [{'y': 1, 'text': 'org aspectj asm intern', 'n...  \n",
       "2  [{'y': 0, 'text': 'test', 'num_words': 1, 'tes...  \n",
       "3  [{'y': 0, 'text': 'org aspectj test compar', '...  \n",
       "4  [{'y': 0, 'text': 'java util collect', 'num_wo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove hard coded dependency getfilename methods get file name part bug need remove hard coded dependency methods use handles get filename methods asmmanager getfilename string aspectjelementhierarchy getfilename string asm manager get file name aspect element hierarchy get file name'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test0['raw_report'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open('/home/hdj/NPCNN_hdj/parameters.in','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[0]\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('/data/hdj/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_emd=w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['author'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectj_data=pickle.load(open('/home/hdj/NPCNN_hdj/parameters.in','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=aspectj_data[3]\n",
    "len(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org aspectj asm java io java util org aspectj asm intern org aspectj bridg isourceloc sourc locat asmmanag asm manag asmmanag instanc asmmanag asm manag asm manag ielementhandleprovid handleprovid element handl provid handl provid list structurelisten arraylist structur listen array list setrelationshipmap irelationshipmap irm mapper irm set relationship map relationship map sethierarchi ihierarchi ih hierarchi ih set hierarchi hierarchi ihierarchi hierarchi hierarchi irelationshipmap mapper relationship map creatingmodel creat model dumpmodelpostbuild dump model post build attemptincrementalmodelrepair attempt increment model repair dumpmodel dump model dumprelationship dump relationship dumpdeltaprocess dump delta process imodelfilt modelfilt model filter model filter string dumpfilenam dump filenam report completingtypebind complet type bind asmmanag asm manag handleprovid jdtlikehandleprovid handl provid jdt like handl provid createnewasm creat asm createnewasm creat asm hierarchi aspectjelementhierarchi aspect element hierarchi mapper relationshipmap hierarchi relationship map handleprovid initi handl provid ihierarchi gethierarchi hierarchi get hierarchi hierarchi asmmanag getdefault asm manag get instanc irelationshipmap getrelationshipmap relationship map get relationship map mapper firemodelupd fire model updat notifylisten notifi listen dumpmodelpostbuild hierarchi getconfigfil dump model post build get config file writestructuremodel hierarchi getconfigfil write structur model get config file hashmap getinlineannot hash map get inlin annot string sourcefil sourc file showsubmemb show sub member showmemberandtyp show member type hierarchi isvalid valid hashmap annot hashmap hash map hash map iprogramel node hierarchi findelementforsourcefil sourcefil program element find element sourc file sourc file node ihierarchi hierarchi structur iprogramel filenod iprogramel node program element file node program element arraylist penod arraylist array list pe node array list getallstructurechildren filenod penod showsubmemb showmemberandtyp get structur children file node pe node show sub member show member type iter penod iter hasnext pe node next iprogramel penod iprogramel next program element pe node program element list entri arraylist array list entri add penod pe node isourceloc sourceloc penod getsourceloc sourc locat sourc loc pe node get sourc locat sourceloc sourc loc integ hash integ sourceloc getlin sourc loc get line list existingentri list annot get hash exist entri existingentri exist entri entri addal existingentri add exist entri annot put hash entri annot getallstructurechildren iprogramel node list result showsubmemb showmemberandtyp get structur children program element show sub member show member type list children node getchildren get children node getchildren get children iter children iter hasnext next iprogramel next iprogramel next program element program element list rel asmmanag getdefault getrelationshipmap get next asm manag get get relationship map next next getkind iprogramel kind code showsubmemb get kind program element show sub member next getkind iprogramel kind code showmemberandtyp get kind program element show member type rel rel size result add next getallstructurechildren iprogramel next result showsubmemb showmemberandtyp get structur children program element show sub member show member type addlisten ihierarchylisten listen add listen hierarchi listen structurelisten add listen structur listen removestructurelisten ihierarchylisten listen remov structur listen hierarchi listen structurelisten remov listen structur listen removealllisten remov listen structurelisten clear structur listen notifylisten notifi listen iter structurelisten iter hasnext structur listen next ihierarchylisten next elementsupd hierarchi hierarchi listen element updat ielementhandleprovid gethandleprovid element handl provid get handl provid handleprovid handl provid sethandleprovid ielementhandleprovid handleprovid set handl provid element handl provid handl provid handleprovid handleprovid handl provid handl provid writestructuremodel string configfilepath write structur model config file path string filepath genexternfilepath configfilepath file path gen extern file path config file path fileoutputstream fo fileoutputstream filepath file output stream file output stream file path objectoutputstream objectoutputstream fo object output stream object output stream writeobject hierarchi write object writeobject mapper write object flush fo flush fo close close except readstructuremodel string configfilepath read structur model config file path hierarchyreadok hierarchi read ok configfilepath config file path hierarchi setroot ihierarchi set root hierarchi structur string filepath genexternfilepath configfilepath file path gen extern file path config file path fileinputstream fileinputstream filepath file input stream file input stream file path objectinputstream objectinputstream object input stream object input stream hierarchi aspectjelementhierarchi readobject aspect element hierarchi read object hierarchyreadok hierarchi read ok mapper relationshipmap readobject relationship map read object relationshipmap mapper sethierarchi hierarchi relationship map set hierarchi filenotfoundexcept fnfe file found except hierarchi setroot ihierarchi set root hierarchi structur eofexcept eof eof except hierarchyreadok hierarchi read ok system err println asmmanag unabl read structur model configfilepath asm manag config file path eof printstacktrac print stack trace hierarchi setroot ihierarchi set root hierarchi structur except hierarchi setroot ihierarchi set root hierarchi structur notifylisten notifi listen string genexternfilepath string configfilepath gen extern file path config file path configfilepath lastindexof lst config file path last index configfilepath configfilepath substr configfilepath lastindexof lst config file path config file path config file path last index configfilepath ajsym config file path string getcanonicalfilepath file get canon file path canonicalfilepathmap get canon file path map canonicalfilepathmap canonicalfilepathmap canonicalfilepathmap canon file path map canon file path map canon file path map canonicalfilepathmap canon file path map max size map pathmap hashmap path map hash map string get file string ret string pathmap get getpath path map get path ret ret getcanonicalpath get canon path ioexcept ioex io except io ex ret getpath get path pathmap put getpath ret path map get path pathmap size pathmap clear path map max size path map ret setreport string filenam dmodel drel ddeltaprocess set report model rel delta process deletefil report dumpmodel dmodel dump model model dumprelationship drel dump relationship rel dumpdeltaprocess ddeltaprocess dump delta process delta process deletefil file filenam delet dumpfilenam filenam dump filenam setreport string filenam dmodel drel ddeltaprocess set report model rel delta process deletefil imodelfilt afilt model filter filter setreport filenam dmodel drel ddeltaprocess deletefil set report model rel delta process modelfilt afilt model filter filter isreport report report setdontreport set dont report report dumpdeltaprocess dump delta process dumpmodel dump model dumprelationship dump relationship reportmodelinfo string reasonforreport report model info reason report dumpmodel dumprelationship dump model dump relationship filewrit fw filewrit dumpfilenam file writer file writer dump filenam bufferedwrit bw bufferedwrit fw buffer writer buffer writer dumpmodel dump model bw write model statu report reasonforreport reason report dumptre bw asmmanag getdefault gethierarchi getroot asm manag get get hierarchi get root bw write end model report dumprelationship dump relationship bw write relationship report reasonforreport reason report dumprel bw bw write end relationship report properti modelinfo summarizemodel getproperti model info summar model get properti enumer pkeyenum key bw write properti model relationship map pkeyenum hasmoreel element string pkey string pkeyenum nextel next element bw write pkey getproperti pkey get properti bw flush fw close ioexcept io except system err println internalerror unabl report model inform intern error printstacktrac print stack trace dumptre writer iprogramel node indent ioexcept program element io except indent write string loc node node getsourceloc get sourc locat loc node getsourceloc tostr get sourc locat string modelfilt loc modelfilt processfileloc loc model filter model filter process fileloc write node node node getkind tostr loc get kind string node iter node getchildren iter hasnext get children next dumptre iprogramel next indent program element dumptre iprogramel node indent ioexcept program element io except indent system print string loc node node getsourceloc get sourc locat loc node getsourceloc tostr get sourc locat string system println node node node getkind tostr loc get kind string node iter node getchildren iter hasnext get children next dumptre iprogramel next indent program element dumprel writer ioexcept io except irelationshipmap irm asmmanag getdefault getrelationshipmap relationship map asm manag get get relationship map ctr set entri irm getentri get entri iter iter entri iter iter hasnext next string hid string iter next list rel irm get hid iter iter rel iter iter hasnext next irelationship ir irelationship iter next relationship relationship list target ir gettarget get target iter iter target iter iter hasnext next string thid string iter next stringbuff sb stringbuff string buffer string buffer modelfilt modelfilt wantshandleid sb append hid ctr model filter model filter want handl id sb append target target size hid ir getnam thid get name write sb tostr string dumprelsstderr string key dumprel stderr system err println relationship dump follow key irelationshipmap irm asmmanag getdefault getrelationshipmap relationship map asm manag get get relationship map ctr set entri irm getentri get entri iter iter entri iter iter hasnext next string hid string iter next list rel irm get hid iter iter rel iter iter hasnext next irelationship ir irelationship iter next relationship relationship list target ir gettarget get target iter iter target iter iter hasnext next string thid string iter next system err println hid ctr target target size hid ir getnam thid get name system err println end relationship dump key removestructuremodelforfil writer fw collect file ioexcept remov structur model file io except ihierarchi model asmmanag getdefault gethierarchi hierarchi asm manag get get hierarchi modelmodifi model modifi set deletednod hashset delet node hash set iter iter file iter iter hasnext next file fileforcompil file iter next file compil string correctedpath asmmanag getdefault getcanonicalfilepath fileforcompil correct path asm manag get get canon file path file compil iprogramel progelem iprogramel model findinfilemap correctedpath program element prog elem program element find file map correct path progelem prog elem dumpdeltaprocess dump delta process fw write delet progelem node file fileforcompil prog elem file compil removenod progelem remov node prog elem deletednod add getcanonicalfilepath progelem getsourceloc getsourcefil delet node get canon file path prog elem get sourc locat get sourc file model removefromfilemap correctedpath tostr remov file map correct path string runtimeexcept whilst repair model remov entri file correctedpath tostr filemap runtim except correct path string modelmodifi model modifi modelmodifi model updatehandlemap deletednod model modifi updat handl map delet node modelmodifi model modifi flushmodelcach flush model cach ihierarchi model asmmanag getdefault gethierarchi hierarchi asm manag get get hierarchi model flushtypemap flush type map fixupstructuremodel writer fw list filestobecompil set set ioexcept fixup structur model file compil file ad file delet io except ihierarchi model asmmanag getdefault gethierarchi hierarchi asm manag get get hierarchi modelmodifi model modifi set filestoremovefromstructuremodel hashset filestobecompil file remov structur model hash set file compil filestoremovefromstructuremodel addal file remov structur model add file delet set deletednod hashset delet node hash set iter iter filestoremovefromstructuremodel iter iter hasnext file remov structur model next file fileforcompil file iter next file compil string correctedpath asmmanag getdefault getcanonicalfilepath fileforcompil correct path asm manag get get canon file path file compil iprogramel progelem iprogramel model findinfilemap correctedpath program element prog elem program element find file map correct path progelem prog elem dumpdeltaprocess dump delta process fw write delet progelem node file fileforcompil prog elem file compil removenod progelem remov node prog elem deletednod add getcanonicalfilepath progelem getsourceloc getsourcefil delet node get canon file path prog elem get sourc locat get sourc file model removefromfilemap correctedpath tostr remov file map correct path string runtimeexcept whilst repair model remov entri file correctedpath tostr filemap runtim except correct path string modelmodifi model modifi modelmodifi model modifi model flushtypemap flush type map model updatehandlemap deletednod updat handl map delet node processdelta list set set process delta file tobecompil file ad file delet writer fw dumpdeltaprocess dump delta process filewrit filew filewrit dumpfilenam file writer file writer dump filenam fw bufferedwrit filew buffer writer fw write process delta chang model fw write file compil size file tobecompil file tobecompil fw write file ad size file ad file ad fw write file delet size file delet file delet stime system currenttimemilli current time milli modificationoccur modif occur modificationoccur modif occur removestructuremodelforfil fw remov structur model file file delet modificationoccur modif occur etim system currenttimemilli current time milli repairrelationship fw repair relationship etim system currenttimemilli current time milli modificationoccur modif occur removestructuremodelforfil fw remov structur model file file tobecompil modificationoccur modif occur dumpdeltaprocess dump delta process fw write delta process time fw write hierarchi etim stime ms relationshipmap etim etim ms fw write travers fw write fw flush fw close reportmodelinfo delta process report model info ioexcept io except printstacktrac print stack trace removerelationshipstargettingthistyp string typenam remov relationship target type debug debug system err println removerelationshipstargettingthistyp typenam remov relationship target type string pkg string type typenam lastsep typenam lastindexof last sep last index lastsep last sep pkg typenam substr lastsep last sep type typenam substr lastsep last sep didsometh iprogramel typenod hierarchi findelementfortyp pkg type program element type node find element type typenod type node set sourcestoremov hashset sourc remov hash set set sourcehandlesset mapper getentri sourcehandl set get entri list relationshipstoremov arraylist relationship remov array list iter keyit sourcehandlesset iter keyit hasnext sourcehandl set next string hid string keyit next iprogramel sourceel hierarchi getel hid program element sourc element get element sourceel sametyp hid sourceel typenod sourc element type sourc element type node relationshipstoremov clear relationship remov list relationship mapper get hid iter relit relationship iter relit hasnext next irelationship rel irelationship relit next relationship relationship rel getkind irelationship kind get kind relationship use pointcut rel isaffect affect relationshipstoremov add rel relationship remov relationshipstoremov size relationship remov didsometh relationshipstoremov size relationship size sourcestoremov add hid relationship remov sourc remov relationshipstoremov size relationship remov relationship remov relationshipstoremov get relationship remov iter srciter sourcestoremov iter srciter hasnext sourc remov next string hid string srciter next mapper removeal hid remov iprogramel ipe hierarchi getel hid program element get element ipe ipe getkind equal iprogramel kind code get kind program element debug system err println sourc handl code node remov well code ipe parent ipe getpar get parent removesinglenod ipe remov singl node debug dumprelsstderr process affectedbi dumprel stderr didsometh sourcestoremov clear sourc remov debug dumprelsstderr process affect dumprel stderr sourcehandlesset mapper getentri sourcehandl set get entri iter keyit sourcehandlesset iter keyit hasnext sourcehandl set next string hid string keyit next iprogramel sourceel hierarchi getel hid program element sourc element get element relationshipstoremov clear relationship remov list relationship mapper get hid iter relit relationship iter relit hasnext next irelationship rel irelationship relit next relationship relationship rel getkind irelationship kind get kind relationship use pointcut rel isaffect affect list target rel gettarget get target list targetstoremov arraylist target remov array list iter targetsit target iter targetsit hasnext target iter target iter next string targethid string targetsit next target iter iprogramel existingtarget hierarchi getel targethid program element exist target get element existingtarget sametyp targethid existingtarget typenod targetstoremov add targethid exist target type exist target type node target remov targetstoremov size target remov targetstoremov size target size relationshipstoremov add rel target remov relationship remov iter targsit targetstoremov iter targsit hasnext targ iter target remov targ iter next string togo string targsit next targ iter target remov togo relationshipstoremov size relationship remov relationshipstoremov size relationship size sourcestoremov add hid relationship remov sourc remov relationshipstoremov size relationship remov relationship remov relationshipstoremov get relationship remov iter srciter sourcestoremov iter srciter hasnext sourc remov next string hid string srciter next mapper removeal hid remov iprogramel ipe hierarchi getel hid program element get element ipe ipe getkind equal iprogramel kind code get kind program element debug system err println sourc handl code node remov well code ipe parent ipe getpar get parent removesinglenod ipe remov singl node debug dumprelsstderr process affect dumprel stderr debug system err println removerelationshipstargettingthisfil remov relationship target file sametyp string hid iprogramel target iprogramel type type program element program element iprogramel containingtyp target program element contain type target runtimeexcept target runtim except type runtimeexcept type runtim except target getkind issourcefil get kind sourc file target getsourceloc get sourc locat type getsourceloc get sourc locat target getsourceloc getsourcefil get sourc locat get sourc file type getsourceloc getsourcefil get sourc locat get sourc file target getsourceloc getsourcefil equal type getsourceloc getsourcefil get sourc locat get sourc file get sourc locat get sourc file containingtyp getkind istyp contain type get kind type containingtyp containingtyp getpar contain type contain type get parent type equal containingtyp contain type repairrelationship writer fw repair relationship ihierarchi model asmmanag getdefault gethierarchi hierarchi asm manag get get hierarchi dumpdeltaprocess fw write repair relationship map dump delta process irelationshipmap irm asmmanag getdefault getrelationshipmap relationship map asm manag get get relationship map set sourcestoremov hashset sourc remov hash set set nonexistinghandl hashset non exist handl hash set srchandlecount tgthandlecount set keyset irm getentri get entri iter keyit keyset iter keyit hasnext next string hid string keyit next srchandlecount nonexistinghandl contain hid non exist handl sourcestoremov add hid sourc remov iprogramel existingel model getel hid program element exist element get element dumpdeltaprocess fw write look handl hid model found existingel dump delta process exist element existingel exist element sourcestoremov add hid sourc remov nonexistinghandl add hid non exist handl list relationship irm get hid list relationshipstoremov arraylist relationship remov array list iter relit relationship iter relit hasnext next irelationship rel irelationship relit next relationship relationship list target rel gettarget get target list targetstoremov arraylist target remov array list iter targetit target iter targetit hasnext target iter target iter next string targethid string targetit next target iter tgthandlecount nonexistinghandl contain targethid non exist handl dumpdeltaprocess fw write target handl targethid srchid hid rel rel getnam exist dump delta process get name targetstoremov add targethid target remov iprogramel existingtarget model getel targethid program element exist target get element existingtarget exist target dumpdeltaprocess fw write target handl targethid srchid hid rel rel getnam exist dump delta process get name targetstoremov add targethid target remov nonexistinghandl add targethid non exist handl targetstoremov size target remov targetstoremov size target size target remov dumpdeltaprocess fw write target remain srchid hid rel rel getnam remov dump delta process get name relationshipstoremov add rel relationship remov iter targsit targetstoremov iter targsit hasnext targ iter target remov targ iter next string togo string targsit next targ iter target remov togo target size dumpdeltaprocess fw write target remain srchid hid rel rel getnam remov dump delta process get name relationshipstoremov add rel relationship remov relationshipstoremov size relationship remov relationshipstoremov size relationship size relationship remov sourcestoremov add hid sourc remov relationshipstoremov size relationship remov irelationship irel irelationship relationshipstoremov get relationship relationship relationship remov verifyassumpt irm remov hid irel fail remov relationship irel getnam shid hid verifi assumpt get name list rel irm get hid rel rel size sourcestoremov add hid sourc remov iter srciter sourcestoremov iter srciter hasnext sourc remov next string hid string srciter next irm removeal hid remov iprogramel ipe model getel hid program element get element ipe ipe getkind equal iprogramel kind code get kind program element removesinglenod ipe remov singl node ioexcept ioe io except system err println fail repair relationship ioe printstacktrac print stack trace removesinglenod iprogramel progelem remov singl node program element prog elem verifyassumpt progelem verifi assumpt prog elem deleteok delet ok iprogramel parent progelem getpar program element prog elem get parent list kid parent getchildren get children kid size kid get equal progelem prog elem kid remov deleteok delet ok verifyassumpt deleteok verifi assumpt delet ok removenod iprogramel progelem remov node program element prog elem verifyassumpt progelem verifi assumpt prog elem iprogramel parent progelem getpar program element prog elem get parent list kid parent getchildren get children kid size kid get equal progelem prog elem kid remov parent getchildren size parent getpar get children get parent parent getkind equal iprogramel kind code get kind program element parent getkind equal iprogramel kind get kind program element removenod parent remov node nullpointerexcept npe pointer except npe printstacktrac print stack trace verifyassumpt string info verifi assumpt system err println assert system err println info thread dumpstack dump stack system err println assert runtimeexcept assert runtim except verifyassumpt verifi assumpt thread dumpstack dump stack runtimeexcept assert runtim except modelinfo model info hashtabl nodetypecount hashtabl node type count properti extraproperti properti extra properti modelinfo ihierarchi hierarchi irelationshipmap relationshipmap model info hierarchi relationship map relationship map iprogramel ipe hierarchi getroot program element get root walkmodel ipe walk model recordstat filemaps record stat file map size integ hierarchi getfilemapentryset size tostr get file map entri set string recordstat relationshipmaps record stat relationship map size integ relationshipmap getentri size tostr relationship map get entri string walkmodel iprogramel ipe walk model program element countnod ipe count node list kid ipe getchildren get children iter iter kid iter iter hasnext next iprogramel nextel iprogramel iter next program element next element program element walkmodel nextel walk model next element countnod iprogramel ipe count node program element string node ipe getkind tostr get kind string integ ctr integ nodetypecount get node node type count ctr nodetypecount put node integ node type count ctr integ ctr intvalu valu nodetypecount put node ctr node type count string tostr string stringbuff sb stringbuff string buffer string buffer sb append model node summari enumer nodekey nodetypecount key node key node type count nodekey hasmoreel node key element string key string nodekey nextel node key next element integ ct integ nodetypecount get key node type count sb append key ct sb append model stat enumer ks extraproperti key extra properti ks hasmoreel element string string ks nextel next element string extraproperti getproperti extra properti get properti sb append sb tostr string properti getproperti get properti properti properti enumer nodekey nodetypecount key node key node type count nodekey hasmoreel node key element string key string nodekey nextel node key next element integ ct integ nodetypecount get key node type count setproperti key ct tostr set properti string putal extraproperti put extra properti recordstat string string string string record stat extraproperti setproperti string string extra properti set properti modelinfo summarizemodel model info summar model modelinfo asmmanag getdefault gethierarchi model info asm manag get get hierarchi asmmanag getdefault getrelationshipmap asm manag get get relationship map setcreatingmodel set creat model creatingmodel creat model iscreatingmodel creatingmodel creat model creat model setcompletingtypebind set complet type bind completingtypebind complet type bind iscompletingtypebind completingtypebind complet type bind complet type bind\n"
     ]
    }
   ],
   "source": [
    "code_list=[]\n",
    "for raw_code in test_raw_code:\n",
    "    code_list.append(raw_code['text'])\n",
    "code_text=' '.join(code_list)\n",
    "print(code_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
