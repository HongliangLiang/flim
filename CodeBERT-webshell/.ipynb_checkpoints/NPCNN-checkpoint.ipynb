{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow版本代码 用来做验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# \n",
    "\n",
    "# from keras.utils import to_categorical#transform one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial = tf.constant(0.1,shape=[100])\n",
    "initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1=tf.constant(2.0,shape=[32,1,1,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 40, 300, 1) (3, 300, 1, 100)\n",
      "(32, 38, 1, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 38, 1, 100])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in = np.array([[\n",
    "  [[2], [1], [2], [0], [1]],\n",
    "  [[1], [3], [2], [2], [3]],\n",
    "  [[1], [1], [3], [3], [0]],\n",
    "  [[2], [2], [0], [1], [1]],\n",
    "  [[0], [0], [3], [1], [2]], ]])\n",
    "kernel_in = np.array([\n",
    " [ [[2, 0.1,2,2,2]], [[3, 0.2,2,2,2]] ],\n",
    " [ [[0, 0.3,2,2,2]],[[1, 0.4,2,2,2]] ], ])\n",
    "input1 = tf.constant(1,shape=[32,40,300,1], dtype=tf.float32)\n",
    "kernel = tf.constant(0.1,shape=[3,300,1,100], dtype=tf.float32)\n",
    "print(x.shape,kernel.shape)\n",
    "out1=tf.nn.conv2d(input1, kernel, strides=[1, 1, 1, 1], padding='VALID')+initial\n",
    "print(out1.shape)\n",
    "h_conv1=tf.nn.relu(out1)\n",
    "h_conv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1, 1, 100])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_poo11 = tf.nn.max_pool(h_conv1,ksize=[1,input1.shape[1]-3+1,1,1],strides=[1,1,1,1],padding='VALID')\n",
    "h_poo11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = tf.concat([output1,h_poo11],axis=3) #把卷积核得出的特征向量连接起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 1, 1, 300])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 300, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2=tf.reshape(output1,[-1,8,300,1])\n",
    "input2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for training\n",
      "(2108, 308) (2108, 100, 40) (2108,) (4206, 300)\n",
      "finish reading\n"
     ]
    }
   ],
   "source": [
    "#测试NPCNN_hdj的代码数据加载部分\n",
    "print(\"loading data for training\")\n",
    "test_num = 58\n",
    "X = pickle.load(open('/home/hdj/NPCNN_hdj/parameters.in','rb'))\n",
    "train_report,train_code,train_labels,W = X[0],X[1],X[2],X[3]\n",
    "embedding = nn.Embedding.from_pretrained(torch.tensor(W))#从已有的weight导入embedding\n",
    "train_code = embedding(torch.tensor(train_code))\n",
    "train_report =embedding(torch.tensor(train_report))\n",
    "print(train_code.shape,train_report.shape)\n",
    "#被废弃的效率极低的代码\n",
    "# train_report = np.array(train_report)\n",
    "# train_code = np.array(train_code)\n",
    "# train_labels = np.array(train_labels)\n",
    "# # train_labels = to_categorical(train_labels)\n",
    "# W = np.array(W)\n",
    "# print(np.array(train_report).shape, np.array(train_code).shape, np.array(train_labels).shape, np.array(W).shape)\n",
    "print(\"finish reading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2108, 100, 40, 300]) torch.Size([2108, 308, 300])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding.from_pretrained(torch.tensor(W))\n",
    "train_code = embedding(torch.tensor(train_code))\n",
    "train_report =embedding(torch.tensor(train_report))\n",
    "print(train_code.shape,train_report.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这一步操作及其慢，需要优化\n",
    "W[[[[0,1]]]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2108, 308])\n"
     ]
    }
   ],
   "source": [
    "#测试torch的embedding使用\n",
    "embedding = nn.Embedding(4206, 300)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.tensor(train_report)\n",
    "print(input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2108, 100, 40, 300])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeTensor = torch.tensor(train_code)\n",
    "output=embedding(codeTensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2108, 308, 300])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=embedding(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_code[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_code[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_report[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_report[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_code = W[train_code]\n",
    "train_report = W[train_report]\n",
    "print(np.array(train_code).shape,np.array(train_report).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading data for training\n",
    "(3292, 208) (3292, 50, 20) (3292,) (3578, 300)\n",
    "finish reading\n",
    "(3292, 50, 20, 300) (3292, 208, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里使用CNN来进行code和report的编码 之后和codeBert的信息进行融合看能否得到提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义 code是两次卷积 report是一次卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AdamW\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data.distributed\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup, AdamW,\n",
    "                          RobertaConfig,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          RobertaTokenizer)\n",
    "from CodeBertModel import TextCNNClassifer\n",
    "import os\n",
    "# for i in range(58):\n",
    "#     os.system('python3 run_classifier.py --model_type roberta --model_name_or_path microsoft/codebert-base --task_name codesearch --do_predict --output_dir ../data/codesearch/test/ --data_dir ../data/codesearch/test/ --max_seq_length 512 --per_gpu_train_batch_size 32 --per_gpu_eval_batch_size 32 --learning_rate 1e-5 --num_train_epochs 8 --test_file aspectj_'+str(i)+'.txt  --pred_model_dir ./models/java/checkpoint-best/ --test_result_dir ./results/java/batch_result_'+str(i)+'.txt')\n",
    "from more_itertools import chunked\n",
    "def calculate_same_value(labels_sorted, test_p_sorted, start_pos):\n",
    "    i = start_pos\n",
    "    num_same = 0\n",
    "    num_p = 0\n",
    "    while test_p_sorted[start_pos] == test_p_sorted[i]:\n",
    "        num_same = num_same + 1\n",
    "        if labels_sorted[i] ==1 : num_p = num_p + 1\n",
    "        i = i + 1\n",
    "        if i == len(labels_sorted ): break\n",
    "    return num_p, num_same\n",
    "def eval_mrr(test_p, labels):#在第二维相似度得分，真实标签\n",
    "    test_p_sorted = test_p\n",
    "    test_p_index = sorted(range(len(test_p_sorted)), key=lambda k: test_p_sorted[k], reverse=True)  # 降序排序\n",
    "    test_p_sorted = sorted(test_p, reverse=True)\n",
    "\n",
    "    labels_sorted = []\n",
    "    for index in test_p_index:\n",
    "        labels_sorted.append(labels[index])\n",
    "\n",
    "    top_num = 10\n",
    "    top10rank = 0\n",
    "    for i in range(top_num):\n",
    "        if (labels_sorted[i] == 1):\n",
    "            '''\n",
    "            num_p, num_s = calculate_same_value(labels_sorted, test_p_sorted, i)\n",
    "            num_r = top_num - i\n",
    "            if num_p > (num_s-num_r):\n",
    "                top10rank = 1\n",
    "                break\n",
    "            v1 = perm(num_s-num_r, num_p)*perm(num_s-num_p,num_s-num_p)\n",
    "            v2 = perm(num_s,num_s)\n",
    "            top10rank = 1-(float)((float)(v1)/(float)(v2))\n",
    "            if top10rank > 1: top10rank=1\n",
    "            if top10rank!=top10rank: top10rank=1\n",
    "            break\n",
    "\n",
    "    return top10rank\n",
    "\n",
    "    '''\n",
    "            top10rank = 1\n",
    "            break\n",
    "    num_p, num_s = calculate_same_value(labels_sorted, test_p_sorted, 10)\n",
    "    if (num_p >= 1): top10rank = 1  # 统计在第十位并列排名相同的文件中，是否含有相关文件\n",
    "\n",
    "    MRRrank = 0.0\n",
    "    for i in range(len(labels_sorted)):\n",
    "        if (labels_sorted[i] == 1):\n",
    "            MRRrank = MRRrank + float(1 / (i + 1))\n",
    "            break\n",
    "\n",
    "    MAPrank = 0.0\n",
    "    pos_num = 0\n",
    "    for i in range(len(labels_sorted)):\n",
    "        if (labels_sorted[i] == 1):\n",
    "            pos_num = pos_num + 1\n",
    "            MAPrank = MAPrank + float(pos_num / (i + 1))\n",
    "    if pos_num==0:\n",
    "        print('出现不存在pos的例子')\n",
    "        pos_num=1\n",
    "    MAPrank = float(MAPrank / pos_num)\n",
    "#     MRRrank = float(MRRrank / pos_num)\n",
    "    \n",
    "    return top10rank, MRRrank, MAPrank\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "#     print(type(preds),type(labels))\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "\n",
    "    return acc_and_f1(preds, labels)\n",
    "#设置种子，为了结果复现\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "def minibatches(inputs_x1 = None,inputs_x2 = None,targets = None,batch_size = None,shuffle = False):\n",
    "    if shuffle:#判断是否混合\n",
    "        indices = np.arange(len(inputs_x1))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0,len(inputs_x1)-batch_size+1,batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx+batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx,start_idx+batch_size)\n",
    "        yield inputs_x1[excerpt],inputs_x2[excerpt],targets[excerpt]\n",
    "\n",
    "def get_testsets(test_code,test_report,test_label):\n",
    "    code_x = test_code[0]\n",
    "    for i in range(300-1):\n",
    "        code_x = np.vstack((code_x,test_code[i+1]))\n",
    "    report_x = test_report\n",
    "    y = test_label\n",
    "    return code_x,report_x,y\n",
    "def calculate_same_value(labels_sorted, test_p_sorted, start_pos):\n",
    "    i = start_pos\n",
    "    num_same = 0\n",
    "    num_p = 0\n",
    "    while test_p_sorted[start_pos] == test_p_sorted[i]:\n",
    "        num_same = num_same + 1\n",
    "        if labels_sorted[i] ==1 : num_p = num_p + 1\n",
    "        i = i + 1\n",
    "        if i == len(labels_sorted ): break\n",
    "    return num_p, num_same\n",
    "\n",
    "def eval_y (test_p, labels ):\n",
    "    test_p_sorted = test_p\n",
    "    test_p_index = sorted(range(len(test_p_sorted)) ,key = lambda k: test_p_sorted[k], reverse =True )#降序排序\n",
    "    test_p_sorted = sorted(test_p, reverse = True)\n",
    "\n",
    "    labels_sorted = []\n",
    "    for index in test_p_index:\n",
    "        labels_sorted.append(labels[index])\n",
    "\n",
    "    top_num = 10\n",
    "    top10rank = 0\n",
    "    for i in range(top_num):\n",
    "        if(labels_sorted[i] ==1 ):\n",
    "            '''\n",
    "            num_p, num_s = calculate_same_value(labels_sorted, test_p_sorted, i)\n",
    "            num_r = top_num - i\n",
    "            if num_p > (num_s-num_r):\n",
    "                top10rank = 1\n",
    "                break\n",
    "            v1 = perm(num_s-num_r, num_p)*perm(num_s-num_p,num_s-num_p)\n",
    "            v2 = perm(num_s,num_s)\n",
    "            top10rank = 1-(float)((float)(v1)/(float)(v2))\n",
    "            if top10rank > 1: top10rank=1\n",
    "            if top10rank!=top10rank: top10rank=1\n",
    "            break\n",
    "\n",
    "    return top10rank\n",
    "\n",
    "    '''\n",
    "            top10rank = 1\n",
    "            break\n",
    "    num_p, num_s = calculate_same_value(labels_sorted,test_p_sorted,10)\n",
    "    if(num_p >=1 ): top10rank = 1 #统计在第十位并列排名相同的文件中，是否含有相关文件\n",
    "\n",
    "    MRRrank = 0.0\n",
    "    for i in range(len(labels_sorted)):\n",
    "        if( labels_sorted[i] == 1 ):\n",
    "            MRRrank = MRRrank +  float(1/(i+1))\n",
    "            break\n",
    "\n",
    "    MAPrank = 0.0\n",
    "    pos_num = 0\n",
    "    for i in range(len(labels_sorted)):\n",
    "        if( labels_sorted[i] == 1):\n",
    "            pos_num = pos_num + 1\n",
    "            MAPrank = MAPrank + float(pos_num/(i+1))\n",
    "\n",
    "    MAPrank =  float(MAPrank/pos_num)\n",
    "#     MRRrank = float(MRRrank/pos_num)\n",
    "   \n",
    "    return top10rank, MRRrank, MAPrank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 38, 1])\n"
     ]
    }
   ],
   "source": [
    "#测试torch代码所用\n",
    "# #输入[batch_size,in_channels,height,weight]\n",
    "# #输出 [batch_size,out_channles,H_out,W_out]\n",
    "# inputCode = torch.randn(32, 1, 40, 300)\n",
    "# conv2 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(3, 300), stride=(1, 1), bias=True)\n",
    "# out=conv2(inputCode)\n",
    "# print(out.shape)\n",
    "# # out\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "# outRelu=F.relu(out)\n",
    "# print(outRelu.shape)\n",
    "# # pool of square window of size=3, stride=2\n",
    "# # m = nn.MaxPool2d(3, stride=2)\n",
    "# # pool of non-square window\n",
    "# m = nn.MaxPool2d(kernel_size=(38, 1), stride=(1, 1))\n",
    "# input = torch.randn(32, 100, 38, 1)\n",
    "# output = m(input)\n",
    "# print(output.shape)\n",
    "# listOut=[output,output]\n",
    "# out = torch.cat(listOut, dim=1)\n",
    "# print(out.shape)\n",
    "# out = out.view(-1, 1, 8,200)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型配置和定义部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 100\n"
     ]
    }
   ],
   "source": [
    "class TCNNConfig(object):\n",
    "    def __init__(self):\n",
    "        self.in_channels=1\n",
    "        self.out_channels=100\n",
    "        self.embedding_size=100\n",
    "        self.vocab_size=100\n",
    "        self.wordNums=40\n",
    "        self.codeLineNums=100\n",
    "        self.reportLineNums=308\n",
    "        self.window_sizes=[3,4,5]\n",
    "        self.num_class=2\n",
    "config=TCNNConfig()\n",
    "print(config.in_channels,config.out_channels)\n",
    "#torch \n",
    "'''\n",
    "in_channels(int) – 输入信号的通道\n",
    "out_channels(int) – 卷积产生的通道\n",
    "kerner_size(int or tuple) - 卷积核的尺寸\n",
    "stride(int or tuple, optional) - 卷积步长\n",
    "padding(int or tuple, optional) - 输入的每一条边补充0的层数\n",
    "dilation(int or tuple, optional) – 卷积核元素之间的间距\n",
    "groups(int, optional) – 从输入通道到输出通道的阻塞连接数\n",
    "bias(bool, optional) - 如果bias=True，添加偏置\n",
    "'''\n",
    "class CodeTextCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CodeTextCNN, self).__init__()\n",
    "        self.config=config\n",
    "#         self.embedding = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.embedding_size)\n",
    "        #(32, 1, 40, 300)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                          nn.Conv2d(in_channels=config.in_channels, out_channels=config.out_channels, kernel_size=(h,config.embedding_size ), stride=(1, 1), bias=True),\n",
    "                          #                              nn.BatchNorm1d(num_features=config.feature_size),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(kernel_size=(config.wordNums-h+1, 1), stride=(1, 1))\n",
    "            )\n",
    "#                           nn.MaxPool1d(kernel_size=config.max_text_len - h + 1))\n",
    "            for h in config.window_sizes\n",
    "        ])\n",
    "        #(32, 1, 100,300)\n",
    "        self.convs_lines = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv2d(in_channels=config.in_channels, out_channels=config.out_channels, kernel_size=(h, config.out_channels*len(config.window_sizes)), stride=(1, 1), bias=True),\n",
    "                          #                              nn.BatchNorm1d(num_features=config.feature_size),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(kernel_size=(config.codeLineNums-h+1, 1), stride=(1, 1))\n",
    "                         )\n",
    "            for h in config.window_sizes\n",
    "        ])\n",
    "    \n",
    "        self.report_convs = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv2d(in_channels=config.in_channels, out_channels=config.out_channels, kernel_size=(h, config.embedding_size), stride=(1, 1), bias=True),\n",
    "                          #                              nn.BatchNorm1d(num_features=config.feature_size),\n",
    "                          nn.ReLU(),\n",
    "                          nn.MaxPool2d(kernel_size=(config.reportLineNums-h+1, 1), stride=(1, 1))\n",
    "                         )\n",
    "            for h in config.window_sizes\n",
    "        ])\n",
    "        self.fc0=nn.Linear(in_features=config.out_channels*len(config.window_sizes)*2,out_features=config.out_channels*len(config.window_sizes)*2,bias=True)\n",
    "        self.fc1 = nn.Linear(in_features=config.out_channels*len(config.window_sizes) * 2,out_features=config.num_class,bias=True)\n",
    "#         self.fc0=nn.Linear(in_features=config.embedding_size*2,out_features=config.embedding_size*2,bias=True)\n",
    "#         self.fc1 = nn.Linear(in_features=config.embedding_size * 2,out_features=config.num_class,bias=True)\n",
    "#         self.drop=nn.Dropout(0.1)\n",
    "        self.classifier=nn.Linear(in_features=768*2,out_features=config.num_class,bias=True)\n",
    "        # if os.path.exists(config.embedding_path) and config.is_training and config.is_pretrain:\n",
    "        # print(\"Loading pretrain embedding...\")\n",
    "        # self.embedding.weight.data.copy_(torch.from_numpy(np.load(config.embedding_path)))\n",
    "        self.transformer=RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "    def forward(self, x_code,y_report,report_ids,report_mask,code_ids,code_mask):\n",
    "        #这是codeBert的编码\n",
    "#         h_report = self.transformer(input_ids=report_ids, attention_mask=report_mask)\n",
    "#         out_report=h_report[1]\n",
    "#         h_code = self.transformer(input_ids=code_ids, attention_mask=code_mask)\n",
    "#         out_code=h_code[1]\n",
    "# #         abs_dist = torch.abs(torch.add(out_report, -out_code))\n",
    "#         abs_dist=torch.cat((out_report,out_code),1)\n",
    "#         codeBertOut=self.classifier(abs_dist)\n",
    "#         return codeBertOut\n",
    "        #这是codeBert的编码\n",
    "        \n",
    "        # embed_x = self.embedding(x)self.out_channels\n",
    "        embed_x = x_code.view(-1,1,self.config.wordNums,config.embedding_size)#[32*100,40,300]-->[32*100,1,40,300]\n",
    "#         embed_y=y_report.view(-1,1,self.config.reportLineNums,config.out_channels*len(config.window_sizes))#[32,150,300]-->[32,1,150,300]\n",
    "#         embed_x = x_code.view(-1,1,self.config.wordNums,config.embedding_size)#[32*100,40,300]-->[32*100,1,40,300]\n",
    "        embed_y=y_report.view(-1,1,self.config.reportLineNums,config.embedding_size)#[32,150,300]-->[32,1,150,300]\n",
    "\n",
    "        out = [conv(embed_x) for conv in self.convs]  #[32*100,1,40,300]--> [32*100,100,38,1]--->pool [32*100,100,1,1]\n",
    "        out = torch.cat(out, dim=1)\n",
    "#         print('code第一次拼接的shape',out.shape)#[32*100, 300, 1, 1]\n",
    "        out = out.view(-1, 1, config.codeLineNums,config.out_channels*len(config.window_sizes))#[32,1,100,300]\n",
    "#         out = out.view(-1, 1, config.codeLineNums,config.embedding_size)#[32,1,100,300]\n",
    "#         print('code第一次卷积',out.shape)\n",
    "\n",
    "        # 开始在行间做二次卷积\n",
    "        out2 = [conv(out) for conv in self.convs_lines]#[32,1,100,300]-->[32,100,98,1]-->pool[32,100,1,1]\n",
    "#         for o in out2:\n",
    "#             print('o2', o.size())  # ([32, 100, 1, 1])\n",
    "        outMerge = torch.cat(out2, dim=1)  # [32,300,1,1]\n",
    "#         print('code二次卷积size：',outMerge.size())  # [32,300,1,1]\n",
    "        \n",
    "        report_out = [conv(embed_y) for conv in self.report_convs]  # [32,1,150,300]-->[32,100,148,1]-->pool[32,100,1,1]\n",
    "        report_outMerge = torch.cat(report_out, dim=1)#[32,300,1,1]\n",
    "#         print('report 卷积size:',report_outMerge.size())\n",
    "        report_outMerge = report_outMerge.view(-1, report_outMerge.size(1))#\n",
    "        \n",
    "        outMerge = outMerge.view(-1, outMerge.size(1))\n",
    "#         print('code size report size : ',outMerge.size(),report_outMerge.size())#torch.Size([32, 300]) torch.Size([32, 300])\n",
    "        #合并code和report编码得到的向量\n",
    "        merge=torch.cat([outMerge,report_outMerge],dim=1)\n",
    "#         print('merge得到的size大小：',merge.shape)\n",
    "#         merge=self.drop(merge)\n",
    "        output0=self.fc0(merge)\n",
    "        output1=self.fc1(output0)\n",
    "#         print('fc后的输出size',output.shape)\n",
    "        # if not self.use_element:\n",
    "        #     out2 = F.dropout(input=out2, p=self.dropout_rate)\n",
    "        #     out2 = self.fc(out2)\n",
    "#         return out2,report_out\n",
    "#         cos_simi=torch.mean(torch.cosine_similarity(outMerge, report_outMerge, dim=1))\n",
    "        return output1\n",
    "#         return outMerge,report_outMerge,output1\n",
    "model=CodeTextCNN(config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) tensor(0.3583) tensor([ 9.5252e-04, -7.2193e-01,  2.1057e-01,  9.7696e-01,  8.7698e-01,\n",
      "         9.3521e-01,  6.4361e-01, -5.6055e-02])\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(8, 2)\n",
    "input2 = torch.randn(8, 2)\n",
    "cos_simi=torch.cosine_similarity(input1, input2, dim=1)\n",
    "mean_cos_simi=torch.mean(cos_simi)\n",
    "print(cos_simi.shape,mean_cos_simi,cos_simi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_type = 'roberta'\n",
    "        self.output_dir='/data/hdj/data/CodeBERT/codesearch/models/java'\n",
    "        self.test_result_dir='/data/hdj/data/CodeBERT/codesearch/results/java/test_v1_0414.txt'\n",
    "        self.start_epoch=0\n",
    "        self.num_train_epochs=1\n",
    "        self.model_type='roberta'\n",
    "        self.config_name=''\n",
    "        self.model_name_or_path=None\n",
    "        self.task_name='codesearch'\n",
    "        self.tokenizer_name=''\n",
    "        self.model_name_or_path='microsoft/codebert-base'\n",
    "        self.do_lower_case=True\n",
    "        self.seed=42\n",
    "        self.gradient_accumulation_steps=1\n",
    "        self.weight_decay=0.0\n",
    "        self.max_grad_norm=1.0\n",
    "        self.learning_rate=1e-4\n",
    "        self.adam_epsilon=1e-8\n",
    "        self.warmup_steps=0\n",
    "        self.max_steps=-1\n",
    "        self.num_train_epochs=30\n",
    "# class args(object):\n",
    "#     \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.model_type = 'codesearch'\n",
    "#         self.output_dir='/data/hdj/data/CodeBERT/codesearch/models/java'\n",
    "#         self.test_result_dir='/data/hdj/data/CodeBERT/codesearch/results/java/test_v1_0414.txt'\n",
    "#         self.start_epoch=0\n",
    "#         self.num_train_epochs=1\n",
    "#         self.model_type='roberta'\n",
    "args=args()\n",
    "#带权重的交叉熵\n",
    "# weights=torch.tensor([0.2,0.8]).cuda()\n",
    "# loss_fun=CrossEntropyLoss(weight=weights)\n",
    "#更换损失函数\n",
    "# lossRankFunction = nn.MarginRankingLoss()\n",
    "#不带权重的交叉熵\n",
    "loss_fun=CrossEntropyLoss()\n",
    "#设置种子 复现结果\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup, AdamW)\n",
    "early_stop=3\n",
    "eval_loss=0.0\n",
    "\n",
    "\n",
    "if  True:\n",
    "    #Google: 使用的优化器和迭代器\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    #Google\n",
    "else:\n",
    "#     hdj: textCNNClassifier 使用的优化器和计划器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-04)#, betas=(0.9, 0.999)\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "#     hdj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for training\n",
      "finish reading\n"
     ]
    }
   ],
   "source": [
    "#被废弃代码\n",
    "#import pickle\n",
    "# index=2\n",
    "# print(\"loading data for training\")\n",
    "# test_num = 58\n",
    "# X = pickle.load(open('/home/hdj/NPCNN_hdj/parameters.in','rb'))\n",
    "\n",
    "# train_report,train_code,train_labels,W = X[0],X[1],X[2],X[3]\n",
    "# embedding = nn.Embedding.from_pretrained(torch.tensor(W))#从已有的weight导入embedding\n",
    "# train_code = embedding(torch.tensor(train_code))\n",
    "# train_report =embedding(torch.tensor(train_report))\n",
    "# # train_report = np.array(train_report)\n",
    "# # train_code = np.array(train_code)\n",
    "# train_labels = np.array(train_labels)\n",
    "# # W = np.array(W)\n",
    "# # print(np.array(train_report).shape, np.array(train_code).shape, np.array(train_labels).shape, np.array(W).shape)\n",
    "# print(\"finish reading\")\n",
    "# #在W中找到所需的单词向量，维度多一维\n",
    "# # train_code = W[train_code]\n",
    "# # train_report = W[train_report]\n",
    "# # print(np.array(train_code).shape,np.array(train_report).shape)\n",
    "\n",
    "# train_x1 = train_code\n",
    "# train_x2 = train_report\n",
    "# train_y = train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('report shape :',train_x2.shape,' code shape:',train_x1.shape,' train_labels shape:',torch.tensor(train_labels).shape,' embedding shape:',embedding.num_embeddings)\n",
    "# #15:40--16:12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要为数据的code生成文本形式\n",
    "def code_merge(raw_code_list):\n",
    "    code_list=[]\n",
    "    for raw_code in raw_code_list:\n",
    "        code_list.append(raw_code['text'])\n",
    "    code_text=' '.join(code_list)\n",
    "    return code_text\n",
    "\n",
    "def data_tensor_generate(data,project,tokenizer,maxLength=512,ttype='train'):\n",
    "    '''\n",
    "        输入是dataframe 包含bug_id\tcnn_report_idx\tcode_idx\tlabel\tpath\traw_report\traw_code\n",
    "    '''\n",
    "    #将raw_code里dict形式合并生成code_text\n",
    "    data['raw_code_text']=data['raw_code'].apply(code_merge)\n",
    "    #生成完成\n",
    "    all_labels=list(data['label'])\n",
    "    all_cnn_report_idx=list(data['cnn_report_idx'])\n",
    "    all_code_idx=list(data['code_idx'])\n",
    "    raw_report=list(data['raw_report'])\n",
    "    raw_code=list(data['raw_code_text'])\n",
    "#     reportInputs=tokenizer.batch_encode_plus(raw_report,add_special_tokens=True,max_length=maxLength,pad_to_max_length=True,return_token_type_ids=True,truncation=True)\n",
    "#     codeInputs=tokenizer.batch_encode_plus(raw_code,add_special_tokens=True,max_length=maxLength,pad_to_max_length=True,return_token_type_ids=True,truncation=True)\n",
    "#     all_report_input_ids=reportInputs['input_ids']\n",
    "#     all_report_input_mask=reportInputs['attention_mask']\n",
    "#     all_code_input_ids=codeInputs['input_ids']\n",
    "#     all_code_input_mask=codeInputs['attention_mask']\n",
    "    all_report_input_ids=[[0]for i in range(len(raw_code))]\n",
    "    all_report_input_mask=[[0]for i in range(len(raw_code))]\n",
    "    all_code_input_ids=[[0]for i in range(len(raw_code))]\n",
    "    all_code_input_mask=[[0]for i in range(len(raw_code))]\n",
    "    all_report_input_ids = torch.tensor(all_report_input_ids, dtype=torch.long)\n",
    "    all_report_input_mask = torch.tensor(all_report_input_mask, dtype=torch.long)\n",
    "    all_code_input_ids = torch.tensor(all_code_input_ids, dtype=torch.long)\n",
    "    all_code_input_mask = torch.tensor(all_code_input_mask, dtype=torch.long)\n",
    "    \n",
    "    all_cnn_report_idx=torch.tensor(all_cnn_report_idx, dtype=torch.long)\n",
    "    all_code_idx=torch.tensor(all_code_idx, dtype=torch.long)\n",
    "    all_labels = torch.tensor(all_labels, dtype=torch.long)\n",
    "    pickle.dump([all_report_input_ids, all_report_input_mask, all_code_input_ids, all_code_input_mask,all_cnn_report_idx,\n",
    "                          all_code_idx, all_labels],open(\"/data/hdj/cross_project_trans/encode_data/\"+project+\"parameters_\"+ttype+\".in\", \"wb\"))\n",
    "    dataset = TensorDataset(all_report_input_ids, all_report_input_mask, all_code_input_ids, all_code_input_mask,all_cnn_report_idx,\n",
    "                          all_code_idx, all_labels)\n",
    "#     if ttype=='train':\n",
    "    return dataset\n",
    "#     else:\n",
    "#         return dataset,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AdamW\n",
    "more_train_data=pd.read_pickle('/home/hdj/cross_project_trans/report/aspectj_more_train_data.pkl')\n",
    "# swt_more_train_data=pd.read_pickle('/home/hdj/cross_project_trans/report/swt_more_train_data.pkl')\n",
    "# zxing_more_train_data=pd.read_pickle('/home/hdj/cross_project_trans/report/zxing_more_train_data.pkl')\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "project='aspectj_'\n",
    "# project='swt_'\n",
    "# project='zxing_'\n",
    "train_set=data_tensor_generate(more_train_data,project,tokenizer=tokenizer,maxLength=256)\n",
    "batch_size=128\n",
    "training_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=False)\n",
    "    #\n",
    "# raw_report[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(aspectj_more_train_data['bug_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#直接从保存的文件里读取train和test数据\n",
    "batch_size=64\n",
    "aspectj_train = pickle.load(open('/data/hdj/cross_project_trans/encode_data/aspectj_parameters_train.in','rb'))\n",
    "all_report_input_ids, all_report_input_mask, all_code_input_ids, all_code_input_mask,all_cnn_report_idx,all_code_idx, all_labels=aspectj_train[0],aspectj_train[1],aspectj_train[2],aspectj_train[3],aspectj_train[4],aspectj_train[5],aspectj_train[6]\n",
    "train_set = TensorDataset(all_report_input_ids, all_report_input_mask, all_code_input_ids, all_code_input_mask,all_cnn_report_idx,\n",
    "                          all_code_idx, all_labels)\n",
    "training_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1668, 1]),\n",
       " torch.Size([1668, 1]),\n",
       " torch.Size([1668, 1]),\n",
       " torch.Size([1668, 1]),\n",
       " torch.Size([1668, 308]),\n",
       " torch.Size([1668, 100, 40]),\n",
       " torch.Size([1668]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.tensors[0].shape,train_set.tensors[1].shape,train_set.tensors[2].shape,train_set.tensors[3].shape,train_set.tensors[4].shape,train_set.tensors[5].shape,train_set.tensors[6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.MarginRankingLoss()\n",
    "# input1 = torch.tensor([[1.0,2.0,3.0]], requires_grad=True)\n",
    "# input2 = torch.tensor([[4.0,5.0,6.0]], requires_grad=True)\n",
    "# print(input1)\n",
    "# print(input2)\n",
    "# # target = torch.randn(3).sign()\n",
    "# target=torch.tensor([[1]])\n",
    "# output = loss(input1, input2, target)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=torch.tensor([1,2,3])\n",
    "target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 23.70 GiB total capacity; 1.89 GiB already allocated; 183.56 MiB free; 1.96 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ddad5fe210f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#codeBert需要的数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#         outputs=model(train_x1_batch,train_x2_batch,report_ids,report_mask,code_ids,code_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x1_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_x2_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreport_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreport_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcode_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcode_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mpred_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         print('cos_simi type',type(cos_simi),cos_simi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/waf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-df2c5a905c1d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_code, y_report, report_ids, report_mask, code_ids, code_mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0membed_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreportLineNums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[32,150,300]-->[32,1,150,300]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#[32*100,1,40,300]--> [32*100,100,38,1]--->pool [32*100,100,1,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#         print('code第一次拼接的shape',out.shape)#[32*100, 300, 1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-df2c5a905c1d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0membed_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreportLineNums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[32,150,300]-->[32,1,150,300]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#[32*100,1,40,300]--> [32*100,100,38,1]--->pool [32*100,100,1,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#         print('code第一次拼接的shape',out.shape)#[32*100, 300, 1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/waf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/waf/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/waf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/waf/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/waf/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 23.70 GiB total capacity; 1.89 GiB already allocated; 183.56 MiB free; 1.96 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# t_total = len(train_x1) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "model.zero_grad()\n",
    "model.train()\n",
    "w= pickle.load(open('/home/hdj/cross_project_trans/report/aspectj_test_middle/w_parameters.in','rb'))\n",
    "# embedding = nn.Embedding.from_pretrained(torch.tensor(aspectj_w[0]))#从已有的weight导入embedding\n",
    "# swt_w= pickle.load(open('/home/hdj/cross_project_trans/report/swt_test_middle/w_parameters.in','rb'))\n",
    "# w= pickle.load(open('/home/hdj/cross_project_trans/report/zxing_test_middle/w_parameters.in','rb'))\n",
    "embedding = nn.Embedding.from_pretrained(torch.tensor(w[0]))#从已有的weight导入embedding\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, args.warmup_steps, t_total)\n",
    "# batch_size=128\n",
    "for epoche in range(10):\n",
    "    total_acc=0.0\n",
    "    total_nums=0\n",
    "    i=0\n",
    "    num_batch = len(train_set) / batch_size #总共训练次数\n",
    "    for _, data in enumerate(training_loader, 0):#start=0 默认就是0\n",
    "        #这是NPCNN需要的数据\n",
    "        train_x1_batch=embedding(data[5])\n",
    "        train_x2_batch=embedding(data[4])\n",
    "        y_train=data[6].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "#         print(train_x1_batch.shape,train_x2_batch.shape)\n",
    "        train_x1_batch = train_x1_batch.reshape(train_x1_batch.shape[0]*train_x1_batch.shape[1],train_x1_batch.shape[2],train_x1_batch.shape[3])\n",
    "        train_x1_batch = train_x1_batch.to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "        train_x2_batch = train_x2_batch.to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "        y_train = y_train.to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        #NPCNN需要的数据\n",
    "        #codeBert需要的数据\n",
    "        report_ids=None\n",
    "        report_mask=None\n",
    "        code_ids=None\n",
    "        code_mask=None\n",
    "#         report_ids=data[0].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "#         report_mask=data[1].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "#         code_ids=data[2].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "#         code_mask=data[3].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        #codeBert需要的数据\n",
    "#         outputs=model(train_x1_batch,train_x2_batch,report_ids,report_mask,code_ids,code_mask)\n",
    "        outputs=model(train_x1_batch,train_x2_batch,report_ids,report_mask,code_ids,code_mask)\n",
    "        pred_choice = outputs.max(1)[1]\n",
    "#         print('cos_simi type',type(cos_simi),cos_simi)\n",
    "#         print(type(y_train),y_train)\n",
    "        loss=loss_fun(outputs, y_train)\n",
    "#         print('codeEmd shape:',codeEmd.shape,reportEmd.shape,y_train.shape)\n",
    "#         lossRank=lossRankFunction(codeEmd,reportEmd,y_train.reshape(-1,1))\n",
    "        eval_loss += loss.item()\n",
    "        \n",
    "#         if args.gradient_accumulation_steps > 1:\n",
    "#              loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "        loss.backward()\n",
    "#         lossRank.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "#         if(i+1)%args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "#             scheduler.step()\n",
    "        model.zero_grad()\n",
    "        correct = pred_choice.eq(y_train).cpu().sum()\n",
    "        metrics = compute_metrics(pred_choice.cpu().numpy(), y_train.cpu().numpy())\n",
    "#         print(metrics)\n",
    "#         if i %4 == 0:\n",
    "#             print('[',epoche,':',i,'/',num_batch,']',\"loss :%.4f\" % loss.item(),' , accuracy: ',correct.item() / float(batch_size) ,'true nums:',sum(y_train.cpu().numpy()),' ration :',sum(y_train.cpu().numpy())/len(y_train),\"acc :\",metrics['acc'],\" f1 :\",metrics['f1'])\n",
    "        #print(train_x1_batch.shape,train_x2_batch.shape,y_train.shape)\n",
    "        print('[',epoche,':',i,'/',num_batch,']',\"loss :%.4f\" % loss.item(),' , accuracy: ',correct.item() / float(batch_size) ,'true nums:',sum(y_train.cpu().numpy()),' ration :',sum(y_train.cpu().numpy())/len(y_train),\"acc :\",metrics['acc'],\" f1 :\",metrics['f1'])\n",
    "#         file_path = '/home/hdj/tencent/textCNN/model_v2_age_0606_predicted.hdf5'\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #之前版本的代码\n",
    "# t_total = len(train_x1) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "# model.zero_grad()\n",
    "# model.train()\n",
    "# # scheduler = get_linear_schedule_with_warmup(optimizer, args.warmup_steps, t_total)\n",
    "# batch_size=128\n",
    "# for epoche in range(10):\n",
    "#     total_acc=0.0\n",
    "#     total_nums=0\n",
    "#     i=0\n",
    "#     num_batch = len(train_x1) / batch_size #总共训练次数\n",
    "#     for train_x1_batch,train_x2_batch,y_train in  minibatches(train_x1,train_x2,train_y,batch_size,True):\n",
    "#         train_x1_batch = train_x1_batch.reshape(train_x1_batch.shape[0]*train_x1_batch.shape[1],train_x1_batch.shape[2],train_x1_batch.shape[3])\n",
    "#         train_x1_batch = torch.tensor(train_x1_batch).to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "#         train_x2_batch = torch.tensor(train_x2_batch).to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "#         y_train = torch.tensor(y_train).to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "#         outputs=model(train_x1_batch,train_x2_batch)\n",
    "#         pred_choice = outputs.max(1)[1]\n",
    "        \n",
    "# #         print(type(y_train),y_train)\n",
    "#         loss=loss_fun(outputs, y_train)\n",
    "#         eval_loss += loss.item()\n",
    "        \n",
    "# #         if args.gradient_accumulation_steps > 1:\n",
    "# #              loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "# #         if(i+1)%args.gradient_accumulation_steps == 0:\n",
    "#         optimizer.step()\n",
    "# #             scheduler.step()\n",
    "#         model.zero_grad()\n",
    "#         correct = pred_choice.eq(y_train).cpu().sum()\n",
    "#         metrics = compute_metrics(pred_choice.cpu().numpy(), y_train.cpu().numpy())\n",
    "# #         print(metrics)\n",
    "#         if i %4 == 0:\n",
    "#             print('[',epoche,':',i,'/',num_batch,']',\"loss :%.4f\" % loss.item(),' , accuracy: ',correct.item() / float(batch_size) ,'true nums:',sum(y_train.cpu().numpy()),' ration :',sum(y_train.cpu().numpy())/len(y_train),\"acc :\",metrics['acc'],\" f1 :\",metrics['f1'])\n",
    "#         #print(train_x1_batch.shape,train_x2_batch.shape,y_train.shape)\n",
    "# #         file_path = '/home/hdj/tencent/textCNN/model_v2_age_0606_predicted.hdf5'\n",
    "#         i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/hdj/NPCNN_hdj/model_31.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('/home/hdj/NPCNN_hdj/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open('/home/hdj/NPCNN_hdj/parameters.in','rb'))\n",
    "\n",
    "train_report,train_code,train_labels,W = X[0],X[1],X[2],X[3]\n",
    "embedding = nn.Embedding.from_pretrained(torch.tensor(W))#从已有的weight导入embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0=pd.read_pickle('/home/hdj/cross_project_trans/report/aspectj_test_middle/test0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 7), (300, 7))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1=pd.read_pickle('/home/hdj/cross_project_trans/report/aspectj_test_middle/test1.pkl')\n",
    "test0.shape,test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.concat([test0,test1])\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectj_w= pickle.load(open('/home/hdj/cross_project_trans/report/aspectj_test_middle/w_parameters.in','rb'))\n",
    "embedding = nn.Embedding.from_pretrained(torch.tensor(aspectj_w[0]))#从已有的weight导入embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17400, 1]) torch.Size([17400, 1]) torch.Size([17400, 1]) torch.Size([17400, 1]) torch.Size([17400, 308]) torch.Size([17400, 100, 40]) torch.Size([17400])\n"
     ]
    }
   ],
   "source": [
    "#将test数据集合并\n",
    "all_test=pd.read_pickle('/home/hdj/cross_project_trans/report/aspectj_test_middle/test'+str(0)+'.pkl')\n",
    "# all_test=pd.read_pickle('/home/hdj/cross_project_trans/report/swt_test_middle/test'+str(0)+'.pkl')\n",
    "# all_test=pd.read_pickle('/home/hdj/cross_project_trans/report/zxing_test_middle/test'+str(0)+'.pkl')\n",
    "for test_index in range(1,58):\n",
    "# for test_index in range(1,20):\n",
    "# for test_index in range(1,4):\n",
    "    test=pd.read_pickle('/home/hdj/cross_project_trans/report/aspectj_test_middle/test'+str(test_index)+'.pkl')\n",
    "#     test=pd.read_pickle('/home/hdj/cross_project_trans/report/swt_test_middle/test'+str(test_index)+'.pkl')\n",
    "#     test=pd.read_pickle('/home/hdj/cross_project_trans/report/zxing_test_middle/test'+str(test_index)+'.pkl')\n",
    "    all_test=pd.concat([all_test,test])\n",
    "test_set=data_tensor_generate(all_test,project,tokenizer=tokenizer,maxLength=256,ttype='test')\n",
    "print(test_set.tensors[0].shape,test_set.tensors[1].shape,test_set.tensors[2].shape,test_set.tensors[3].shape,test_set.tensors[4].shape,test_set.tensors[5].shape,test_set.tensors[6].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set.tensors[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7e95b4add7da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#start=0 默认就是0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_x1_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_x2_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtest_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding' is not defined"
     ]
    }
   ],
   "source": [
    "top10_all = []\n",
    "MRR_all = []\n",
    "MAP_all = []\n",
    "#直接从保存的文件里读取train和test数据\n",
    "# batch_size=4\n",
    "test = pickle.load(open('/data/hdj/cross_project_trans/encode_data/aspectj_parameters_test.in','rb'))\n",
    "# test = pickle.load(open('/data/hdj/cross_project_trans/encode_data/swt_parameters_test.in','rb'))\n",
    "# test = pickle.load(open('/data/hdj/cross_project_trans/encode_data/zxing_parameters_test.in','rb'))\n",
    "all_report_input_ids, all_report_input_mask, all_code_input_ids, all_code_input_mask,all_cnn_report_idx,all_code_idx, all_labels=test[0],test[1],test[2],test[3],test[4],test[5],test[6]\n",
    "test_set = TensorDataset(all_report_input_ids, all_report_input_mask, all_code_input_ids, all_code_input_mask,all_cnn_report_idx,\n",
    "                          all_code_idx, all_labels)\n",
    "\n",
    "# test_set=data_tensor_generate(all_test,tokenizer=tokenizer,maxLength=10)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=512, shuffle=False, num_workers=2, drop_last=False)\n",
    "test_p=[]\n",
    "test_y_labels=[]\n",
    "for _, data in enumerate(test_loader, 0):#start=0 默认就是0\n",
    "    with torch.no_grad():\n",
    "        train_x1_batch=embedding(data[5])\n",
    "        train_x2_batch=embedding(data[4])\n",
    "        test_labels=data[6]\n",
    "        print(train_x1_batch.shape,train_x2_batch.shape)\n",
    "        #NPCNN的数据处理\n",
    "        train_x1_batch = train_x1_batch.reshape(train_x1_batch.shape[0]*train_x1_batch.shape[1],train_x1_batch.shape[2],train_x1_batch.shape[3])\n",
    "        train_x1_batch = train_x1_batch.to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "        train_x2_batch = train_x2_batch.to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "        #NPCNN的数据处理\n",
    "        \n",
    "        #codeBert的数据处理\n",
    "        \n",
    "        report_ids= None #data[0].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        report_mask=None #data[1].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        code_ids=   None #data[2].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        code_mask=  None #data[3].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        #codeBert需要的数据\n",
    "        outputs=model(train_x1_batch,train_x2_batch,report_ids,report_mask,code_ids,code_mask)\n",
    "#         outputs=model(train_x1_batch,train_x2_batch)\n",
    "        test_p.extend(outputs[:,1].cpu())\n",
    "        test_y_labels.extend(test_labels)\n",
    "\n",
    "\n",
    "print(len(test_p),len(test_y_labels))\n",
    "for i in range(58):\n",
    "# for i in range(20):\n",
    "# for i in range(4):\n",
    "#     top10,MRR,MAP = eval_y(test_p[i*1000:(i+1)*1000], test_y_labels[i*1000:(i+1)*1000])\n",
    "    top10,MRR,MAP = eval_y(test_p[i*300:(i+1)*300], test_y_labels[i*300:(i+1)*300])\n",
    "    top10_all.append(top10)\n",
    "    MRR_all.append(MRR)\n",
    "    MAP_all.append(MAP)\n",
    "\n",
    "print('top10:', np.mean(top10_all))\n",
    "print('MRR:', np.mean(MRR_all))\n",
    "print('MAP:',np.mean(MAP_all))\n",
    "#16:17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e5332029971e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m58\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     top10,MRR,MAP = eval_y(test_p[i*1000:(i+1)*1000], test_y_labels[i*1000:(i+1)*1000])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtop10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMRR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtop10_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mMRR_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMRR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_p' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(58):\n",
    "#     top10,MRR,MAP = eval_y(test_p[i*1000:(i+1)*1000], test_y_labels[i*1000:(i+1)*1000])\n",
    "    top10,MRR,MAP = eval_y(test_p[i*300:(i+1)*300], test_y_labels[i*300:(i+1)*300])\n",
    "    top10_all.append(top10)\n",
    "    MRR_all.append(MRR)\n",
    "    MAP_all.append(MAP)\n",
    "\n",
    "print('top10:', np.mean(top10_all))\n",
    "print('MRR:', np.mean(MRR_all))\n",
    "print('MAP:',np.mean(MAP_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08290816326530612,\n",
       " 0.07522146111129586,\n",
       " 0.5,\n",
       " 0.11688311688311688,\n",
       " 0.08290816326530612,\n",
       " 0.07522146111129586,\n",
       " 0.5,\n",
       " 0.11688311688311688,\n",
       " 0.6575757575757576,\n",
       " 0.7553475935828877,\n",
       " 0.016666666666666666,\n",
       " 1.0,\n",
       " 0.18258673693456304,\n",
       " 0.25,\n",
       " 0.19105691056910568,\n",
       " 0.3938640132669983,\n",
       " 0.181592039800995,\n",
       " 0.6666666666666666,\n",
       " 0.22932489451476792,\n",
       " 0.5074074074074074,\n",
       " 0.26785714285714285,\n",
       " 0.5222222222222223,\n",
       " 0.1111111111111111,\n",
       " 0.07142857142857142,\n",
       " 0.3992424242424243,\n",
       " 0.19136539651806828,\n",
       " 0.1,\n",
       " 0.30952380952380953,\n",
       " 0.4109523809523809,\n",
       " 0.5069818913480885,\n",
       " 0.21929824561403508,\n",
       " 0.5113260130649486,\n",
       " 0.6904761904761906,\n",
       " 0.12261904761904763,\n",
       " 0.6785714285714285,\n",
       " 0.06069376026272578,\n",
       " 0.0707070707070707,\n",
       " 0.13508813435284023,\n",
       " 0.14285714285714285,\n",
       " 0.36964285714285716,\n",
       " 0.152451054692434,\n",
       " 0.6276085982982534,\n",
       " 0.08391608391608392,\n",
       " 0.5,\n",
       " 0.30240442096948034,\n",
       " 0.09793650793650795,\n",
       " 0.11688311688311688,\n",
       " 0.18888888888888888,\n",
       " 0.08566392479435958,\n",
       " 0.21807824969732437,\n",
       " 0.221957671957672,\n",
       " 0.24156746031746032,\n",
       " 0.3286844536844537,\n",
       " 0.2745759551961102,\n",
       " 0.5833333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.16247184684684685,\n",
       " 0.1,\n",
       " 0.3398301193755739,\n",
       " 0.7,\n",
       " 0.5,\n",
       " 0.5344444444444445]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #老版本代码\n",
    "# top10_all = []\n",
    "# MRR_all = []\n",
    "# MAP_all = []\n",
    "\n",
    "# for test_index in range(58):\n",
    "#     X = pickle.load(open(\"/home/hdj/NPCNN_hdj/middle/test_middle\"+str(test_index)+\".csv\",\"rb\"))\n",
    "#     test_report,test_code,test_labels = X[0],X[1],X[2]\n",
    "#     test_report = np.array(test_report,dtype='int')[:,:308]#TODO 308 表示最大report长度+2*filter_h\n",
    "#     test_code = np.array(test_code,dtype='int')\n",
    "#     test_labels = np.array(test_labels,dtype='int')\n",
    "#    # print(test_code.shape,test_report.shape,train_labels.shape)\n",
    "#     test_code_x, test_report_x, test_label = get_testsets(test_code,test_report,test_labels)\n",
    "# #     test_code_x = W[test_code_x].reshape(300*config.codeLineNums,config.wordNums,config.embedding_size)#300*50 TODO 100 表示最大语句数量 40表示每个statement最大词汇\n",
    "# #     test_report_x = W[test_report_x]\n",
    "#     test_code_x = embedding(torch.tensor(test_code_x)).reshape(300*config.codeLineNums,config.wordNums,config.embedding_size)\n",
    "#     test_report_x=embedding(torch.tensor(test_report_x))\n",
    "#     test_code_x = torch.tensor(test_code_x).to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "#     test_report_x = torch.tensor(test_report_x).to(device, dtype=torch.float).cuda(non_blocking=True)\n",
    "#     print('test_code_x :',test_code_x.shape,'test_report_x :',test_report_x.shape)\n",
    "#     output=model(test_code_x,test_report_x)\n",
    "# #     y_train = torch.tensor(y_train).to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "#     #print(test_code_x.shape,test_report_x.shape)\n",
    "    \n",
    "#     #pre,label = sess.run([prediction,output_label],feed_dict={x1:test_code_x,x2:test_report_x,keep_prob:0.25})\n",
    "#     #print(pre,label)\n",
    "#     test_p = output[:,1]\n",
    "#     test_y_labels = test_labels\n",
    "#    # top10 = eval_y(test_p, label, test_y_labels)\n",
    "#     #top10_all.append(top10)\n",
    "# #print('top10:', np.mean(top10_all))\n",
    "\n",
    "\n",
    "#     top10,MRR,MAP = eval_y(test_p, test_y_labels)\n",
    "#     top10_all.append(top10)\n",
    "#     MRR_all.append(MRR)\n",
    "#     MAP_all.append(MAP)\n",
    "\n",
    "# print('top10:', np.mean(top10_all))\n",
    "# print('MRR:', np.mean(MRR_all))\n",
    "# print('MAP:',np.mean(MAP_all))\n",
    "# #16:17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codeInput = torch.randn(32*100, 40,300)\n",
    "# reportInput=torch.randn(32,150,300)\n",
    "# output=cnnMode(codeInput,reportInput)\n",
    "# print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
