{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载包 定义相关util函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel, AdamW\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data.distributed\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import (compute_metrics, convert_examples_to_features,\n",
    "                        output_modes, processors)\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from transformers import (WEIGHTS_NAME, get_linear_schedule_with_warmup, AdamW,\n",
    "                          RobertaConfig,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          RobertaTokenizer)\n",
    "from CodeBertModel import TextCNNClassifer\n",
    "import os\n",
    "# for i in range(58):\n",
    "#     os.system('python3 run_classifier.py --model_type roberta --model_name_or_path microsoft/codebert-base --task_name codesearch --do_predict --output_dir ../data/codesearch/test/ --data_dir ../data/codesearch/test/ --max_seq_length 512 --per_gpu_train_batch_size 32 --per_gpu_eval_batch_size 32 --learning_rate 1e-5 --num_train_epochs 8 --test_file aspectj_'+str(i)+'.txt  --pred_model_dir ./models/java/checkpoint-best/ --test_result_dir ./results/java/batch_result_'+str(i)+'.txt')\n",
    "from more_itertools import chunked\n",
    "def calculate_same_value(labels_sorted, test_p_sorted, start_pos):\n",
    "    i = start_pos\n",
    "    num_same = 0\n",
    "    num_p = 0\n",
    "    while test_p_sorted[start_pos] == test_p_sorted[i]:\n",
    "        num_same = num_same + 1\n",
    "        if labels_sorted[i] ==1 : num_p = num_p + 1\n",
    "        i = i + 1\n",
    "        if i == len(labels_sorted ): break\n",
    "    return num_p, num_same\n",
    "def eval_mrr(test_p, labels):#在第二维相似度得分，真实标签\n",
    "    test_p_sorted = test_p\n",
    "    test_p_index = sorted(range(len(test_p_sorted)), key=lambda k: test_p_sorted[k], reverse=True)  # 降序排序\n",
    "    test_p_sorted = sorted(test_p, reverse=True)\n",
    "\n",
    "    labels_sorted = []\n",
    "    for index in test_p_index:\n",
    "        labels_sorted.append(labels[index])\n",
    "\n",
    "    top_num = 10\n",
    "    top10rank = 0\n",
    "    for i in range(top_num):\n",
    "        if (labels_sorted[i] == 1):\n",
    "            '''\n",
    "            num_p, num_s = calculate_same_value(labels_sorted, test_p_sorted, i)\n",
    "            num_r = top_num - i\n",
    "            if num_p > (num_s-num_r):\n",
    "                top10rank = 1\n",
    "                break\n",
    "            v1 = perm(num_s-num_r, num_p)*perm(num_s-num_p,num_s-num_p)\n",
    "            v2 = perm(num_s,num_s)\n",
    "            top10rank = 1-(float)((float)(v1)/(float)(v2))\n",
    "            if top10rank > 1: top10rank=1\n",
    "            if top10rank!=top10rank: top10rank=1\n",
    "            break\n",
    "\n",
    "    return top10rank\n",
    "\n",
    "    '''\n",
    "            top10rank = 1\n",
    "            break\n",
    "    num_p, num_s = calculate_same_value(labels_sorted, test_p_sorted, 10)\n",
    "    if (num_p >= 1): top10rank = 1  # 统计在第十位并列排名相同的文件中，是否含有相关文件\n",
    "\n",
    "    MRRrank = 0.0\n",
    "    for i in range(len(labels_sorted)):\n",
    "        if (labels_sorted[i] == 1):\n",
    "            MRRrank = MRRrank + float(1 / (i + 1))\n",
    "\n",
    "    MAPrank = 0.0\n",
    "    pos_num = 0\n",
    "    for i in range(len(labels_sorted)):\n",
    "        if (labels_sorted[i] == 1):\n",
    "            pos_num = pos_num + 1\n",
    "            MAPrank = MAPrank + float(pos_num / (i + 1))\n",
    "    if pos_num==0:\n",
    "        print('出现不存在pos的例子')\n",
    "        pos_num=1\n",
    "    MAPrank = float(MAPrank / pos_num)\n",
    "    MRRrank = float(MRRrank / pos_num)\n",
    "\n",
    "    return top10rank, MRRrank, MAPrank\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "#     print(type(preds),type(labels))\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "\n",
    "    return acc_and_f1(preds, labels)\n",
    "#设置种子，为了结果复现\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_type = 'roberta'\n",
    "        self.output_dir='/data/hdj/data/CodeBERT/codesearch/models/java'\n",
    "        self.test_result_dir='/data/hdj/data/CodeBERT/codesearch/results/java/test_v1_0414.txt'\n",
    "        self.start_epoch=0\n",
    "        self.num_train_epochs=1\n",
    "        self.model_type='roberta'\n",
    "        self.config_name=''\n",
    "        self.model_name_or_path=None\n",
    "        self.task_name='codesearch'\n",
    "        self.tokenizer_name=''\n",
    "        self.model_name_or_path='microsoft/codebert-base'\n",
    "        self.do_lower_case=True\n",
    "        self.seed=42\n",
    "        self.gradient_accumulation_steps=1\n",
    "        self.weight_decay=0.0\n",
    "        self.max_grad_norm=1.0\n",
    "        self.learning_rate=1e-6\n",
    "        self.adam_epsilon=1e-8\n",
    "        self.warmup_steps=0\n",
    "        self.max_steps=-1\n",
    "        self.num_train_epochs=3.0\n",
    "# class args(object):\n",
    "#     \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.model_type = 'codesearch'\n",
    "#         self.output_dir='/data/hdj/data/CodeBERT/codesearch/models/java'\n",
    "#         self.test_result_dir='/data/hdj/data/CodeBERT/codesearch/results/java/test_v1_0414.txt'\n",
    "#         self.start_epoch=0\n",
    "#         self.num_train_epochs=1\n",
    "#         self.model_type='roberta'\n",
    "args=args()\n",
    "#带权重的交叉熵\n",
    "# weights=torch.tensor([0.1,0.9]).cuda()\n",
    "# loss_fun=CrossEntropyLoss(weight=weights)\n",
    "#不带权重的交叉熵\n",
    "loss_fun=CrossEntropyLoss()\n",
    "#设置种子 复现结果\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#加载GraphCodeBert模型代码\n",
    "# from transformers import AutoModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "# model = AutoModel.from_pretrained(\"/home/hdj/graphcodebert-base\")\n",
    "#加载完毕\n",
    "\n",
    "# print(model) 打印模型结构信息\n",
    "# model.config 打印模型配置信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载并向量化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据并保存在cache里\n",
    "def load_and_cache_examples(data_dir,train_file,dev_file,test_file, task, tokenizer,max_seq_length, ttype='train'):\n",
    "    '''\n",
    "        data_dir:数据目录\n",
    "        train_file:训练文件\n",
    "        dev_file:验证文件\n",
    "        test_file:测试文件\n",
    "        task:任务名 固定为codesearch\n",
    "        tokenizer:分词器\n",
    "        max_seq_length:最大序列长度 convert_examples_to_features使用到了\n",
    "        ttype:类型 train dev test\n",
    "        \n",
    "        return：包装好的数据集 [all_input_ids, all_input_mask, all_segment_ids, all_label_ids]\n",
    "    '''\n",
    "    processor = processors[task]()#拿到CodesearchProcessor\n",
    "    output_mode = output_modes[task]#输出模式 固定为 classification\n",
    "    # if os.path.exists(cached_features_file):\n",
    "\n",
    "    label_list = processor.get_labels()#固定为 [\"0\",\"1\"]\n",
    "\n",
    "    '''\n",
    "        example结构：[InputExample(uid 'test-1',report,code,label)]\n",
    "    '''\n",
    "    examples=None\n",
    "    if ttype == 'train':\n",
    "        examples = processor.get_train_examples(data_dir, train_file)\n",
    "    elif ttype == 'dev':\n",
    "        examples = processor.get_dev_examples(data_dir, dev_file)\n",
    "    elif ttype == 'test':\n",
    "        #如果是test的话，instances就是[每一行是一条代测试的数据]\n",
    "        examples, instances = processor.get_test_examples(data_dir, test_file)\n",
    "\n",
    "    '''\n",
    "        example:[[uid,report,code,label],[]...]\n",
    "        label_list:['0','1']\n",
    "        max_seq_length:200\n",
    "        tokenizer:分词器\n",
    "        output_mode:输出模式 固定为 classification\n",
    "        cls_token:用来分类的token     '<s>', 0, \n",
    "        sep_token:用来分割语句的token '</s>', 2\n",
    "        \n",
    "        return：[  [       ( input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id)\n",
    "                    ]\n",
    "                ]\n",
    "                instances：[[\"0\",\"5222\",\"i am a noy\",\"public class\"],[],,]\n",
    "    '''\n",
    "#     features = convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_mode,\n",
    "#                                             cls_token=tokenizer.cls_token,\n",
    "#                                             sep_token=tokenizer.sep_token,\n",
    "#                                            )\n",
    "    features =convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_mode,\n",
    "                                                cls_token_at_end=bool(args.model_type in ['xlnet']),\n",
    "                                                # xlnet has a cls token at the end\n",
    "                                                cls_token=tokenizer.cls_token,\n",
    "                                                sep_token=tokenizer.sep_token,\n",
    "                                                cls_token_segment_id=2 if args.model_type in ['xlnet'] else 1,\n",
    "                                                pad_on_left=bool(args.model_type in ['xlnet']),\n",
    "                                                # pad on the left for xlnet\n",
    "                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if (ttype == 'test'):\n",
    "        return dataset, instances\n",
    "    else:\n",
    "        return dataset\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经写入数据量 0 of 222638\n",
      "已经写入数据量 10000 of 222638\n",
      "已经写入数据量 20000 of 222638\n",
      "已经写入数据量 30000 of 222638\n",
      "已经写入数据量 40000 of 222638\n",
      "已经写入数据量 50000 of 222638\n",
      "已经写入数据量 60000 of 222638\n",
      "已经写入数据量 70000 of 222638\n",
      "已经写入数据量 80000 of 222638\n",
      "已经写入数据量 90000 of 222638\n",
      "已经写入数据量 100000 of 222638\n",
      "已经写入数据量 110000 of 222638\n",
      "已经写入数据量 120000 of 222638\n",
      "已经写入数据量 130000 of 222638\n",
      "已经写入数据量 140000 of 222638\n",
      "已经写入数据量 150000 of 222638\n",
      "已经写入数据量 160000 of 222638\n",
      "已经写入数据量 170000 of 222638\n",
      "已经写入数据量 180000 of 222638\n",
      "已经写入数据量 190000 of 222638\n",
      "已经写入数据量 200000 of 222638\n",
      "已经写入数据量 210000 of 222638\n",
      "已经写入数据量 220000 of 222638\n",
      "已经写入数据量 0 of 55984\n",
      "已经写入数据量 10000 of 55984\n",
      "已经写入数据量 20000 of 55984\n",
      "已经写入数据量 30000 of 55984\n",
      "已经写入数据量 40000 of 55984\n",
      "已经写入数据量 50000 of 55984\n"
     ]
    }
   ],
   "source": [
    "#数据集的配置参数\n",
    "data_dir = \"/data/hdj/data/CodeBERT/data/codesearch/train_valid/\"\n",
    "test_dir=\"/data/hdj/data/CodeBERT/data/codesearch/test/swt_test/swt/\" #注意目前测试集的目录是单独的，更换测试数据需要更改\n",
    "# train_file = \"aspectj_train_small.txt\"\n",
    "# dev_file = \"aspectj_val_small.txt\"\n",
    "test_file = \"swt_0.txt\"#目前只使用一个例子来进行测试，后期增加数据量\n",
    "train_file='aspectj_train_oversample.txt'\n",
    "dev_file = \"aspectj_val_oversample.txt\"\n",
    "task_name = \"codesearch\"\n",
    "max_seq_length = 200 #关键参数设置 整个序列的最大长度\n",
    "#加载数据集\n",
    "#这里load_and_cache_examples 是没有将数据进行缓存的，和codeBert里的同名函数意义不一样，所以统一按照每次都读取数据这样的模式来处理\n",
    "train_set = load_and_cache_examples(data_dir,train_file,dev_file,test_file,task_name,tokenizer,max_seq_length,'train')\n",
    "val_set = load_and_cache_examples(data_dir,train_file,dev_file,test_file,task_name,tokenizer,max_seq_length,'dev')\n",
    "# test_set, instances=load_and_cache_examples(test_dir,train_file,dev_file,test_file,task_name,tokenizer,max_seq_length,'test')\n",
    "#将数据进行batch_size设置\n",
    "batch_size = 64 #关键参数\n",
    "training_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "#测试集顺序不应该打乱，不然后期就对不上了 shuffle的作用是在枚举的时候才会改变顺序 \n",
    "# testing_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64 #关键参数\n",
    "# val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集比例 tensor(40920) 222638 tensor(0.1838)\n",
      "验证集比例 tensor(10560) 55984 tensor(0.1886)\n"
     ]
    }
   ],
   "source": [
    "#目前train val数据集比较不均衡 test数据集比较均衡\n",
    "print('训练集比例',sum(list(training_loader.dataset.tensors[3])),len(training_loader.dataset.tensors[3]),sum(list(training_loader.dataset.tensors[3]))/len(training_loader.dataset.tensors[3]))\n",
    "print('验证集比例',sum(list(val_loader.dataset.tensors[3])),len(val_loader.dataset.tensors[3]),sum(list(val_loader.dataset.tensors[3]))/len(val_loader.dataset.tensors[3]))\n",
    "# print('测试集比例',sum(list(testing_loader.dataset.tensors[3])),len(testing_loader.dataset.tensors[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据加载 设计好 小数据 均匀一些 方便切换大数据 \n",
    "#方便添加多个训练集的数据 进行数据拆分重组\n",
    "#添加数据统计模块 数据均衡度如何 \n",
    "#定义好数据的格式 比例\n",
    "#各个函数的参数类型 格式 方便查找 不然又忘了\n",
    "#train val test\n",
    "\n",
    "#训练采用codeBERT GraphCodeBert 或者使用cnn/ast + codeBERT/GraphCodeBert\n",
    "#模型1 [codeBert]\n",
    "#模型2 [GraphCodeBert]\n",
    "#模型3 [cnn,codeBert]\n",
    "#模型4 [cnn,GraphCodeBert]\n",
    "#模型5 [ast,codeBert]\n",
    "#模型6 [ast,GraphCodeBert]\n",
    "#模型7 [cnn,ast,codeBert/GraphCodeBert]\n",
    "#模型8 [cnn,ast,codeBert/GraphCodeBert]\n",
    "#训练采用cross_entroy作为损失函数/复杂一点的使用triplet loss,val时就采用 f1、map mrr top来衡量 测试也是使用一样的指标\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载和信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用RobertClassification 模型\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'args' object has no attribute 'adam_epsilon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7963aef204e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mno_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     ]\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_grouped_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m#Google\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'args' object has no attribute 'adam_epsilon'"
     ]
    }
   ],
   "source": [
    "#模型1的实现\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "google_v=True #当为true时，使用RobertClassification模型\n",
    "num_labels=2\n",
    "\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
    "                                     num_labels=num_labels, finetuning_task=args.task_name)\n",
    "if google_v:\n",
    "    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path),\n",
    "                                    config=config)\n",
    "    print('使用RobertClassification 模型')\n",
    "else:\n",
    "    model = TextCNNClassifer()\n",
    "    print('使用TextCNNClassifier模型')\n",
    "_ = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if google_v:\n",
    "    #Google: 使用的优化器和迭代器\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    #Google\n",
    "else:\n",
    "#     hdj: textCNNClassifier 使用的优化器和计划器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-05, betas=(0.9, 0.999))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "#     hdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证和test的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始验证模型的有效性 把模型预测结果连同数据一起保存起来 然后再去判断top mrr mAP\n",
    "#流程 1.训练过程中 每个10轮去在val集上验证 比较f1值是否优于之前，是就保存结果到best，2.每次保存上一次验证的结果 3.最后再在test集上进行测试\n",
    "\n",
    "#check 词表是否与之前model有很大的不同 如果有那么该怎么改善，之前embedding矩阵是否就作废了\n",
    "def evaluate(args, model,eval_loader,instances,tokenizer, checkpoint=None, prefix=\"\", mode='dev'):\n",
    "    '''\n",
    "        args:模型训练时关键参数\n",
    "        model：待验证的模型\n",
    "        eval_loader:待验证的数据集 DataLoader 类型的参数\n",
    "        instances: 若为test模式，则将原始数据输入进来，然后将预测结果一起保存，等待后面计算TOP MAP MRR使用\n",
    "        tokenizer:分词器\n",
    "        checkpoint: 检查点\n",
    "        prefix:前缀\n",
    "        mode: 模式 分为 dev 和 test（需要保存数据）\n",
    "        \n",
    "        return: dict{'acc':,'f1':,'acc_f1'} 主要关注f1的值 越大说明正例的预测越准\n",
    "    '''\n",
    "    #待返回的验证结果\n",
    "    results={}\n",
    "    eval_loss = 0.0 #验证损失累加\n",
    "    nb_eval_steps = 0 #验证批次\n",
    "    preds = None #保存预测的值\n",
    "    out_label_ids = None #保存真实标签target\n",
    "    print('开始验证。。。')\n",
    "    for _, data in enumerate(eval_loader, 0):#start=0 默认就是0\n",
    "        with torch.no_grad():\n",
    "            ids = data[0].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "            mask = data[1].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "            token_type_ids=None#token_type_ids = data[2].to(device, dtype=torch.long).cuda(non_blocking=True) 因为模型不支持2维设置 所以不再输入这个参数\n",
    "            targets = data[3].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "            if google_v:\n",
    "                pred = model(input_ids=ids, attention_mask=mask,token_type_ids=None, labels=targets)\n",
    "            else:\n",
    "                pred = model(ids, mask, token_type_ids)#输出是[batch_size,2]\n",
    "            # print(pred, target)\n",
    "#             loss = loss_fun(pred, targets)\n",
    "#             pred_choice = pred.max(1)[1]\n",
    "#             correct = pred_choice.eq(targets).cpu().sum()\n",
    "#             metrics = compute_metrics(pred_choice.cpu().numpy(), targets.cpu().numpy())\n",
    "#             print('[',epoch,': ',_,'/',num_batch,'] ',blue('val'),\" loss :%.4f\" % loss.item(),' , accuracy: ',correct.item() / float(batch_size) ,'true nums:',sum(targets.cpu().numpy()),' ration :',sum(targets.cpu().numpy())/len(targets),\"acc :\",metrics['acc'],\" f1 :\",metrics['f1'])\n",
    "#             loss_list_val.append(loss.item())  \n",
    "            loss=pred[0]\n",
    "            eval_loss += loss.item()\n",
    "            if(_%100==0):\n",
    "                print(_,'/',len(eval_loader))\n",
    "        nb_eval_steps += 1\n",
    "        #将preds值和标签进行保存\n",
    "        if preds is None:\n",
    "            if google_v:\n",
    "                preds = pred[1].detach().cpu()\n",
    "            else:\n",
    "                preds=pred.detach().cpu()\n",
    "            out_label_ids = targets.detach().cpu()\n",
    "        else:\n",
    "            if google_v:\n",
    "                preds = np.append(preds, pred[1].detach().cpu().numpy(), axis=0)\n",
    "            else:    \n",
    "                preds = np.append(preds, pred.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, targets.detach().cpu().numpy(), axis=0)       \n",
    "    print('验证结束，开始计算指标')\n",
    "    #计算指标，并更新结果\n",
    " \n",
    "    #ndarra转换为tensor类型\n",
    "    preds=torch.tensor(preds)\n",
    "    out_label_ids=torch.tensor(out_label_ids)\n",
    "#     print(type(preds),preds.shape,preds)\n",
    "#     print(type(out_label_ids),out_label_ids.shape,out_label_ids)    \n",
    "    pred_choice = preds.max(1)[1]#[batch_size,2]\n",
    "    correct = pred_choice.eq(out_label_ids).cpu().sum() #eq需要tensor类型的数据\n",
    "    #但是compute_metrics 需要numpy的数据类型 真烦人\n",
    "    metrics = compute_metrics(pred_choice.numpy(), out_label_ids.numpy())\n",
    "    results.update(metrics)\n",
    "    #计算完成\n",
    "    \n",
    "    #下面分dev 和 test 模式来完成数据模型的保存工作\n",
    "    if(mode=='dev'):\n",
    "        #这里只是把result字典里acc和f1给保存到文件\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a+\") as writer:\n",
    "            print(\"***** Eval results {} *****\".format(prefix))\n",
    "            writer.write('evaluate %s\\n' % checkpoint)\n",
    "            for key in sorted(results.keys()):\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(results[key])))\n",
    "    elif(mode=='test'):\n",
    "        #test结果目录 将test数据连同预测结果进行保存\n",
    "        if(instances==None or len(instances)==0):\n",
    "            print('输入的测试数据有问题 为None 或者 长度为0')\n",
    "        #如果目录不存在就创建\n",
    "        output_test_file = args.test_result_dir\n",
    "        output_dir = os.path.dirname(output_test_file)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        #检验创建目录完成\n",
    "        with open(output_test_file, \"w\") as writer:\n",
    "            print(\"***** Output test results *****\")\n",
    "            all_logits = preds.tolist()\n",
    "            for i, logit in tqdm(enumerate(all_logits), desc='Testing'):#desc是进度条的标题\n",
    "                #instances即代表test集里每一行数据的值\n",
    "                instance_rep = '<CODESPLIT>'.join([item.encode('ascii', 'ignore').decode('ascii') for item in instances[i]])\n",
    "                writer.write(instance_rep + '<CODESPLIT>' + '<CODESPLIT>'.join([str(l) for l in logit]) + '\\n')\n",
    "            #打印验证的结果\n",
    "            for key in sorted(results.keys()):\n",
    "                print(\"%s = %s\" % (key, str(results[key])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和验证并保存最好模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training!!!\n",
      "[ 1 : 0 / 3478.71875 ] loss :0.7214  , accuracy:  0.390625 true nums: 9  ration : 0.140625 acc : 0.390625  f1 : 0.29090909090909095\n",
      "[ 1 : 100 / 3478.71875 ] loss :0.4448  , accuracy:  0.84375 true nums: 10  ration : 0.15625 acc : 0.84375  f1 : 0.0\n",
      "[ 1 : 200 / 3478.71875 ] loss :0.5115  , accuracy:  0.78125 true nums: 14  ration : 0.21875 acc : 0.78125  f1 : 0.0\n",
      "[ 1 : 300 / 3478.71875 ] loss :0.4255  , accuracy:  0.828125 true nums: 11  ration : 0.171875 acc : 0.828125  f1 : 0.0\n",
      "[ 1 : 400 / 3478.71875 ] loss :0.3050  , accuracy:  0.890625 true nums: 7  ration : 0.109375 acc : 0.890625  f1 : 0.0\n",
      "[ 1 : 500 / 3478.71875 ] loss :0.3815  , accuracy:  0.859375 true nums: 10  ration : 0.15625 acc : 0.859375  f1 : 0.47058823529411764\n",
      "[ 1 : 600 / 3478.71875 ] loss :0.2494  , accuracy:  0.921875 true nums: 11  ration : 0.171875 acc : 0.921875  f1 : 0.7058823529411764\n",
      "[ 1 : 700 / 3478.71875 ] loss :0.2976  , accuracy:  0.859375 true nums: 6  ration : 0.09375 acc : 0.859375  f1 : 0.30769230769230765\n",
      "[ 1 : 800 / 3478.71875 ] loss :0.2346  , accuracy:  0.921875 true nums: 9  ration : 0.140625 acc : 0.921875  f1 : 0.7058823529411765\n",
      "[ 1 : 900 / 3478.71875 ] loss :0.3225  , accuracy:  0.84375 true nums: 12  ration : 0.1875 acc : 0.84375  f1 : 0.4444444444444444\n",
      "[ 1 : 1000 / 3478.71875 ] loss :0.4252  , accuracy:  0.875 true nums: 14  ration : 0.21875 acc : 0.875  f1 : 0.6\n",
      "[ 1 : 1100 / 3478.71875 ] loss :0.2749  , accuracy:  0.921875 true nums: 14  ration : 0.21875 acc : 0.921875  f1 : 0.782608695652174\n",
      "[ 1 : 1200 / 3478.71875 ] loss :0.3307  , accuracy:  0.875 true nums: 11  ration : 0.171875 acc : 0.875  f1 : 0.6363636363636364\n",
      "[ 1 : 1300 / 3478.71875 ] loss :0.2928  , accuracy:  0.875 true nums: 9  ration : 0.140625 acc : 0.875  f1 : 0.6\n",
      "[ 1 : 1400 / 3478.71875 ] loss :0.2238  , accuracy:  0.890625 true nums: 10  ration : 0.15625 acc : 0.890625  f1 : 0.4615384615384615\n",
      "[ 1 : 1500 / 3478.71875 ] loss :0.2204  , accuracy:  0.9375 true nums: 9  ration : 0.140625 acc : 0.9375  f1 : 0.75\n",
      "[ 1 : 1600 / 3478.71875 ] loss :0.2380  , accuracy:  0.9375 true nums: 12  ration : 0.1875 acc : 0.9375  f1 : 0.8\n",
      "[ 1 : 1700 / 3478.71875 ] loss :0.2098  , accuracy:  0.9375 true nums: 12  ration : 0.1875 acc : 0.9375  f1 : 0.8\n",
      "[ 1 : 1800 / 3478.71875 ] loss :0.1843  , accuracy:  0.921875 true nums: 13  ration : 0.203125 acc : 0.921875  f1 : 0.7826086956521738\n",
      "[ 1 : 1900 / 3478.71875 ] loss :0.2713  , accuracy:  0.90625 true nums: 16  ration : 0.25 acc : 0.90625  f1 : 0.7692307692307693\n",
      "[ 1 : 2000 / 3478.71875 ] loss :0.3496  , accuracy:  0.859375 true nums: 15  ration : 0.234375 acc : 0.859375  f1 : 0.6666666666666665\n",
      "[ 1 : 2100 / 3478.71875 ] loss :0.2366  , accuracy:  0.90625 true nums: 16  ration : 0.25 acc : 0.90625  f1 : 0.7857142857142857\n",
      "[ 1 : 2200 / 3478.71875 ] loss :0.1516  , accuracy:  0.9375 true nums: 5  ration : 0.078125 acc : 0.9375  f1 : 0.6666666666666666\n",
      "[ 1 : 2300 / 3478.71875 ] loss :0.1699  , accuracy:  0.9375 true nums: 13  ration : 0.203125 acc : 0.9375  f1 : 0.8181818181818181\n",
      "[ 1 : 2400 / 3478.71875 ] loss :0.1111  , accuracy:  0.984375 true nums: 7  ration : 0.109375 acc : 0.984375  f1 : 0.9333333333333333\n",
      "[ 1 : 2500 / 3478.71875 ] loss :0.2130  , accuracy:  0.9375 true nums: 11  ration : 0.171875 acc : 0.9375  f1 : 0.7999999999999999\n",
      "[ 1 : 2600 / 3478.71875 ] loss :0.1420  , accuracy:  0.96875 true nums: 16  ration : 0.25 acc : 0.96875  f1 : 0.9333333333333333\n",
      "[ 1 : 2700 / 3478.71875 ] loss :0.1530  , accuracy:  0.921875 true nums: 14  ration : 0.21875 acc : 0.921875  f1 : 0.8275862068965518\n",
      "[ 1 : 2800 / 3478.71875 ] loss :0.2495  , accuracy:  0.890625 true nums: 14  ration : 0.21875 acc : 0.890625  f1 : 0.7407407407407408\n",
      "[ 1 : 2900 / 3478.71875 ] loss :0.2926  , accuracy:  0.859375 true nums: 9  ration : 0.140625 acc : 0.859375  f1 : 0.4\n",
      "[ 1 : 3000 / 3478.71875 ] loss :0.2125  , accuracy:  0.90625 true nums: 13  ration : 0.203125 acc : 0.90625  f1 : 0.7500000000000001\n",
      "[ 1 : 3100 / 3478.71875 ] loss :0.1822  , accuracy:  0.953125 true nums: 14  ration : 0.21875 acc : 0.953125  f1 : 0.888888888888889\n",
      "[ 1 : 3200 / 3478.71875 ] loss :0.3404  , accuracy:  0.890625 true nums: 15  ration : 0.234375 acc : 0.890625  f1 : 0.7200000000000001\n",
      "[ 1 : 3300 / 3478.71875 ] loss :0.1539  , accuracy:  0.96875 true nums: 9  ration : 0.140625 acc : 0.96875  f1 : 0.8888888888888888\n",
      "[ 1 : 3400 / 3478.71875 ] loss :0.2453  , accuracy:  0.921875 true nums: 18  ration : 0.28125 acc : 0.921875  f1 : 0.8484848484848485\n",
      "开始验证。。。\n",
      "0 / 874\n",
      "100 / 874\n",
      "200 / 874\n",
      "300 / 874\n",
      "400 / 874\n",
      "500 / 874\n",
      "600 / 874\n",
      "700 / 874\n",
      "800 / 874\n",
      "验证结束，开始计算指标\n",
      "***** Eval results  *****\n",
      "验证结果 ： {'acc': 0.8815431922196796, 'f1': 0.6242486106385392, 'acc_and_f1': 0.7528959014291094}\n",
      "Saving model checkpoint to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimizer and scheduler states to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-last\n",
      "Saving model checkpoint to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  33%|███▎      | 1/3 [24:49<49:39, 1489.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimizer and scheduler states to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-best\n",
      "[ 1 : 0 / 3478.71875 ] loss :0.0839  , accuracy:  0.984375 true nums: 11  ration : 0.171875 acc : 0.984375  f1 : 0.9523809523809523\n",
      "[ 1 : 100 / 3478.71875 ] loss :0.2482  , accuracy:  0.921875 true nums: 12  ration : 0.1875 acc : 0.921875  f1 : 0.7368421052631579\n",
      "[ 1 : 200 / 3478.71875 ] loss :0.1932  , accuracy:  0.921875 true nums: 12  ration : 0.1875 acc : 0.921875  f1 : 0.7368421052631579\n",
      "[ 1 : 300 / 3478.71875 ] loss :0.1268  , accuracy:  0.953125 true nums: 9  ration : 0.140625 acc : 0.953125  f1 : 0.8421052631578948\n",
      "[ 1 : 400 / 3478.71875 ] loss :0.1750  , accuracy:  0.953125 true nums: 14  ration : 0.21875 acc : 0.953125  f1 : 0.896551724137931\n",
      "[ 1 : 500 / 3478.71875 ] loss :0.2023  , accuracy:  0.921875 true nums: 17  ration : 0.265625 acc : 0.921875  f1 : 0.8387096774193549\n",
      "[ 1 : 600 / 3478.71875 ] loss :0.2122  , accuracy:  0.921875 true nums: 15  ration : 0.234375 acc : 0.921875  f1 : 0.8148148148148148\n",
      "[ 1 : 700 / 3478.71875 ] loss :0.2317  , accuracy:  0.890625 true nums: 13  ration : 0.203125 acc : 0.890625  f1 : 0.7199999999999999\n",
      "[ 1 : 800 / 3478.71875 ] loss :0.1174  , accuracy:  0.953125 true nums: 11  ration : 0.171875 acc : 0.953125  f1 : 0.8695652173913043\n",
      "[ 1 : 900 / 3478.71875 ] loss :0.1638  , accuracy:  0.921875 true nums: 7  ration : 0.109375 acc : 0.921875  f1 : 0.6666666666666666\n",
      "[ 1 : 1000 / 3478.71875 ] loss :0.1209  , accuracy:  0.953125 true nums: 12  ration : 0.1875 acc : 0.953125  f1 : 0.8695652173913043\n",
      "[ 1 : 1100 / 3478.71875 ] loss :0.1888  , accuracy:  0.921875 true nums: 13  ration : 0.203125 acc : 0.921875  f1 : 0.7826086956521738\n",
      "[ 1 : 1200 / 3478.71875 ] loss :0.2425  , accuracy:  0.875 true nums: 14  ration : 0.21875 acc : 0.875  f1 : 0.6666666666666666\n",
      "[ 1 : 1300 / 3478.71875 ] loss :0.2292  , accuracy:  0.875 true nums: 10  ration : 0.15625 acc : 0.875  f1 : 0.5555555555555556\n",
      "[ 1 : 1400 / 3478.71875 ] loss :0.0802  , accuracy:  0.96875 true nums: 6  ration : 0.09375 acc : 0.96875  f1 : 0.8333333333333334\n",
      "[ 1 : 1500 / 3478.71875 ] loss :0.1679  , accuracy:  0.953125 true nums: 15  ration : 0.234375 acc : 0.953125  f1 : 0.896551724137931\n",
      "[ 1 : 1600 / 3478.71875 ] loss :0.0682  , accuracy:  0.96875 true nums: 4  ration : 0.0625 acc : 0.96875  f1 : 0.8\n",
      "[ 1 : 1700 / 3478.71875 ] loss :0.1942  , accuracy:  0.921875 true nums: 14  ration : 0.21875 acc : 0.921875  f1 : 0.8148148148148148\n",
      "[ 1 : 1800 / 3478.71875 ] loss :0.1449  , accuracy:  0.9375 true nums: 7  ration : 0.109375 acc : 0.9375  f1 : 0.6666666666666666\n",
      "[ 1 : 1900 / 3478.71875 ] loss :0.1727  , accuracy:  0.953125 true nums: 12  ration : 0.1875 acc : 0.953125  f1 : 0.8571428571428571\n",
      "[ 1 : 2000 / 3478.71875 ] loss :0.1525  , accuracy:  0.96875 true nums: 17  ration : 0.265625 acc : 0.96875  f1 : 0.9411764705882353\n",
      "[ 1 : 2100 / 3478.71875 ] loss :0.0767  , accuracy:  0.984375 true nums: 8  ration : 0.125 acc : 0.984375  f1 : 0.9411764705882353\n",
      "[ 1 : 2200 / 3478.71875 ] loss :0.1039  , accuracy:  0.953125 true nums: 10  ration : 0.15625 acc : 0.953125  f1 : 0.8421052631578948\n",
      "[ 1 : 2300 / 3478.71875 ] loss :0.2001  , accuracy:  0.90625 true nums: 12  ration : 0.1875 acc : 0.90625  f1 : 0.6666666666666666\n",
      "[ 1 : 2400 / 3478.71875 ] loss :0.0604  , accuracy:  0.984375 true nums: 8  ration : 0.125 acc : 0.984375  f1 : 0.9411764705882353\n",
      "[ 1 : 2500 / 3478.71875 ] loss :0.3623  , accuracy:  0.859375 true nums: 12  ration : 0.1875 acc : 0.859375  f1 : 0.64\n",
      "[ 1 : 2600 / 3478.71875 ] loss :0.0487  , accuracy:  1.0 true nums: 7  ration : 0.109375 acc : 1.0  f1 : 1.0\n",
      "[ 1 : 2700 / 3478.71875 ] loss :0.2068  , accuracy:  0.921875 true nums: 10  ration : 0.15625 acc : 0.921875  f1 : 0.7368421052631577\n",
      "[ 1 : 2800 / 3478.71875 ] loss :0.1506  , accuracy:  0.9375 true nums: 16  ration : 0.25 acc : 0.9375  f1 : 0.8571428571428571\n",
      "[ 1 : 2900 / 3478.71875 ] loss :0.1588  , accuracy:  0.921875 true nums: 9  ration : 0.140625 acc : 0.921875  f1 : 0.7058823529411765\n",
      "[ 1 : 3000 / 3478.71875 ] loss :0.1555  , accuracy:  0.9375 true nums: 11  ration : 0.171875 acc : 0.9375  f1 : 0.7777777777777778\n",
      "[ 1 : 3100 / 3478.71875 ] loss :0.1649  , accuracy:  0.953125 true nums: 14  ration : 0.21875 acc : 0.953125  f1 : 0.88\n",
      "[ 1 : 3200 / 3478.71875 ] loss :0.0872  , accuracy:  0.953125 true nums: 16  ration : 0.25 acc : 0.953125  f1 : 0.9090909090909091\n",
      "[ 1 : 3300 / 3478.71875 ] loss :0.1995  , accuracy:  0.90625 true nums: 12  ration : 0.1875 acc : 0.90625  f1 : 0.8\n",
      "[ 1 : 3400 / 3478.71875 ] loss :0.0685  , accuracy:  0.984375 true nums: 15  ration : 0.234375 acc : 0.984375  f1 : 0.967741935483871\n",
      "开始验证。。。\n",
      "0 / 874\n",
      "100 / 874\n",
      "200 / 874\n",
      "300 / 874\n",
      "400 / 874\n",
      "500 / 874\n",
      "600 / 874\n",
      "700 / 874\n",
      "800 / 874\n",
      "验证结束，开始计算指标\n",
      "***** Eval results  *****\n",
      "验证结果 ： {'acc': 0.8917870423340961, 'f1': 0.663329439902108, 'acc_and_f1': 0.7775582411181021}\n",
      "Saving model checkpoint to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-last\n",
      "Saving optimizer and scheduler states to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-last\n",
      "Saving model checkpoint to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  67%|██████▋   | 2/3 [49:55<24:54, 1494.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimizer and scheduler states to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-best\n",
      "[ 1 : 0 / 3478.71875 ] loss :0.1911  , accuracy:  0.921875 true nums: 11  ration : 0.171875 acc : 0.921875  f1 : 0.7368421052631579\n",
      "[ 1 : 100 / 3478.71875 ] loss :0.2044  , accuracy:  0.9375 true nums: 13  ration : 0.203125 acc : 0.9375  f1 : 0.8333333333333333\n",
      "[ 1 : 200 / 3478.71875 ] loss :0.1398  , accuracy:  0.9375 true nums: 11  ration : 0.171875 acc : 0.9375  f1 : 0.7999999999999999\n",
      "[ 1 : 300 / 3478.71875 ] loss :0.1583  , accuracy:  0.953125 true nums: 11  ration : 0.171875 acc : 0.953125  f1 : 0.8695652173913043\n",
      "[ 1 : 400 / 3478.71875 ] loss :0.0779  , accuracy:  0.96875 true nums: 7  ration : 0.109375 acc : 0.96875  f1 : 0.8333333333333333\n",
      "[ 1 : 500 / 3478.71875 ] loss :0.1078  , accuracy:  0.96875 true nums: 11  ration : 0.171875 acc : 0.96875  f1 : 0.9090909090909091\n",
      "[ 1 : 600 / 3478.71875 ] loss :0.1311  , accuracy:  0.953125 true nums: 14  ration : 0.21875 acc : 0.953125  f1 : 0.888888888888889\n",
      "[ 1 : 700 / 3478.71875 ] loss :0.1355  , accuracy:  0.9375 true nums: 11  ration : 0.171875 acc : 0.9375  f1 : 0.8181818181818182\n",
      "[ 1 : 800 / 3478.71875 ] loss :0.2016  , accuracy:  0.9375 true nums: 15  ration : 0.234375 acc : 0.9375  f1 : 0.8571428571428571\n",
      "[ 1 : 900 / 3478.71875 ] loss :0.1580  , accuracy:  0.9375 true nums: 8  ration : 0.125 acc : 0.9375  f1 : 0.75\n",
      "[ 1 : 1000 / 3478.71875 ] loss :0.0449  , accuracy:  1.0 true nums: 10  ration : 0.15625 acc : 1.0  f1 : 1.0\n",
      "[ 1 : 1100 / 3478.71875 ] loss :0.0801  , accuracy:  0.984375 true nums: 12  ration : 0.1875 acc : 0.984375  f1 : 0.9600000000000001\n",
      "[ 1 : 1200 / 3478.71875 ] loss :0.1242  , accuracy:  0.96875 true nums: 11  ration : 0.171875 acc : 0.96875  f1 : 0.9\n",
      "[ 1 : 1300 / 3478.71875 ] loss :0.0878  , accuracy:  0.984375 true nums: 11  ration : 0.171875 acc : 0.984375  f1 : 0.9523809523809523\n",
      "[ 1 : 1400 / 3478.71875 ] loss :0.1739  , accuracy:  0.9375 true nums: 14  ration : 0.21875 acc : 0.9375  f1 : 0.8666666666666666\n",
      "[ 1 : 1500 / 3478.71875 ] loss :0.2016  , accuracy:  0.953125 true nums: 12  ration : 0.1875 acc : 0.953125  f1 : 0.8571428571428571\n",
      "[ 1 : 1600 / 3478.71875 ] loss :0.1691  , accuracy:  0.9375 true nums: 10  ration : 0.15625 acc : 0.9375  f1 : 0.8181818181818182\n",
      "[ 1 : 1700 / 3478.71875 ] loss :0.1015  , accuracy:  0.953125 true nums: 10  ration : 0.15625 acc : 0.953125  f1 : 0.8571428571428572\n",
      "[ 1 : 1800 / 3478.71875 ] loss :0.1292  , accuracy:  0.953125 true nums: 17  ration : 0.265625 acc : 0.953125  f1 : 0.9090909090909091\n",
      "[ 1 : 1900 / 3478.71875 ] loss :0.1106  , accuracy:  0.953125 true nums: 9  ration : 0.140625 acc : 0.953125  f1 : 0.8571428571428571\n",
      "[ 1 : 2000 / 3478.71875 ] loss :0.2549  , accuracy:  0.890625 true nums: 17  ration : 0.265625 acc : 0.890625  f1 : 0.787878787878788\n",
      "[ 1 : 2100 / 3478.71875 ] loss :0.0788  , accuracy:  0.984375 true nums: 13  ration : 0.203125 acc : 0.984375  f1 : 0.9600000000000001\n",
      "[ 1 : 2200 / 3478.71875 ] loss :0.0710  , accuracy:  0.96875 true nums: 15  ration : 0.234375 acc : 0.96875  f1 : 0.9285714285714286\n",
      "[ 1 : 2300 / 3478.71875 ] loss :0.1341  , accuracy:  0.953125 true nums: 9  ration : 0.140625 acc : 0.953125  f1 : 0.823529411764706\n",
      "[ 1 : 2400 / 3478.71875 ] loss :0.1523  , accuracy:  0.953125 true nums: 13  ration : 0.203125 acc : 0.953125  f1 : 0.8695652173913044\n",
      "[ 1 : 2500 / 3478.71875 ] loss :0.0470  , accuracy:  0.984375 true nums: 12  ration : 0.1875 acc : 0.984375  f1 : 0.9565217391304348\n",
      "[ 1 : 2600 / 3478.71875 ] loss :0.1064  , accuracy:  0.984375 true nums: 9  ration : 0.140625 acc : 0.984375  f1 : 0.9473684210526316\n",
      "[ 1 : 2700 / 3478.71875 ] loss :0.1130  , accuracy:  0.953125 true nums: 14  ration : 0.21875 acc : 0.953125  f1 : 0.896551724137931\n",
      "[ 1 : 2800 / 3478.71875 ] loss :0.2633  , accuracy:  0.875 true nums: 10  ration : 0.15625 acc : 0.875  f1 : 0.6\n",
      "[ 1 : 2900 / 3478.71875 ] loss :0.0741  , accuracy:  0.984375 true nums: 10  ration : 0.15625 acc : 0.984375  f1 : 0.9473684210526316\n",
      "[ 1 : 3000 / 3478.71875 ] loss :0.1207  , accuracy:  0.96875 true nums: 13  ration : 0.203125 acc : 0.96875  f1 : 0.9230769230769231\n",
      "[ 1 : 3100 / 3478.71875 ] loss :0.0820  , accuracy:  0.984375 true nums: 9  ration : 0.140625 acc : 0.984375  f1 : 0.9473684210526316\n",
      "[ 1 : 3200 / 3478.71875 ] loss :0.1942  , accuracy:  0.921875 true nums: 14  ration : 0.21875 acc : 0.921875  f1 : 0.8148148148148148\n",
      "[ 1 : 3300 / 3478.71875 ] loss :0.1437  , accuracy:  0.953125 true nums: 13  ration : 0.203125 acc : 0.953125  f1 : 0.888888888888889\n",
      "[ 1 : 3400 / 3478.71875 ] loss :0.1587  , accuracy:  0.9375 true nums: 11  ration : 0.171875 acc : 0.9375  f1 : 0.8181818181818182\n",
      "开始验证。。。\n",
      "0 / 874\n",
      "100 / 874\n",
      "200 / 874\n",
      "300 / 874\n",
      "400 / 874\n",
      "500 / 874\n",
      "600 / 874\n",
      "700 / 874\n",
      "800 / 874\n",
      "验证结束，开始计算指标\n",
      "***** Eval results  *****\n",
      "验证结果 ： {'acc': 0.8949692505720824, 'f1': 0.6657183499288762, 'acc_and_f1': 0.7803438002504793}\n",
      "Saving model checkpoint to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-last\n",
      "Saving optimizer and scheduler states to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-last\n",
      "Saving model checkpoint to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 100%|██████████| 3/3 [1:15:03<00:00, 1501.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimizer and scheduler states to %s /data/hdj/data/CodeBERT/codesearch/models/java/checkpoint-best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#模型应该保存效果最好的那个，而不是训练到最后的那个模型\n",
    "print(\"training!!!\")\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "epoch=1\n",
    "best_f1=0.0 #保存最好的f1的模型\n",
    "model.zero_grad()\n",
    "model.train()\n",
    "set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
    "#这里就是在指定 一共训练多少Epoch [start_epoch,num_train_epcchs]\n",
    "t_total = len(training_loader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, args.warmup_steps, t_total)\n",
    "\n",
    "train_iterator = trange(args.start_epoch, int(args.num_train_epochs), desc=\"Epoch\")\n",
    "for idx, p in enumerate(train_iterator):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    loss_list_val = []\n",
    "    acc_list_val = []\n",
    "    num_batch = len(train_set) / batch_size #总共训练次数\n",
    "    for _, data in enumerate(training_loader, 0):#start=0 默认就是0\n",
    "        ids = data[0].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        mask = data[1].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        token_type_ids=None\n",
    "    #     token_type_ids = data[2].to(device, dtype=torch.long).cuda(non_blocking=True) 因为模型不支持2维设置 所以不再输入这个参数\n",
    "        targets = data[3].to(device, dtype=torch.long).cuda(non_blocking=True)\n",
    "        if google_v:\n",
    "#             outputs = model(ids, mask, token_type_ids)\n",
    "            outputs = model(input_ids=ids, attention_mask=mask,token_type_ids=None, labels=targets)\n",
    "            loss=outputs[0]#google loss\n",
    "#             print(outputs)\n",
    "#             loss_hdj = loss_fun(outputs[1], targets)\n",
    "#             print('google loss ',loss,' loss_hdj :',loss_hdj)\n",
    "            pred_choice = outputs[1].max(1)[1]\n",
    "        else:\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            # print(outputs)\n",
    "            loss = loss_fun(outputs, targets)\n",
    "    #         print('查看target和ouput的类型 检查max函数返回的东西是否正确',type(outputs),type(targets))\n",
    "        #     print(outputs)\n",
    "            #取outputs二维中最大维的下标\n",
    "            pred_choice = outputs.max(1)[1]\n",
    "        if args.gradient_accumulation_steps > 1:\n",
    "             loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        if(_+1)%args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "        correct = pred_choice.eq(targets).cpu().sum()\n",
    "        metrics = compute_metrics(pred_choice.cpu().numpy(), targets.cpu().numpy())\n",
    "        if _ %100 == 0:\n",
    "            print('[',epoch,':',_,'/',num_batch,']',\"loss :%.4f\" % loss.item(),' , accuracy: ',correct.item() / float(batch_size) ,'true nums:',sum(targets.cpu().numpy()),' ration :',sum(targets.cpu().numpy())/len(targets),\"acc :\",metrics['acc'],\" f1 :\",metrics['f1'])\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(correct.item() / float(batch_size))\n",
    "        \n",
    "       \n",
    "    #下面是在训练过程中进行验证 每一轮都会验证一次，保存模型最好的 和最近一次的模型\n",
    "    results = evaluate(args, model,val_loader,None ,tokenizer, checkpoint=str(args.start_epoch + idx),mode='dev')\n",
    "    print('验证结果 ：',results)\n",
    "    #torch.save(model, '%s/cls_model_%d.pth' % ('models', epoch))\n",
    "    #./models/java/checkpoint-last\n",
    "    last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "    if not os.path.exists(last_output_dir):\n",
    "        os.makedirs(last_output_dir)\n",
    "#     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#     model_to_save.save_pretrained(last_output_dir)\n",
    "    torch.save(model, '%s/cls_model_%d.pth' % (last_output_dir, epoch))\n",
    "    print(\"Saving model checkpoint to %s\", last_output_dir)\n",
    "    #保存idx_file 文件 第几轮训练的记录\n",
    "    idx_file = os.path.join(last_output_dir, 'idx_file.txt')\n",
    "    with open(idx_file, 'w', encoding='utf-8') as idxf:\n",
    "        idxf.write(str(args.start_epoch + idx) + '\\n')\n",
    "    #保存idx_file完成\n",
    "    #保存优化器和计划器\n",
    "    torch.save(optimizer.state_dict(), os.path.join(last_output_dir, \"optimizer.pt\"))\n",
    "    torch.save(scheduler.state_dict(), os.path.join(last_output_dir, \"scheduler.pt\"))\n",
    "    print(\"Saving optimizer and scheduler states to %s\", last_output_dir)\n",
    "\n",
    "    if (results['f1'] > best_f1):\n",
    "        best_f1 = results['f1']\n",
    "        #./models/checkpoint-best 检查目录是否存在 否则创建\n",
    "        output_dir = os.path.join(args.output_dir, 'checkpoint-best')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        #保存model\n",
    "#         model_to_save = model.module if hasattr(model,'module') else model  # Take care of distributed/parallel training\n",
    "#         model_to_save.save_pretrained(output_dir)\n",
    "        torch.save(model, '%s/cls_model_%d.pth' % (output_dir, epoch))\n",
    "#         torch.save(args, os.path.join(output_dir, 'training_{}.bin'.format(idx)))\n",
    "        print(\"Saving model checkpoint to %s\", output_dir)\n",
    "        #保存优化器的计划器\n",
    "        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "        print(\"Saving optimizer and scheduler states to %s\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载模型进行test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经写入数据量 0 of 58000\n",
      "已经写入数据量 10000 of 58000\n",
      "已经写入数据量 20000 of 58000\n",
      "已经写入数据量 30000 of 58000\n",
      "已经写入数据量 40000 of 58000\n",
      "已经写入数据量 50000 of 58000\n"
     ]
    }
   ],
   "source": [
    "# results = evaluate(args, model,val_loader,None ,tokenizer, checkpoint=str(args.start_epoch + idx),mode='dev')\n",
    "#下面开启test模型性能\n",
    "#加载模型进行测试\n",
    "# del model\n",
    "import gc\n",
    "gc.collect()\n",
    "test_dir=\"/data/hdj/data/CodeBERT/data/codesearch/test/aspectj_test/aspectj/\" #注意目前测试集的目录是单独的，更换测试数据需要更改\n",
    "# test_file = \"test_all.txt\"#目前只使用一个例子来进行测试，后期增加数据量\n",
    "# test_dir=\"/data/hdj/data/CodeBERT/data/codesearch/test/swt_test/swt/\" \n",
    "test_file = \"test_all.txt\"#\n",
    "# train_file = \"aspectj_train_small.txt\"\n",
    "# dev_file = \"aspectj_val_small.txt\"\n",
    "\n",
    "test_set, instances=load_and_cache_examples(test_dir,train_file,dev_file,test_file,task_name,tokenizer,max_seq_length,'test')\n",
    "testing_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集比例 tensor(207) 58000 tensor(0.0036)\n"
     ]
    }
   ],
   "source": [
    "# testing_loader.dataset.tensors[0].shape\n",
    "print('测试集比例',sum(list(testing_loader.dataset.tensors[3])),len(testing_loader.dataset.tensors[3]),sum(list(testing_loader.dataset.tensors[3]))/len(testing_loader.dataset.tensors[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证。。。\n",
      "0 / 907\n",
      "100 / 907\n",
      "200 / 907\n",
      "300 / 907\n",
      "400 / 907\n",
      "500 / 907\n",
      "600 / 907\n",
      "700 / 907\n",
      "800 / 907\n",
      "900 / 907\n",
      "验证结束，开始计算指标\n",
      "***** Output test results *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 0it [00:00, ?it/s]\u001b[A\n",
      "Testing: 17899it [00:00, 178988.04it/s]\u001b[A\n",
      "Testing: 36168it [00:00, 180080.80it/s]\u001b[A\n",
      "Testing: 58000it [00:00, 173053.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.9355862068965517\n",
      "acc_and_f1 = 0.4917482512566347\n",
      "f1 = 0.04791029561671764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#先不删除模型直接进行预测 之后再删除模型 加载保存下来的模型进行预测\n",
    "test_results = evaluate(args, model,testing_loader,instances ,tokenizer, checkpoint=str(args.start_epoch + idx),mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始验证。。。\n",
      "0 / 584\n",
      "100 / 584\n",
      "200 / 584\n",
      "300 / 584\n",
      "400 / 584\n",
      "500 / 584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 0it [00:00, ?it/s]\u001b[A\n",
      "Testing: 7973it [00:00, 79725.89it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证结束，开始计算指标\n",
      "***** Output test results *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: 15345it [00:00, 77817.86it/s]\u001b[A\n",
      "Testing: 23594it [00:00, 79161.58it/s]\u001b[A\n",
      "Testing: 37362it [00:00, 79045.44it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.8480541726888282\n",
      "acc_and_f1 = 0.4280459575658015\n",
      "f1 = 0.008037742442774767\n"
     ]
    }
   ],
   "source": [
    "#加载模型 进行验证 保存结果 22;43\n",
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "output_dir = os.path.join(args.output_dir, 'checkpoint-best')\n",
    "model=torch.load('%s/cls_model_%d.pth' % (output_dir, epoch))\n",
    "test_results = evaluate(args, model,testing_loader,instances ,tokenizer, checkpoint=str(args.start_epoch + idx),mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #使用codeBert的模型不微调 测试效果如何\n",
    "# idx=0\n",
    "# test_results = evaluate(args, model,testing_loader,instances ,tokenizer, checkpoint=str(args.start_epoch + idx),mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算最终指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取的文件长度 : 58000\n",
      "start processing\n",
      "top10rank, MRRrank, MAPrank 1 0.07878151260504201 0.0861344537815126\n",
      "top10rank, MRRrank, MAPrank 0 0.009058978933503584 0.015132057368664686\n",
      "top10rank, MRRrank, MAPrank 1 1.0 1.0\n",
      "top10rank, MRRrank, MAPrank 1 0.10116822429906543 0.10233644859813085\n",
      "top10rank, MRRrank, MAPrank 0 0.01015057367669308 0.01752450185286006\n",
      "top10rank, MRRrank, MAPrank 1 0.15225882083833703 0.27646823354491906\n",
      "top10rank, MRRrank, MAPrank 0 0.002150537634408602 0.002150537634408602\n",
      "top10rank, MRRrank, MAPrank 1 1.0 1.0\n",
      "top10rank, MRRrank, MAPrank 0 0.003859979795035898 0.006283373394900087\n",
      "top10rank, MRRrank, MAPrank 1 0.5 0.5\n",
      "top10rank, MRRrank, MAPrank 1 0.062182775017363905 0.08360714858150343\n",
      "top10rank, MRRrank, MAPrank 1 0.11327286470143612 0.11611489040060469\n",
      "top10rank, MRRrank, MAPrank 1 0.06162790697674419 0.07325581395348837\n",
      "top10rank, MRRrank, MAPrank 1 0.09545454545454546 0.14090909090909093\n",
      "top10rank, MRRrank, MAPrank 0 0.015407654712831823 0.023176566092097426\n",
      "top10rank, MRRrank, MAPrank 1 0.05179004090750038 0.05459684248683478\n",
      "top10rank, MRRrank, MAPrank 0 0.02085115483319076 0.023845166809238663\n",
      "top10rank, MRRrank, MAPrank 0 0.012281494876431584 0.018233875828812536\n",
      "top10rank, MRRrank, MAPrank 1 1.0 1.0\n",
      "top10rank, MRRrank, MAPrank 1 1.0 1.0\n",
      "top10rank, MRRrank, MAPrank 0 0.019807697207501022 0.025892720648185895\n",
      "top10rank, MRRrank, MAPrank 0 0.008514888012366715 0.011923155019636493\n",
      "top10rank, MRRrank, MAPrank 0 0.010638297872340425 0.010638297872340425\n",
      "top10rank, MRRrank, MAPrank 1 0.12021553325901152 0.13359345968041622\n",
      "top10rank, MRRrank, MAPrank 1 0.2392607697872234 0.29915063982638623\n",
      "top10rank, MRRrank, MAPrank 1 0.31178315770752746 0.44001934106976126\n",
      "top10rank, MRRrank, MAPrank 1 0.26063829787234044 0.2712765957446808\n",
      "top10rank, MRRrank, MAPrank 1 0.1208888884504571 0.3994065671838686\n",
      "top10rank, MRRrank, MAPrank 0 0.024250860938167442 0.029568642571738548\n",
      "top10rank, MRRrank, MAPrank 1 0.04336795986149389 0.05714790474171436\n",
      "top10rank, MRRrank, MAPrank 1 0.14566864295125162 0.1792654808959157\n",
      "top10rank, MRRrank, MAPrank 0 0.012560397188336974 0.015842953593595693\n",
      "top10rank, MRRrank, MAPrank 0 0.02669969721992843 0.029589870630333057\n",
      "top10rank, MRRrank, MAPrank 1 0.14855269518094752 0.1586810681354713\n",
      "top10rank, MRRrank, MAPrank 1 0.5 0.5\n",
      "top10rank, MRRrank, MAPrank 1 0.03837883092889806 0.04699680726895988\n",
      "top10rank, MRRrank, MAPrank 1 0.039036880863754043 0.06264766733472892\n",
      "top10rank, MRRrank, MAPrank 1 0.2789216977754359 0.553776624850536\n",
      "top10rank, MRRrank, MAPrank 1 0.5208333333333334 0.5416666666666666\n",
      "top10rank, MRRrank, MAPrank 1 0.18467554197584748 0.2550814782482177\n",
      "top10rank, MRRrank, MAPrank 0 0.027940928978598727 0.0574094298481206\n",
      "top10rank, MRRrank, MAPrank 0 0.023600678041938786 0.025456688780471016\n",
      "top10rank, MRRrank, MAPrank 1 0.2692307692307692 0.28846153846153844\n",
      "top10rank, MRRrank, MAPrank 0 0.012834159450700804 0.01460586422992438\n",
      "top10rank, MRRrank, MAPrank 1 0.17127548404905554 0.17627370953113294\n",
      "top10rank, MRRrank, MAPrank 1 0.03922514078146089 0.08145019490447084\n",
      "top10rank, MRRrank, MAPrank 1 0.1796296296296296 0.1962962962962963\n",
      "top10rank, MRRrank, MAPrank 1 0.26233508818442347 0.28096428436162557\n",
      "top10rank, MRRrank, MAPrank 1 0.31307152806164934 0.5447570816490288\n",
      "top10rank, MRRrank, MAPrank 1 0.07955876445106057 0.12265799437825674\n",
      "top10rank, MRRrank, MAPrank 0 0.03154305200341006 0.04134697357203751\n",
      "top10rank, MRRrank, MAPrank 1 0.10114678899082569 0.10229357798165138\n",
      "top10rank, MRRrank, MAPrank 1 0.02627397307922349 0.03210873711308795\n",
      "top10rank, MRRrank, MAPrank 1 0.5045045045045045 0.509009009009009\n",
      "top10rank, MRRrank, MAPrank 1 0.22411293802271245 0.3406258319792154\n",
      "top10rank, MRRrank, MAPrank 0 0.005423845014043391 0.005993321096047946\n",
      "top10rank, MRRrank, MAPrank 0 0.03125 0.03125\n",
      "top10rank, MRRrank, MAPrank 1 0.11536160490442504 0.1326776490785659\n",
      "最后平均得分 top10rank, MRRrank, MAPrank : 0.6724137931034483 0.18557431053494355 0.21630298492225347\n"
     ]
    }
   ],
   "source": [
    "#开始计算TOP MRR MAP指标\n",
    "#有个问题 如果预测a=[0.2,0.5] b=[0.8,0.7] 这样的话，是将a排在b前 还是相反 ；暂时还是按照分数的绝对大小来判断吧\n",
    "#每1000验证一次三个指标\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/aspectj_withou_code_clasMeth_summDesc_result_all.txt'\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/aspectj_clasMeth_summDesc_result_all.txt'\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/aspectj_oversam_60_clasMeth_summDesc_result_all.txt'\n",
    "\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/swt_clasMeth_summDesc_result_all.txt'\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/swt_overoversam_60_clasMeth_summDesc_result_all.txt'\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/swt_withou_code_clasMeth_summDesc_result_all.txt'\n",
    "\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/512_0202_result_all.txt'\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/result_all.txt'\n",
    "path='/data/hdj/data/CodeBERT/codesearch/results/java/test_v1_0414.txt'\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/0415_aspectj_withou_code_clasMeth_summDesc_result_all.txt'\n",
    "# path='/data/hdj/data/CodeBERT/codesearch/results/java/0415_swt_withou_code_clasMeth_summDesc_result_all.txt'\n",
    "with open(path,'r',encoding='utf-8') as f_in:\n",
    "    data = f_in.readlines()\n",
    "    print('读取的文件长度 :',len(data))\n",
    "    #注意swt是479 aspectj是1000\n",
    "#     batched_data = chunked(data, 479)\n",
    "    batched_data = chunked(data, 1000)\n",
    "    print(\"start processing\")\n",
    "    top10ranks=[]\n",
    "    MRRranks=[]\n",
    "    MAPranks=[]\n",
    "    for batch_idx, batch_data in enumerate(batched_data):\n",
    "        preds=[]\n",
    "        out_label_ids=[]\n",
    "        report_ids=[]\n",
    "        paths=[]\n",
    "        for d_idx, d in enumerate(batch_data):\n",
    "            line=d.split('<CODESPLIT>')\n",
    "            out_label_ids.append(int(line[0]))\n",
    "            preds.append(float(line[-1]))\n",
    "            report_ids.append(line[1])\n",
    "            paths.append(line[2])\n",
    "        # print(len(preds),len(out_label_ids))\n",
    "        top10rank, MRRrank, MAPrank = eval_mrr(preds, out_label_ids)\n",
    "        print('top10rank, MRRrank, MAPrank',top10rank, MRRrank, MAPrank)\n",
    "        top10ranks.append(top10rank)\n",
    "        MRRranks.append(MRRrank)\n",
    "        MAPranks.append(MAPrank)\n",
    "        # with open('/data/hdj/data/CodeBERT/data/codesearch/test/aspectj_result_check/aspectj_'+str(batch_idx)+'.txt','w',encoding='utf-8') as f_out:\n",
    "        #     # f_out.writelines('\\n'.join(out_label_ids))\n",
    "        #      for label,pred,id,path in zip(out_label_ids,preds,report_ids,paths):\n",
    "        #          f_out.write(str(label)+\" \"+str(pred)+' '+id+' '+path+'\\n')\n",
    "\n",
    "    print('最后平均得分 top10rank, MRRrank, MAPrank :', sum(top10ranks)/len(top10ranks), sum(MRRranks)/len(MRRranks),sum(MAPranks)/len(MAPranks) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "37362/479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面试测试功能的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666 0.8\n"
     ]
    }
   ],
   "source": [
    "#测试compute_metrics函数作用的代码\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "#从这来看，必须保证正例样本要有，不然f1值都为0 尽量使数据不要那么失衡\n",
    "logit_last=np.array([[4,5],[4,5],[3,2]])\n",
    "label_ids=np.array([1,1,1])\n",
    "preds_label_last = np.argmax(logit_last, axis=1)\n",
    "result_last = compute_metrics(preds_label_last, label_ids)\n",
    "print(result_last['acc'],result_last['f1'])\n",
    "#测试结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.5, 'f1': 0.6}\n",
      "{'acc': 0.6, 'f1': 0.7}\n"
     ]
    }
   ],
   "source": [
    "#测试字典更新函数的代码\n",
    "results={}\n",
    "res1={'acc':0.5,'f1':0.6}\n",
    "res2={'acc':0.6,'f1':0.7}\n",
    "results.update(res1)\n",
    "print(results)\n",
    "results.update(res2)\n",
    "print(results)\n",
    "#测试结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inputs [tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [3, 3, 3],\n",
      "        [4, 4, 4]])]\n",
      "1 inputs [tensor([[5, 5, 5],\n",
      "        [6, 6, 6]])]\n"
     ]
    }
   ],
   "source": [
    "#测试shuffle作用的代码\n",
    "all_input_ids=torch.tensor([[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6]])\n",
    "dataset = TensorDataset(all_input_ids)\n",
    "testing_loader = DataLoader(dataset=dataset, batch_size=4, shuffle=False, num_workers=2, drop_last=False)\n",
    "testing_loader.dataset.tensors\n",
    "for i, data in enumerate(testing_loader, 0):\n",
    "    print(i, \"inputs\", data)\n",
    "#测试结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试eq函数的代码 eq只能是tensor变量才能使用\n",
    "preds=torch.tensor(np.array([[2.02,2.00],[1.26,1],[-2.3,1.2]]))\n",
    "print(type(preds),preds)\n",
    "targets=torch.tensor([0,1,1])\n",
    "pred_choice = preds.max(1)[1]#[batch_size,2]\n",
    "print(type(pred_choice),pred_choice)\n",
    "correct = pred_choice.eq(targets).cpu().sum()\n",
    "correct\n",
    "#测试结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试trange如何使用\n",
    "from tqdm import tqdm, trange\n",
    "# #这里就是在指定 一共训练多少Epoch\n",
    "# train_iterator = trange(args.start_epoch, int(args.num_train_epochs), desc=\"Epoch\")\n",
    "# for idx, _ in enumerate(train_iterator):\n",
    "#     print(\"a\",idx,_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试weight参数如何设置\n",
    "#输入target 一维类别LongTensor input二维 增加weight来控制0类别的比例\n",
    "weights=torch.tensor([0.2,0.7]).cuda()\n",
    "loss_fun=CrossEntropyLoss(weight=weights)\n",
    "# pred = torch.FloatTensor([[0.5, 0.8], [-0.1, 0.7]]).cuda()\n",
    "# target = torch.LongTensor([1, 0]).cuda()\n",
    "# print(pred)\n",
    "# loss=loss_fun(pred,target)\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看测试文件的顺序是否一致 必须一致下面计算TOP MRR MAP的结果才能正确\n",
    "# with open('/data/hdj/data/CodeBERT/codesearch/results/java/swt_clasMeth_summDesc_result_all.txt','r',encoding='utf-8') as f_in:\n",
    "#     for i,line  in enumerate(f_in):\n",
    "#         strings=line.split('<CODESPLIT>')\n",
    "#         print(strings[0],strings[1],strings[-2],strings[-1])\n",
    "#         if(i==2000):\n",
    "#             break\n",
    "#查看test集的数据是否正常\n",
    "# for i,ins in  enumerate(instances):\n",
    "#     print(i,ins[0],ins[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载打印模型的相关信息\n",
    "# from transformers import AutoModel\n",
    "# tokenizer=RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "# tokenizer.cls_token,tokenizer.cls_token_id,tokenizer.sep_token,tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_type = 'roberta'\n",
    "        self.output_dir='/data/hdj/data/CodeBERT/codesearch/models/java'\n",
    "        self.test_result_dir='/data/hdj/data/CodeBERT/codesearch/results/java/test_v1_0414.txt'\n",
    "        self.start_epoch=0\n",
    "        self.num_train_epochs=1\n",
    "        self.model_type='roberta'\n",
    "        self.config_name=''\n",
    "        self.model_name_or_path=None\n",
    "        self.task_name='codesearch'\n",
    "        self.tokenizer_name=''\n",
    "        self.model_name_or_path='microsoft/codebert-base'\n",
    "        self.do_lower_case=True\n",
    "args=args()\n",
    "# weights=torch.tensor([0.1,0.9]).cuda()\n",
    "# loss_fun=CrossEntropyLoss(weight=weights)\n",
    "# loss_fun=CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#210415 问题1 RobertClassification模型具体结构和参数使用\n",
    "num_labels=2\n",
    "MODEL_CLASSES = {'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
    "                                      num_labels=num_labels, finetuning_task=args.task_name)\n",
    "if args.tokenizer_name:\n",
    "    tokenizer_name = args.tokenizer_name\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer_name = 'roberta-base'\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=args.do_lower_case)\n",
    "model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path),\n",
    "                                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "input_ids=torch.tensor([[1,1,1],[2,2,2],[3,3,3]])\n",
    "attention_mask=torch.tensor([[1,1,1],[1,1,1],[1,1,1]])\n",
    "label=torch.tensor([0,0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=None, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7257, grad_fn=<NllLossBackward>),\n",
       " tensor([[-0.0580,  0.1690],\n",
       "         [ 0.0895,  0.0964],\n",
       "         [ 0.0569,  0.1093]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = output[1]\n",
    "y_pred_label = y_pred_prob.argmax(dim=1)\n",
    "# 计算loss\n",
    "# 这个 loss 和 output[0] 是一样的\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(y_pred_prob.view(-1, 2), label.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7257, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(model)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以覆盖最后一层的值\n",
    "# model.classifier.out_proj=nn.Linear(768, 9)\n",
    "model.base_model.add_module=nn.Linear(768,768)\n",
    "model.base_model.add_module=nn.Linear(768,768)\n",
    "model.base_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "robertaModel=RobertaModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel(\n",
      "  (embeddings): RobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(robertaModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
